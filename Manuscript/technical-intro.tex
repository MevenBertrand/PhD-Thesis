\chapter{The Calculus of Inductive Constructions}
\label{chap:tech-intro}

Most of this thesis revolves around \kl[dependent type]{dependent type systems}.
Due to their complexity, there is a high number of
points subject to slight variations
when one tries to give a precise definition of a system.
Some of these variations are unimportant, but some introduce subtle albeit large differences
in the resulting systems. In this chapter we go in details over
the definition of what we refer to as the
\kl{Calculus of Inductive Constructions} (\kl{CIC}) in the rest of this
thesis, where it serves as the base system.
While doing so, I try to give an idea of the trade-offs involved, and of the reasons
behind the choices. Quite a few of those vary during the thesis,
and this is by design: there is no single better choice,
instead one has to adapt to the setting.

For the impatient specialists, let me say now that with \kl{CIC}, I
mean an intentional type theory, with Curry-style abstractions,
a predicative hierarchy of universes%
\sidenote{
  And only those: by default I do \emph{not} include an impredicative sort of propositions, a feature often associated with the name \kl{CIC}.}
\textit{à la} Russell, and any amount of inductive types presented by recursors.
% – the ones appearing most often in what comes next being the empty and
% unit types, booleans, natural numbers, dependent sums, lists, vectors and the equality.
Conversion is the reflexive, symmetric, transitive and congruent
closure of βι-reduction, and so in particular it is untyped.

For the others, this \namecref{chap:tech-intro} aims at introducing the basic
systems and properties which we will refer to in the rest of the text.
\cref{sec:tech-typing} introduces the basic notions;
\cref{sec:tech-ccw} presents a first type system,
the \kl{Calculus of Constructions} (\kl{CCω}),
the purely functional core all our systems rely on;
\cref{sec:tech-conversion} defines the various notions of \kl{conversion} and
\kl{reduction} encountered in the remaining of the thesis;
\cref{sec:tech-properties} introduces the main properties our systems should satisfy;
\cref{sec:tech-cic} adds inductive types to \kl{CCω} to build
\kl{CIC}; finally \cref{sec:tech-pcuic} discusses the extra additions to go from
\kl{CIC} to the \kl{Polymorphic, Cumulative Calculus of Inductive Constructions}
(\kl{PCUIC}), a relatively faithful model of the type theory implemented by the
kernel of \kl{Coq}.

\section{Terms, typing and derivations}
\label{sec:tech-typing}

Throughout this chapter, type systems are defined by means of a relation
$\Gamma \vdash t : T$, which reads "in the context $\Gamma$, the term $t$ has type $T$".
From the logical point of view, this judgement means that $\Gamma$
contains the hypothesis available to deduce the
conclusion $T$ by means of the proof $t$.
On the programming side, it means that $t$ is a well-formed program of type $T$,
which uses the variables listed together with their types in $\Gamma$.
Hence, $\Gamma$ is a list of declarations, of the form $x : A$.
We write $\emptycon$ for the
empty context, $\Gamma, x : A$ for the extension of context $\Gamma$ with the new variable $x : A$, and $(x : A) \in \Gamma$ to denote that the declaration $x : A$ appears at some
point in the context $\Gamma$.

\begin{marginfigure}
  \ContinuedFloat*
  \begin{mathpar}
  \inferdef{Var}{(x : A) \in \Gamma \\ \vdash \Gamma}{\Gamma \vdash x : A}
  \label{rule:cic-var}
  \end{mathpar}
  \caption{Typing rule for a variable}
  \label{fig:cic-var}
\end{marginfigure}

This typing relation itself is defined by means of inference rules,
such as \ruleref{rule:cic-var} opposite. The way to read this rule is that the judgement
underneath the line follows from the one above,
\ie from $(x : A) \in \Gamma$
and $\vdash \Gamma$ – a judgement that will soon be defined asserting that the context
$\Gamma$ is well-formed – we can deduce $\Gamma \vdash x : A$.
When objects appear in the hypothesis but not the conclusion, they are implicitly
universally quantified.
Once a set of such inference rules is fixed,
typing is defined as the least relation closed by those
rules. Equivalently, a judgement such as $\Gamma \vdash t : T$
holds whenever we can build a tree whose nodes are instances of the inference rules,
and whose root is the judgement in question. A general setting
for this kind of definitions of type systems can be found in \sidetextcite{Bauer2020},
but in our case we restrict to this level of informality for the time being.%
\sidenote{In \arefpart{metacoq}, however, such judgements
are formalized as inductively defined propositions.}

\AP As we have already introduced variables, a word on those as well. Variables are difficult
to account for precisely, because of issues like shadowing – a conflict between two variables
with the same name – or \intro{α-equality} – the identification between two terms
only differing on variable names. There are multiple techniques to solve these issues
– see the many solutions to the POPLMark Challenge~\sidecite{Aydemir2005} –, 
but we again treat these in an informal way, assuming
there is no shadowing whatsoever and identifying α-equal terms when needed.%
\sidenote{A precise treatment is given in \arefpart{metacoq}, where we
use de Bruijn variables.}

A final important building block of all our type theories is substitution,
that we write $\subs{t}{x}{u}$. This replaces every occurrence of $x$ in $t$ by the term
$u$. Once again, we treat this operation informally, assuming it never creates
shadowing – what is sometimes called "capture-avoiding" substitution.
It is sometimes useful to substitute multiple variable at once in parallel,
which we write $\multisubs{t}{x_1 \into u_1, \dots, x_n \into u_n}$.

\section{Functional core: \kl{CCω}}
\label{sec:tech-ccw}

\AP Let us now turn to the core of CIC, namely the
\intro{Calculus of Constructions} (\kl{CCω}). Through the \kl{Curry-Howard correspondence},
it is both a typed form of λ-calculus – \ie a kind of purely functional
programming language – and a minimal form
of logic – only containing universal quantification and implication.
Since its introduction by \sidetextcite{Coquand1988}, it has been the subject of intense
theoretical study, modifications, and extensions, so let us fix what we exactly mean
with "\kl{CCω}".

\subsection{Functions and applications}

Let us start with the basic terms: functions and applications.

\begin{marginfigure}
  \begin{mathpar}
    \inferrule{\Gamma, x : A \vdash t : T}{\Gamma \vdash \l x : A .\ t : A \to T}
    \and
    \inferrule{\Gamma \vdash f : A \to T \\ \Gamma \vdash u : A }{ \Gamma \vdash t\ u : T}
  \end{mathpar}
  \caption{Typing for non-dependent functions}
  \label{fig:cic-nondep-fun}
\end{marginfigure}

Functions, also called λ-abstractions, are written $\l x : A .\ t$. This corresponds
to the mathematical notation $x \mapsto t$: the body $t$ of the function
is a term that might contain the variable $x$,
and the constructor λ abstracts over that variable to build a function.
Conversely, function application is denoted by simple juxtaposition, as in $t\ u$.
The type of functions is written $\to$, as in ordinary mathematics.
You can see those at work in \cref{fig:cic-nondep-fun}: an abstraction builds a term of arrow
type, and application needs its function to be of an arrow type,
whose domain must moreover correspond to that of the argument for it to be well-typed.
Logically, those rules make sense if $\to$ is read as implication:
if from a hypothesis $A$ one can deduce $T$, then $A \to T$ holds; conversely if $A \to T$
and $A$ both hold, then $T$ does as well.

\begin{marginfigure}
  \ContinuedFloat*
  \begin{mathpar}
    \inferdef{Abs}{\Gamma, x : A \vdash t : T}{\Gamma \vdash \l x : A .\ t : \P x : A.\ T}
    \label{rule:cic-abs}
    \and
    \inferdef{App}{\Gamma \vdash f : \P x : A.\ T \\ \Gamma \vdash u : A }{ \Gamma \vdash f\ u : \subs{T}{x}{u}}
    \label{rule:cic-app}
  \end{mathpar}
  \caption{Typing for dependent functions}
  \label{fig:cic-dep-fun}
\end{marginfigure}
These arrow types, however, are not as expressive as one could hope for.
Remember that we are in the realms of dependent types, so not only $t$ might mention $x$,
but also $T$. For instance, $T$ might be something like "$x$ is even". In such a case,
we need to record that dependency, which is the point of Π-types
– or dependent function types –, shown in \cref{fig:cic-dep-fun}.
Seen as function types, they record the fact that the codomain
might vary depending on the argument. This is reflected in the typing rule for application:
since the codomain $T$ might depend on $x$, the type of the application $f\ u$ is $T$
\emph{specialized at the argument $u$}, using substitution.
Seen on the logical side, Π-types correspond to universal quantification
$\operatorname{\forall} x : A.\ T(x)$.
Indeed, if one can show that $T(x)$ holds for an unspecified $x$,
then it must hold for all $x : A$ – this is \ruleref{rule:cic-abs}.
Conversely, if $T$ holds for all $x : A$, then one can deduce $T(u)$ for any specific
$u : A$ – this is \ruleref{rule:cic-app}.
The rules of \cref{fig:cic-nondep-fun} are just a special case
of those, in the case where the codomain $T$ does not depend
on the variable $x$, and we use this convention throughout the thesis:
$A \to T$ is shorthand for $\P x : A.\ T$ when $T$ does not mention $x$.

\AP One last thing to note about our functions is that they record the type of their
domain – what is called \intro{Church-style}
abstraction~\sidecite[][Section~3]{Barendregt1992}. There is an alternative – 
the \intro{Curry-style} abstractions –, that
does not do so, simply using $\l x.\ t$ for functions.
This difference becomes important as soon as one looks at the bidirectional structure. 
Indeed, the annotation is required if one wants to infer types for functions,
rather than barely checking them.
The \kl{Curry-style} option is sensible though,
see for instance the implementation of the proof assistant \kl{Agda} \sidecite[][p.~19]{Norell2007}, \sidetextcite{Abel2017} or \sidetextcite{McBride2022}.
In the end, this is really a design choice between being able to infer a type for any term,
or requiring annotations that in a lot of cases are useless. In this
thesis we stick with the approach used in \kl{Coq}, and annotate our abstractions.

\subsection{Universes}

To be able to express ideas like induction principles or polymorphic functions, it is
extremely useful to use functions and products quantifying over types.
This is what the universe $\uni$ are for. It is the type… of a type.
This means that the border between types and terms is not syntactic any more.
Instead, types are simply terms of type $\uni$.
Despite this, we still use upper case letters for terms which we want to think of as types.
Such a universe is called \textit{à la} Russell~\sidecite{Palmgren1998}, by contrast with
universes \textit{à la} Tarski, which regain the distinction between types and terms at
the cost of a somewhat heavier treatment of types.
Since we have not much use for a presentation \textit{à la} Tarski in this thesis,
we use the simpler one.

\begin{marginfigure}
  \ContinuedFloat
  \begin{mathpar}
    \inferdef{Univ}
    {\vdash \Gamma}
    {\Gamma \vdash \uni[i] : \uni[\unext{i}]}
    \label{rule:cic-univ}
  \end{mathpar}
  \caption{Typing for universes}
  \label{fig:cic-univ}
\end{marginfigure}

\AP There is an important caveat regarding universes.
Since the paradox exhibited by Russell in the \citetitle{Begriffsschrift},
logicians know that considering a set of all sets is a great
source of inconsistencies. Type theory is not devoid of this issue:
Girard~\sidecite[][Annex~A]{Girard1972}
shows how having a type with itself as type is inconsistent.
This inconsistency directly applies to the first dependent type system proposed by
Martin-Löf~\sidecite{MartinLoef1972}, which has a single universe $\uni$ and a rule $\uni : \uni$.
A common solution to this issue
is to stratify universes into an infinite hierarchy, which gives us \ruleref{rule:cic-univ}.
Note how $\uni$ is indexed by the \intro{universe levels} $i$ and $\unext{i}$.

\begin{marginfigure}
  \ContinuedFloat
  \begin{mathpar}
    \inferdef{Prod}
    {\Gamma \vdash A : \uni[i] \\ \Gamma, x : A \vdash B : \uni[j]}
    {\Gamma \vdash \P x : A.\ B : \uni[\umax{i}{j}]}
    \label{rule:cic-prod}
  \end{mathpar}
  \caption{Typing for product types}
  \label{fig:cic-prod}
\end{marginfigure}

\begin{marginfigure}
  \ContinuedFloat
  \begin{mathpar}
    \inferdef{EmptyCon}
    { }{\vdash \cdot}
    \label{rule:cic-empty-con} \and
    \inferdef{ConsCon}
    {\vdash \Gamma \\ \Gamma \vdash A : \uni}{\vdash \Gamma, x : A}
    \label{rule:cic-cons-con}
  \end{mathpar}
  \caption{Context well-formedness}
  \label{fig:cic-con}
\end{marginfigure}

Using those universes, \ruleref{rule:cic-prod} gives the typing rule for the product constructor. We can also now give a definition of the $\vdash \Gamma$
judgement, asserting that a context is well-formed, in \cref{fig:cic-con}.
It simply means that all its types
are indeed types. Note that in \ruleref{rule:cic-cons-con}, we did not write down a
\kl{level} for the universe, we do so to mean the existence of some unconstrained one.

\AP One last important point regarding universes is the kind of \kl{levels} used.
The simplest solution is to rely on natural number (of the meta-theory), with the $\unextsymb$
and $\umaxsymb$ operations interpreted by the usual ones.
This is however not strictly necessary: we need levels
to form an order so as to ensure we avoid inconsistency, and operations
such as $\unextsymb$ and $\umaxsymb$, but levels could very well be something
different from natural numbers.
In particular, the natural number approach fixes at which level a particular construction
is done, which is usually much more rigid than what one would wish for.
A more flexible approach, introduced under the name \intro{typical ambiguity} by
\sidetextcite{Harper1991},
uses level expressions based on level variables, rather than numbers.
This way, one can collect exactly the constraints between levels required for a
term to type-check in a consistent system, without artificially enforcing a
rigid interpretation by fixing their to a precise number once and for all.
To simplify the presentation, our default \kl{CCω}/\kl{CIC} nonetheless use integers,
but \kl{typical ambiguity} appears at multiple points in this thesis.

\section{50 Shades of Conversion}
\label{sec:tech-conversion}

\begin{marginfigure}
  \ContinuedFloat
  \begin{mathpar}
  \inferdef{Conv}
    {\Gamma \vdash t : T \\ \Gamma \vdash T \conv T' : \uni}
    {\Gamma \vdash t : T'}
  \label{rule:cic-conv}
  \end{mathpar}
  \caption{Conversion rule}
\end{marginfigure}

\AP There is one big missing part in the picture so far. Remember we are working with
dependent types, and that those can contain terms, which in turn can be seen as programs.
In the case for instance of the vector type we used in the introduction (and that we are
about to introduce formally), what happens if a function expects an argument of type
$\Vect(A,3)$, but it is given an argument the output of a concatenation function,
which naturally has type $\Vect(A,(2+1))$?
Surely we must have a way to relate both, since after all
the small program $2+1$ ought to compute $3$! This is exactly what
\ruleref{rule:cic-conv}%
\sidenote{This wraps up our typing
rules for \kl{CCω}, collected in \cref{fig:ccw-typing}.}
is for: it allows to replace a type $T$ with one that is related to it by
\intro{conversion} – written $\conv$.
As usual, there are two ways to look at this relation. From the point of view of programs,
it incorporates a computational aspect directly inside the type system.
From the point of view of logics, it corresponds to types being the same "by definition"
rather than due to some reasoning
– which is why conversion is also called definitional equality or judgmental equality.
In our vector example, for instance, the two types are the same by the definition of addition.

\begin{figure*}[ht]
  \LastFloat

  \begin{mathpar}
    %
    \jform{\vdash \Gamma}
    \inferdef{EmptyCon}
      { }{\vdash \cdot}
    \and
    \inferdef{ConsCon}
      {\vdash \Gamma \\ \Gamma \vdash A : \uni}{\vdash \Gamma, x : A}
    \\\\
    \jform{\Gamma \vdash t : T}
    \inferdef{Var}{(x : A) \in \Gamma \\ \vdash \Gamma}{\Gamma \vdash x : A}
    \and
    \inferdef{Univ}
      {\vdash \Gamma}
      {\Gamma \vdash \uni[i] : \uni[\unext{i}]}
    \and
    \inferdef{Prod}
      {\Gamma \vdash A : \uni[i] \\ \Gamma, x : A \vdash B : \uni[j]}
      {\Gamma \vdash \P x : A.\ B : \uni[\umax{i}{j}]}
    \and
    \inferdef{Abs}{\Gamma, x : A \vdash t : T}{\Gamma \vdash \l x : A .\ t : \P x : A.\ T}
    \and
    \inferdef{App}
      {\Gamma \vdash f : \P x : A.\ T \\ \Gamma \vdash u : A }
      {\Gamma \vdash f\ u : \subs{T}{x}{u}}
    \and
  \inferdef{Conv}
    {\Gamma \vdash t : T \\ \Gamma \vdash T \conv T' : \uni}
    {\Gamma \vdash t : T'}
  \end{mathpar}

  \caption{Collected typing rules for \kl{CCω}}
  \label{fig:ccw-typing}
\end{figure*}

Conversion is a very complex relation, arguably the most subtle part of dependent types.
Consequently, there are very different ways to present it, which in turn serve different
needs.
For this reason, we took care to set the typing rules of
\cref{fig:ccw-typing} up so that nothing has to
be changed in those when one definition of conversion or another is taken. The only
difference is in how the relation $\Gamma \vdash T \conv T' : \uni$ is defined.
This way, we can treat conversion as a black box when talking about typing,
making the theory modular.

\AP A first important divide is between \intro(conv){typed} and
\intro(conv){untyped} conversion.
On one side, conversion is seen as an intrinsically typed relation: terms are only convertible
\emph{at a given type}. On the other, conversion is a relation between raw terms,
that does not presuppose any form of typing. \Cref{fig:typed-untyped-conv} gives an
example of the computation rule for functions in both systems.
The "content" of the two rules is the same – they equate $(\l x : A.\ t)\ u$
and $\subs{t}{x}{u}$ – only the side-conditions differ wildly.
\kl{Typed conversion} goes back to the type theory of
\sidetextcite{MartinLoef1972}, and is a recurring feature in its many descendants.
\kl{Untyped conversion} relates strongly to (untyped) λ-calculus – Barendregt
for instance uses the name "conversion" for the equational theory of untyped λ-calculus
in his reference work on the subject~\sidecite{Barendregt1985} –, via
the \kl{Pure Type Systems} (\intro{PTS})~\sidecite{Barendregt1991} literature.
In this thesis, we mainly consider untyped conversion, as \kl{Coq}’s meta-theory
has been mostly studied in that tradition.
But the relation between both in the context of
bidirectional typing is the main subject of \cref{chap:bidir-conv}.

\begin{figure}[ht]
  \begin{mathpar}
    \inferrule
      {\Gamma, x : A \vdash t : B \\ \Gamma \vdash u : A}
      {\Gamma \vdash (\l x : A.\ t)\ u \tdconv \subs{t}{x}{u} : \subs{B}{x}{u}}
    \and
    \inferrule{ }{(\l x : A.\ t)\ u \udconv \subs{t}{x}{u}}
  \end{mathpar}
  \caption{Example: typed and untyped β rule for conversion}
  \label{fig:typed-untyped-conv}
\end{figure}

A second axis is about how close the conversion relation is to an implementation.
For instance, conversion should be an equivalence relation,
but there are two approaches to that. The first – and standard – one
is to simply \emph{define} conversion as an equivalence relation, by adding rules 
for \eg transitivity, as the one of \cref{fig:trans-conv}.
\begin{marginfigure}
  \begin{mathpar}
    \inferrule
      {t \udconv t' \\ t' \udconv t''}
      {t \udconv t''}
  \end{mathpar}
  \caption{Example: transitivity rule for conversion}
  \label{fig:trans-conv}
\end{marginfigure}
This ensures that conversion has the right properties, but means it does not directly correspond
to an algorithm, as this transitivity rule cannot be directly implemented.
% , since its middle term is not recorded in any place.
The λ-calculus theorists have known this issue for a long time, and they
have a solution: characterizing conversion by means of a \kl{reduction} relation $\red$, which
corresponds to the idea of program evaluation – see \sidetextcite{Barendregt1985} for
instance. If this reduction is well-behaved,
%\sidenote{The main one being confluence.}
then two terms are convertible exactly when they reduce to the same third term.
This more operational characterization is closer to what can be implemented.
Turning things around, one can define conversion through reduction,
and only \emph{show} in retrospect that it has the good properties
that were enforced in the first approach – typically, that it is transitive.
\AP Conversion of the first kind we call \intro{declarative conversion}, while for the second
we talk about \intro{algorithmic conversion}.

In the rest of this section we give two presentations of \kl{untyped conversion}.
First, a \kl(conv){declarative} one, which we use to define \kl{CCω}, as is the standard.
Second, an \kl(conv){algorithmic} one, anticipating the need for it later on
in \arefpart{metacoq} and \arefpart{gradual}.
% in \arefpart{metacoq} where it is used to show decidability of type-checking, and
% in \arefpart{gradual}, where we extend it into a relation that is by design not transitive, so
% that basing it on declarative conversion would be nonsensical.

\subsection{Declarative conversion}

\begin{marginfigure}
  \ContinuedFloat*
  \begin{mathpar}
    \inferdef{UConv}{\Gamma \vdash T' : \uni \\ T \udconv T'}{\Gamma \vdash T \conv T' : \uni}
    \label{rule:cic-conv-unty}
  \end{mathpar}
  \caption{Typing constraint on untyped conversion}
\end{marginfigure}

To start our presentation of \kl{untyped conversion},
let us first go back to \ruleref{rule:cic-conv}.
Even if we wish to describe conversion as
an untyped relation, we still enforce a typing constraint in \ruleref{rule:cic-conv},
in order to ensure that, whenever $\Gamma \vdash t : T$ is derivable,
$\Gamma \vdash T : \uni$ is as well.
This is exactly the content of \ruleref{rule:cic-conv-unty}, which combines conversion
with a check that the target type is indeed a well-formed type.

\begin{marginfigure}
  \ContinuedFloat
  \begin{mathpar}
    \inferdef{βConv}{ }{(\l x : A.\ t)\ u \udconv \subs{t}{x}{u}}
    \label{rule:cic-uconv-beta}
  \end{mathpar}
  \caption{Computation rule for functions}
\end{marginfigure}

Regarding conversion itself, the first rule is \ruleref{rule:cic-uconv-beta},
which corresponds to the computational behaviour
of functions: the variable of an applied λ-abstraction is replaced by the argument, using
substitution.

The rest of the rules ensure conversion has the properties it should. First are the
ones ensuring it forms an equivalence relation: it
is reflexive (\ruleref{rule:cic-uconv-refl}), symmetric (\ruleref{rule:cic-uconv-sym}),
and transitive (\ruleref{rule:cic-uconv-trans}).

\begin{figure}[ht]
  \ContinuedFloat
  \begin{mathpar}
    \inferdef{ConvRefl}{ }{t \udconv t}
    \label{rule:cic-uconv-refl} \and
    \inferdef{ConvSym}{t \udconv t'}{t' \udconv t}
    \label{rule:cic-uconv-sym} \and
    \inferdef{ConvTrans}
      {t \udconv t' \\ t' \udconv t''}
      {t \udconv t''}
    \label{rule:cic-uconv-trans}
  \end{mathpar}
  \caption{Equivalence rules}
  \label{fig:cic-uconv-equiv}
\end{figure}

A second set of rules, collected in \cref{fig:cic-uconv-cong},
asserts that conversion is a congruence, meaning that it is compatible
with all term formers. As for the previous three, these correspond to properties we expect
from the conversion relation, that we simply declare to be true. Note that we include only
congruence rules for term formers with sub-terms – we \eg omit $\uni$. To be exhaustive,
we could have included congruence rules for all term formers, but when they have no
subterm congruence is simply a special cases of \ruleref{rule:cic-uconv-refl}.
Conversely, we could omit \ruleref{rule:cic-uconv-refl}
altogether and derive it from congruence rules,
which can be seen as a generalized form of reflexivity.

\begin{figure}[hb]
  \ContinuedFloat
  \begin{mathpar}
    \inferrule
    % \inferdef{ProdConv}
      {A \udconv A' \\ B \udconv B'}
      {\P x : A.\ B \udconv \P x : A'.\ B'}
    % \label{rule:cic-uconv-prod}
    \and
    \inferrule
    % \inferdef {AbsConv}
      {A \udconv A' \\ t \udconv t'}
      {\l x : A .\ t \udconv \l x : A'.\ t'}
    % \label{rule:cic-uconv-abs}
    \and
    % \inferdef{AppConv}
    \inferrule
      {f \udconv f' \\ u \udconv u' }
      {f\ u \udconv f'\ u'}
    % \label{rule:cic-uconv-app}
  \end{mathpar}
  \caption{Congruence rules}
  \label{fig:cic-uconv-cong}
\end{figure}

\subsection{Algorithmic conversion}

\AP Before we can describe \kl{algorithmic conversion}, we first need
to give a look at \intro{reduction}. Reduction is in some way an operational version of
conversion. The main difference is that it is oriented, in the direction which would
correspond to program evaluation. It itself decomposes into three components.

\begin{marginfigure}
  \ContinuedFloat*
  \begin{mathpar}
    \inferdef{βRed}{ }{(\l x : A.\ t)\ u \tred \subs{t}{x}{u}}
    \label{rule:beta-red}
  \end{mathpar}
  \caption{Top-level reduction}
\end{marginfigure}

\AP The first is \intro{top-level
reduction} $\tred$, which corresponds purely to computation, without any congruent closure.
In \kl{CCω} there is only the single \ruleref{rule:beta-red}.

\AP The second component is the congruent closure of top-level reduction,
\intro{one-step reduction} $\ored$, which allows triggering top-level reduction exactly once,
but at any position in a term. Its definition is given in \cref{fig:ccw-ored}, as
the congruent closure of \kl(red){top-level} reduction.
Note that while we talk about congruent closure both for
\kl(decl){conversion} (\cref{fig:cic-uconv-cong})
and \kl{one-step reduction}, we mean a different form of closure:
in the case of conversion, we demand the relation to hold in all sub-terms,
while for one-step reduction it is allowed in exactly one sub-term.

\begin{figure}[ht]
  \ContinuedFloat
  \begin{mathpar}
    \inferrule
    % \inferdef{TopRed}
      {t \tred t'}
      {t \ored t'}
    % \label{rule:top-red}
    \and
    \inferrule
    % \inferdef{ProdRedDom}
      {A \ored A'}
      {\P x : A.\ B \ored \P x : A'.\ B}
    % \label{rule:red-prod-dom}
    \and
    \inferrule
    % \inferdef{ProdRedCod}
      {B \ored B'}
      {\P x : A.\ B \ored \P x : A.\ B'}
    % \label{rule:red-prod-cod}
    \and
    \inferrule
    % \inferdef{AbsRedDom}
      {A \ored A'}
      {\l x : A .\ t \ored \l x : A'.\ t}
    % \label{rule:red-abs-dom}
    \and
    \inferrule
    % \inferdef{AbsRedBod}
      {t \ored t'}
      {\l x : A .\ t \ored \l x : A.\ t'}
    % \label{rule:red-abs-bod}
    \and
    \inferrule
    % \inferdef{AppRedFun}
      {f \ored f'}
      {f\ u \ored f'\ u}
    % \label{rule:red-app-fun}
    \and
    \inferrule
    % \inferdef{AppRedArg}
      {u \ored u'}
      {f\ u \ored f\ u'}
    % \label{rule:red-app-arg}
  \end{mathpar}
  \caption{One-step reduction}
  \label{fig:ccw-ored}
\end{figure}

Finally, we obtain \kl{reduction} as the reflexive
transitive closure of one-step reduction, see \cref{fig:red}.

\begin{figure}[ht]
  \ContinuedFloat
  \begin{mathpar}
    \inferrule
    % \inferdef{TopRed}
      {t \tred t'}
      {t \ored t'} \and
    % \label{rule:top-red}
    \inferrule{ }{t \fred t}
    \and
    \inferrule
      {t \ored t' \\ t' \fred t''}
      {t \fred t''}
  \end{mathpar}
  \caption{Reduction}
  \label{fig:red}
\end{figure}

With reduction out of the way, we can finally get to \kl{algorithmic conversion}: two terms
are convertible whenever they reduce to terms that are \kl{α-equal}.
As for declarative conversion,
we impose a typing condition on the target type. Altogether, this leads to
\ruleref{rule:alg-conv}. For once, we make α-equality explicit to anticipate
its replacement by more complex relations later on.

\begin{figure}[ht]
  \ContinuedFloat
  \begin{mathpar}
    \inferdef{AlgConv}
    {\Gamma \vdash T : \uni \\ T \red U \\ T' \red U' \\ U \alpheq U' }
    {\Gamma \vdash T \conv T' : \uni}
    \label{rule:alg-conv}
  \end{mathpar}
  \caption{Algorithmic conversion}
\end{figure}

To wrap up this section, let us backtrack for a moment on the reason why we separate
the definition of reduction in three layers. The reason is that as we defined it, reduction
is somewhat too unconstrained.\sidenote{In particular, it is non-deterministic.}
In what follows, a recurring need is that of a deterministic notion of reduction 
which is able to expose a canonical term former, if it exists.
\AP There is a way to do so, what is called
\intro{weak-head reduction} $\hred$. It amounts to restricting the place in a term where
\kl{top-level reduction} can be used, by removing some congruence rules compared to reduction. More precisely, λ-abstractions, product types and universes are not reduced further,
as they already are canonical forms of their types.
Variables are not reduced either, since they simply cannot be.
Thus, the only reduction that is allowed is in the function position of an application,
with the hope to get a λ-abstraction there that can be further reduced using top-level reduction.
In the end, we get \cref{fig:wh-red}.
When we want to contrast this \kl{weak-head reduction} with the
previously defined one $\red$, we call the latter \intro{full reduction}.
\begin{figure}[ht]
  \begin{mathpar}
    \inferrule
    % \inferdef{TopRed}
      {t \tred t'}
      {t \hored t'}
    % \label{rule:top-red}
    \and
    \inferrule
      {f \hored f'}
      {f\ u \hored f'\ u}
    \and
    \inferrule{ }{t \hred t}
    \label{rule:red-refl} \and
    \inferrule
      {t \hored t' \\ t' \hred t''}
      {t \hred t''}
    \label{rule:red-trans}
  \end{mathpar}
  \caption{Weak-head reduction}
  \label{fig:wh-red}
\end{figure}

\section{The Good Properties}
\label{sec:tech-properties}

\begin{itemize}
  \item Weakening, substitution
  \item Validity, principality
  \item SR (confluence, injectivity of type constructors)
  \item Normalization
  \item Progress, canonicity, consistency
\end{itemize}


\section{Adding Inductive Types: \kl{CIC}}
\label{sec:tech-cic}

Of course, not everything in mathematics of programming is a function. In particular,
it is missing basic objects like natural numbers.
\kl{CCω} is powerful enough to enable encodings of many of those objects, the natural
numbers for instance.
However, this is not satisfactory: \sidetextcite{Geuvers2001} shows
that whatever the encoding, it is impossible to retrieve a type of natural numbers
satisfying an induction principle, which is however their defining characteristic!
Because of this and other limitations of encodings, and in order to faithfully
represent the use of induction in mathematics and pattern-matching in programming languages,
\intro{inductive types} have been introduced by \sidetextcite{PaulinMohring1993}.
Adding these to \kl{CCω} results in \intro{CIC},
the \kl{Calculus of Inductive Constructions}.

\subsection{Booleans}

Let us start with a very simple example: \intro{booleans}.
To add them to \kl{CCω}, we need to specify three new kinds of term formers.
The first is the type, that we write $\Bool$.
Next we need \intro{constructors}, giving the canonical inhabitants of the type.
In the case of booleans, there are two of them: the false boolean $\false$ and the true one $\true$.
The last one is a way to use those canonical inhabitants. For booleans,
this corresponds to a conditional,
taking one branch or another depending on the value of the term being used,
whose typing rule is given in \ruleref{rule:bool-ind}.%
\sidenote{In that term former, we call $s$ the \emph{scrutinee}, $P$ the \emph{predicate}
and $b_{\false}$, $b_{\true}$ the \emph{branches}.}
As was the case for dependent functions, here also there is a generalization with respect to
conditionals in usual programming languages: the return type itself can depend on the scrutinee.
The usual if-then-else conditional with the same type in both branches is a special case,
when $P$ does not depend on the variable $z$.
The name $\indop$ itself is generic for all inductive
types, and stands for induction, as this dependently-typed conditional gives a form of
induction principle. Indeed, one can read
$\ind{\Bool}{s}{z.P}{b_{\false},b_{\true}}$ as case distinction on the scrutinee:
to prove that $P$ holds for an arbitrary boolean $s$, it suffices to show that both
$\subs{P}{z}{\false}$ and $\subs{P}{z}{\true}$ do – these are respectively proven by
$b_{\false}$ and $b_{\true}$. The name induction is not really
suitable here because we only have base cases and no induction step, but we get those
as soon as the inductive type itself is recursive.

\begin{figure}[ht]
  \ContinuedFloat*
  \begin{mathpar}
    \inferdef{BoolTy}{\vdash \Gamma}{\Gamma \vdash \Bool : \uni[0]} \and
    \inferdef{False}{\vdash \Gamma}{\Gamma \vdash \false : \Bool} \and
    \inferdef{True}{\vdash \Gamma}{\Gamma \vdash \true : \Bool} \and
    \inferdef{BoolInd}
      {\Gamma \vdash s : \Bool \\
      \Gamma, z : \Bool \vdash P : \uni \\
      \Gamma \vdash b_{\false} : \subs{P}{z}{\false} \\
      \Gamma \vdash b_{\true} : \subs{P}{z}{\true}}
      {\Gamma \vdash \ind{\Bool}{s}{z.P}{b_{\false},b_{\true}} : \subs{P}{z}{s}}
      \label{rule:bool-ind}
  \end{mathpar}
  \caption{Typing rules for booleans}
  \label{fig:bool-typ}
\end{figure}

\begin{marginfigure}
  \ContinuedFloat
  \begin{mathpar}
    \inferdef{ιFalse}
    { }
    {\ind{\Bool}{\false}{z.P}{b_{\false},b_{\true}} \tred b_{\false}} \and
    \inferdef{ιTrue}
    { }
    {\ind{\Bool}{\true}{z.P}{b_{\false},b_{\true}} \tred b_{\true}}
  \end{mathpar}
  \caption{\kl{Top-level reduction} for booleans (ι-reduction)}
  \label{fig:bool-red}
\end{marginfigure}

One thing is still missing in this picture: computation. The extension to
\kl{top-level reduction} is given in \cref{fig:bool-red} – our first example of
ι-reduction, the name we use globally to refer to reduction rules of inductive types.
These rules simply pick the branch corresponding to the scrutinee,
which is not very surprising when $\indop_{\Bool}$ is understood as a conditional.
\kl{Declarative conversion} can be extended in exactly the same way.
Finally, to account for the arguments of the newly introduced term former $\indop_{\Bool}$,
we need to add new congruence rules, see \cref{fig:bool-cong}.
For \kl{one-step reduction} and \kl{declarative conversion},
there is no subtlety, all positions behave the same. The interesting rule is the
one for \kl{weak-head reduction}: there is only one congruence rule, which allows for
reduction of the scrutinee. This is similar to functions, we allow reduction in the
place in the term that triggers a computation if it is a canonical form – in this 
case, a constructor $\false$ or $\true$.

\begin{figure*}[ht]
  \ContinuedFloat
  \begin{mathpar}
    \inferrule
    {s \udconv s' \\ P \udconv P' \\ b_{\false} \udconv b'_{\false} \\ b_{\true} \udconv b'_{\true}}
    {\ind{\Bool}{s}{z.P}{b_{\false},b_{\true}}
      \udconv \ind{\Bool}{s'}{z.P'}{b'_{\false},b'_{\true}}} \and
    \inferrule
    {s \ored s'}
      {\ind{\Bool}{s}{z.P}{b_{\false},b_{\true}}
        \ored \ind{\Bool}{s'}{z.P}{b_{\false},b_{\true}}} \and
    \inferrule
    {P \ored P'}
      {\ind{\Bool}{s}{z.P}{b_{\false},b_{\true}}
        \ored \ind{\Bool}{s}{z.P'}{b_{\false},b_{\true}}} \and
    \inferrule
    {b_{\false} \ored b'_{\false}}
      {\ind{\Bool}{s}{z.P}{b_{\false},b_{\true}}
        \ored \ind{\Bool}{s}{z.P}{b'_{\false},b_{\true}}} \and 
    \inferrule
    {b_{\true} \ored b'_{\true}}
      {\ind{\Bool}{s}{z.P}{b_{\false},b_{\true}}
        \ored \ind{\Bool}{s}{z.P}{b_{\false},b'_{\true}}} \and
    \inferrule
    {s \hored s'}
      {\ind{\Bool}{s}{z.P}{b_{\false},b_{\true}}
        \hored \ind{\Bool}{s'}{z.P}{b_{\false},b_{\true}}} \and
  \end{mathpar}

  \caption{Congruence rules for booleans}
  \label{fig:bool-cong}
\end{figure*}



\subsection{Recursion}

Booleans are very simple, but we of course want more. The first thing to add is recursion.
Here the easiest example is that of natural numbers, given in \cref{fig:nat}.
The rules are more verbose than those for booleans, but the general idea is very similar:
\ruleref{rule:nat-type} introduces a new type, \ruleref{rule:zero-type}
and \ruleref{rule:succ-type} its constructors, and \ruleref{rule:nat-ind} its induction
principle. This time the induction principle is a real induction, as we can see in
the second branch, where there is an induction hypothesis $p_{y}$ on the predecessor $y$.
As for booleans, the induction principle reduces when applied to a constructor.
But, again since we have real recursion,
a recursive call appears in the reduct of \ruleref{rule:iota-succ}.
We do not repeat the congruence rules, as they are similar to those for booleans
(\cref{fig:bool-cong}). The only difference is that now there is also a need for congruence
rules for the term former $\Sop$, as it takes an argument.

\begin{figure}[h]
  \begin{mathpar}
    \inferdef{Nat}{\vdash \Gamma}{\Gamma \vdash \Nat : \uni[0]}
    \label{rule:nat-type} \and
    \inferdef{Zero}{\vdash \Gamma}{\Gamma \vdash \z : \Nat}
    \label{rule:zero-type} \and
    \inferdef{Succ}{\Gamma \vdash n : \Nat}{\Gamma \vdash \S{n} : \Nat}
    \label{rule:succ-type} \and
    \inferdef{NatInd}
      {\Gamma \vdash s : \Nat \\
      \Gamma, z : \Nat \vdash P : \uni \\
      \Gamma \vdash b_{\z} : \subs{P}{z}{\z} \\
      \Gamma, y : \Nat, p_{y} : \subs{P}{z}{y} \vdash b_{\Sop} : \subs{P}{z}{\S{y}}}
      {\Gamma \vdash \ind{\Nat}{s}{z.P}{b_{\z},y.p_{y}.b_{\Sop}} : \subs{P}{z}{s}}
    \label{rule:nat-ind} \and
    \inferdef{ιZero}
    { }
    {\ind{\Nat}{\z}{z.P}{b_{\z},y.p_{y}.b_{\Sop}} \tred b_{\z}} 
    \label{rule:iota-zero} \and
    \inferdef{ιSucc}
    { }
    {\ind{\Nat}{\S{n}}{z.P}{b_{\z},y.p_{y}.b_{\Sop}} \tred\\
      \multisubs{b_{\Sop}}{y \into n, p_{y} \into \ind{\Nat}{n}{z.P}{b_{\z},y.p_{y}.b_{\Sop}}}}
    \label{rule:iota-succ}
  \end{mathpar}
  \caption{Natural numbers}
  \label{fig:nat}
\end{figure}

At this point, it might be good to add a note on the way we represent constructors:
we enforce them to be fully applied, meaning $\Sop$ does not make sense on its own
as a term. \kl{Coq} is slightly more permissive, and allows $\Sop : \Nat \to \Nat$.
We forbid this – but one can always consider $\l x : \Nat.\ \S{x}$ instead if needed.
Likewise, inductive types are also enforced to be fully applied.
We also avoid using the $\P$ and $\l$ term formers to represent binding in the predicate
and branches of constructors, rather using contexts directly.
This allows for a clear separation
of concerns, by reducing interactions between the functional fragment and
inductive types. \kl{Coq}'s kernel used to rely on $\P$ and $\l$ abstractions,
but a version closed to our presentation has recently replaced it%
\sidenote{The exact change is documented by pull-request \coqPR{13563}.}, 
in part due to concerns raised while working on this thesis.

\subsection{Parameters}

A second direction for enhancement is the ability to have inductive types with parameters.
The main use of this is for type constructors, for instance the pair type $A \times B$
that appears in \cref{chap:intro-fr,chap:intro-en}. As is probably not very surprising by
now, this type is a specific instance of a more generic variant, the
dependent pair type $\Sb x : A.\ B$. Logically, the dependency of $B$ on $A$ means
that if we see it as a property, the whole pair type describes a subset of $A$ – those elements
which validate $B$. The rules are given in \cref{fig:sig}.
As for functions we need an annotation on
the pair constructor, for the exact same reason: we want to ensure that any term can
infer a type. We also omit congruence rules, as they are again similar to that of
\cref{fig:bool-cong}, although now not only the pair constructor but also the type constructor
$\Sb$ get their congruence rules, since both have arguments.

\begin{figure}[h]
  \begin{mathpar}
    \inferdef{PairTy}
      {\vdash A : \uni[i] \\ \Gamma, x : A \vdash B : \uni[j]}
      {\Gamma \vdash \Sb x : A.\ B : \uni[\umax{i}{j}]}
    \label{rule:sig-type} \and
    \inferdef{Pair}
    {\Gamma \vdash A : \uni \\ \Gamma, x : A \vdash B : \uni \\
    \Gamma \vdash t : A \\ \Gamma \vdash u : \subs{B}{x}{t}}
    {\Gamma \vdash \pair[A][B]{t}{u} : \Sb x : A.\ B}
    \label{rule:pair-type} \and
    \inferdef{PairInd}
      {\Gamma \vdash s : \Sb x : A.\ B \\
      \Gamma, z : \Sb x : A.\ B \vdash P : \uni \\
      \Gamma, y_1 : A, y_2 : \subs{B}{x}{y_1} \vdash b : \subs{P}{z}{\pair[A][B]{y_1}{y_2}}}
      {\Gamma \vdash \ind{\Sb}{s}{z.P}{b} : \subs{P}{z}{s}}
      \label{rule:sig-ind} \and
    \inferdef{ιPair}
    { }
    {\ind{\Sb}{\pair[A][B]{t}{u}}{z.P}{b} \tred \multisubs{b}{y_1 \into t, y_2 \into u}} 
    \label{rule:iota-sig}
  \end{mathpar}
  \caption{Inductive dependent pair type}
  \label{fig:sig}
\end{figure}

As an example which combines both recursion and parameters, we have the polymorphic list
type $\List$. It should not come as very surprising, as it mainly combines what
we already covered for natural numbers and pairs.

\begin{figure}[h]
\begin{mathpar}
  \inferdef{ListTy}
    {\vdash A : \uni[i]}
    {\Gamma \vdash \List(A) : \uni[i]}
  \label{rule:list-type} \and
  \inferdef{Nil}
    {\Gamma \vdash A : \uni}
    {\Gamma \vdash \lnil[A] : \List(A)}
  \label{rule:nil-type} \and
  \inferdef{Cons}
    {\Gamma \vdash A : \uni \\ 
    \Gamma \vdash a : A \\ \Gamma \vdash l : \List(A)}
    {\Gamma \vdash \lcons[A]{a}{l} : \List(A)}
  \label{rule:cons-type} \and
  \inferdef{ListInd}
    {\Gamma \vdash s : \List(A) \\
    \Gamma, z : \List(A) \vdash P : \uni \\
    \Gamma \vdash b_{\lnil} : \subs{P}{z}{\lnil} \\
    \Gamma, y_1 : A, y_2 : \List(A), p_{y_2} : \subs{P}{z}{y_2}
      \vdash b_{\lconsop} : \subs{P}{z}{\lcons[A]{y_1}{y_2}}}
    {\Gamma \vdash \ind{\List}{s}{z.P}{b_{\lnil},y_1.y_2.p_{y_2}.b_{\lconsop}}
      : \subs{P}{z}{s}}
  \label{rule:list-ind} \and
  \inferdef{ιNil}
    { }
    {\ind{\List}{\lnil[A]}{z.P}{b_{\lnil},y_1.y_2.p_{y_2}.b_{\lconsop}} \tred b_{\lnil}} 
  \label{rule:iota-nil} \and
  \inferdef{ιCons}
    { }
    {\ind{\List}{\lcons[A]{a}{l}}{z.P}{b_{\lnil},y_1.y_2.p_{y_2}.b_{\lconsop}} \tred \\
      \multisubs{b_{\lconsop}}{y_1 \into a, y_2 \into l, p_{y_2} \into \ind{\List}{l}{z.P}{b_{\lnil},y_1.y_2.p_{y_2}.b_{\lconsop}}}}
  \label{rule:iota-cons}
\end{mathpar}
\caption{List type}
\label{fig:list}
\end{figure}

Inductive types using recursion and parameters we call \intro{datatypes},
as they roughly correspond to what one can find in usual programming languages.
For instance, they are closely related to the algebraic types of the \kl{OCaml} language.

\subsection{Indices}

\begin{figure}
  \begin{mathpar}
  \inferdef{EqType}
    {\Gamma \vdash A : \uni[i] \\ \Gamma \vdash a : A \\ \Gamma \vdash a' : A}
    {\Gamma \vdash \eqty[A]{a}{a'} : \uni[i]}
  \label{rule:eq-type}
  \and
  \inferdef{EqRefl}
    {\Gamma \vdash A : \uni[i] \\ \Gamma \vdash a : A}
    {\Gamma \vdash \refl[A][a] : \eqty[A]{a}{a}}
  \label{rule:eq-refl}
  \and
  \inferdef{EqInd}
    {\Gamma \vdash s : \eqty[A]{a}{a'} \\
    \Gamma, y : A, z : \eqty[A]{a}{y} \vdash P : \uni \\
    \Gamma \vdash b : \multisubs{P}{y \into a, z \into \refl[A][a]}}
    {\Gamma \vdash \ind{\eqtyop}{s}{y.z.P}{b} : \multisubs{P}{y \into a', z \into s}}
  \label{rule:eq-ind} \and
  \inferdef{ιEq}
    { }
    {\ind{\eqtyop}{\refl[A][a]}{y.z.P}{b} \tred b}
  \label{rule:eq-iota}
  \end{mathpar}
  \caption{Equality type}
  \label{fig:eq-type}
\end{figure}

There is one last feature missing in the previous inductive types. In datatypes,
the return types of constructors are always the same. In some way, datatypes do not exploitable
the fact that we deal with dependent types. What if we wanted constructors
to specify that they inhabit a type at some specific value? This is exactly the point of
indexed inductive types.

The paradigmatic example here is (propositional) equality, an
inductive meant to represent equality \emph{internally} to the logic, \ie as a notion on
which one can reason (for instance, do proofs by induction), rather than an external one
such as conversion.
Rules for equality are given in \cref{fig:eq-type}. \ruleref{rule:eq-type} is fairly
non-surprising, apart from the fact that it takes not only a type as a parameter, but
also a term. \ruleref{rule:eq-refl} is already more interesting. Here we can see that
the second argument of type $a$ is fixed to be $a$ by the constructor. This gets
more visible in \ruleref{rule:eq-ind}: in order for the single branch $b$
to be typeable, the predicate needs to abstract not only on the scrutinee,
but also on that second argument.

As for the logical interpretation, in the simplified case where $P$ only depends on the index,
\ruleref{rule:eq-ind} corresponds to the idea that equal terms should be indiscernible:
whenever both $\eqty[A]{a}{a'}$ and $\subs{P}{y}{a}$ hold, then so does $\subs{P}{y}{a'}$.
In words, every property true of $a$ is also true of $a'$. Paired with the power of
dependent types this presentation of equality gives rise to a very rich theory, and
forms the basis for the whole line of research in Homotopy Type Theory \sidecite{UniFoundationsProgram2013}.

However, in the context of bare \kl{CIC}, this richness is also
a curse, and indexed inductive types can be very tricky to handle. In particular, the
gradual extension of \arefpart{gradual} do not extend well to generic inductive types.
There is, however, a somewhat simpler kind of inductive types, where the indices are
not any term of any type – as in the case of equality –, but inhabitants of a datatype.
Such a case is often easier to handle, and is often sufficient, especially for
dependently-typed programming. The prototypical example here is that of vectors, which
we have already encountered in \cref{chap:intro-fr,chap:intro-en}, and are given in
detail in \cref{fig:vect}. They are similar to
lists, but with an natural number index which records the length of the vector in its type.
This allows for finely-grained specification, for instance a head function that takes as input 
a vector of length at least one, and is thus ensured to never fail on an empty vector
by mere virtue of its type.

\begin{figure}[h]
  \begin{mathpar}
    \inferdef{VectType}
      {\vdash A : \uni[i] \\ \vdash n : \Nat}
      {\Gamma \vdash \Vect(A,n) : \uni[i]}
    \label{rule:vect-type} \and
    \inferdef{Vnil}
      {\Gamma \vdash A : \uni}
      {\Gamma \vdash \vnil[A] : \Vect(A,\z)}
    \label{rule:vnil-type} \and
    \inferdef{Vcons}
      {\Gamma \vdash A : \uni \\ \Gamma \vdash n : \Nat \\
      \Gamma \vdash a : A \\ \Gamma \vdash l : \Vect(A,n)}
      {\Gamma \vdash \vcons[A][n]{a}{l} : \Vect(A,\S n)}
    \label{rule:vcons-type} \and
    \inferdef{VectInd}
      {\Gamma \vdash s : \Vect(A,n) \\
      \Gamma, y : \Nat, z : \Vect(A,y) \vdash P : \uni \\
      \Gamma \vdash b_{\vnil} : \multisubs{P}{y \into 0, z \into \vnil} \\
      \Gamma, y_1 : \Nat, y_2 : A, y_3 : \Vect(A,y_1), p_{y_3} \vdash
        b_{\vconsop} : \multisubs{P}{y \into z, y_2 \into \vcons[A][y_1]{y_2}{y_3}}}
      {\Gamma \vdash \ind{\Vect}{s}{y.z.P}{b_{\vnil},y_1.y_2.y_3.p_{y_3}.b_{\vconsop}}
        : \subs{P}{z}{s}}
    \label{rule:vect-ind} \and
    \inferdef{ιVnil}
      { }
      {\ind{\Vect}{\vnil[A]}{y.z.P}{b_{\vnil},y_1.y_2.y_3.p_{y_3}.b_{\vconsop}}
        \tred b_{\lnil}} 
    \label{rule:iota-vnil} \and
    \inferdef{ιVcons}
      { }
      {\ind{\Vect}{\vcons[A][n]{a}{l}}{y.z.P}{b_{\vnil},y_1.y_2.y_3.p_{y_3}.b_{\vconsop}} \tred \\
        \multisubs{b_{\vconsop}}{y_1 \into n, y_2 \into a, y_3 \into l, p_{y_3} \into \ind{\Nat}{l}{z.P}{b_{\lnil},y_1.y_2.y_3.p_{y_3}.b_{\vconsop}}}}
    \label{rule:iota-vcons}
  \end{mathpar}
  \caption{Vector type}
  \label{fig:vect}
  \end{figure}

\subsection{The \kl{Calculus of Constructions}}

So far we only gave examples of the inductive types one could wish for.
A description of how to generally define inductive types
in a way that keeps the good properties of the system would be very verbose and not
very enlightening at this point. Let us simply say that the main restriction – barring typing
constraints – is to ensure, through a criterion called (strict) positivity,
that the recursive structure of the inductive type
is well-founded, so that positing its existence does not allow for the
introduction of paradoxes.
On paper, rather than a hard-to-read general presentation we reuse the previous
set of examples to show how our setting adapts to inductive types in their
three main complexities – recursion, parameters and indices.
The formalizations handle the general case, in the even more complex setting of \kl{PCUIC}
presented in \cref{sec:tech-pcuic}.
In the end, when we talk about \kl{CIC} we mean the extension of \kl{CCω} with any number of
valid inductive types in the previous sense,
and by \intro{CIC-} the restriction of \kl{CIC} to datatypes,
\eg the exclusion of indexed inductive types.

\section{Beyond \kl{CIC}: \kl{PCUIC}}
\label{sec:tech-pcuic}

\kl{CIC} as described in the previous section is already very expressive and powerful.
However, it is nevertheless
still far from the type theory implemented in \kl{Coq} and
formalized in \kl{MetaCoq}, the \intro{Polymorphic, Cumulative Calculus of Inductive Constructions} (\kl{PCUIC}).
As some of these additions are discussed throughout this
thesis, we wish to already give a high level idea of them,
while reserving the fully precise technical details for \arefpart{metacoq}.


\subsection{Cumulativity}

The first addition of \kl{PCUIC} is \kl{cumulativity}. 
To see why this is useful, consider the polymorphic identity function
$\l (A : \uni[i]) (x : A).\ x$, of type $\P A : \uni[i].\ A \to A$.
If we want to use it at type $\Nat$, we must force $i$ to be $0$. But this means that we
cannot use it later on at type $\uni[0]$! In a concrete system, where a huge number of
universe levels appear under the hood, this would quickly become unhandy.

Instead, \intro{cumulativity} – written $\cum$ – is an extension of conversion
with a limited form of subtyping,
corresponding to inclusion of a universe $\uni[i]$ in any larger universe $\uni[j]$.
This means that while $\uni[i] \conv \uni[j]$ is true only if $i = j$, cumulativity
allows for $\uni[i] \cum \uni[j]$ as soon as $i \leq j$.
This subtyping can be extended to function types,
by allowing $\P x : A.\ B \cum \P x : A'.\ B$ whenever $A \conv A'$ and $B \cum B'$.
Note that contrarily to other forms of subtyping, this does not allow for contravariant
subtyping on the domain – that would correspond to $A' \cum A$ –, only for equivariant
one – the domains should be convertible. This is because cumulativity is usually modelled
using set inclusion \sidetextcite{Lee2011}, which extends straightforwardly to equivariant
subtyping, but not so easily to contravariant subtyping.

\begin{marginfigure}
  \begin{mathpar}
    \inferdef{UnivCum}{i \leq j}{\uni[i] \cum \uni[j]} \label{rule:univ-cum} \and
    \inferdef{ΠCum}{A \conv A' \\ B \cum B'}{\P x : A.\ B \cum \P x : A'.\ B'}
      \label{rule:prod-cum} \and
    \inferdef{ConvCum}{A \conv A'}{A \cum A'} \label{rule:conv-cum} \and
    \inferdef{Refl}{ }{A \cum A} \label{rule:cum-refl} \and
    \inferdef{Trans}{A \cum A' \\ A' \cum A''}{A \cum A''} \label{rule:cum-trans} \and
    \inferdef{UCum}{\Gamma \vdash T' : \uni \\ T \cum T'}{\Gamma \vdash T \cum T' : \uni}
      \label{rule:cic-ucum} \and
    \inferdef{Cum}{\Gamma \vdash t : T \\ \Gamma \vdash T \cum T' : \uni}
    {\Gamma \vdash t : T'}
      \label{rule:cic-cum}
  \end{mathpar}
  \caption{Rules for declarative cumulativity}
  \label{fig:tech-cumul}
\end{marginfigure}

To adapt the definitions of \cref{sec:tech-conversion} to cumulativity,
\kl{declarative conversion} the three important rules are given in \cref{fig:tech-cumul}.
The first two rules are the ones we already defined: \ruleref{rule:univ-cum} is the base case for cumulativity, and \ruleref{rule:prod-cum}
is the relaxed congruence rule for products. The next one, \ruleref{rule:conv-cum}, allows
to turn any proof of conversion in a cumulativity one, effectively describing how cumulativity
behaves outside the "fragment of interest" formed by Π-types and universes. 
Next come Rules \nameref{rule:cum-refl} and \nameref{rule:cum-trans}, which assert that cumulativity
is a preorder. Of course there is no rule for symmetry, because cumulativity should not be
an equivalence relation. Finally, Rules \nameref{rule:cic-ucum} and \nameref{rule:cic-cum} show how
cumulativity is used: it simply replaces conversion. 


\subsection{The sort of propositions}

\begin{marginfigure}
  \begin{mathpar}
    \inferdef{Prop}{\vdash \Gamma}{\Gamma \vdash \Prop : \uni[0]}
    \label{rule:cic-prop} \and
    \inferdef{ΠProp}{\Gamma \vdash A : u \\ \Gamma, x : A \vdash P : \Prop}
    {\Gamma \vdash \P x : A.\ P : \Prop}
    \label{rule:prop-prod}
  \end{mathpar}
  \caption{Typing rules for propositions}
  \label{fig:cic-prop}
\end{marginfigure}

A second addition in \kl{PCUIC}, and one that has been a distinctive feature of \kl{Coq}
for a very long time – it is already present in \sidetextcite{Coquand1988} –
is the sort $\Prop$. This is a universe, like $\uni[i]$, but it is designed to
be a type for propositions – hence the name. It has two main distinctive characteristics.

The first one is its \intro{impredicativity}, meaning that while $\Prop$ is at the bottom
of the universe hierarchy (\ruleref{rule:cic-prop}), any quantification with a proposition
as codomain is again a proposition (\ruleref{rule:prop-prod}, where $s$ stands for any
universe, that is either $\uni$ or $\Prop$).
This means that propositions are able to formalize properties of types at any level.
Due to this impredicative nature, having such a sort of propositions makes the system
much more powerful as a logic, which also makes it much harder to build models of it.
Indeed, those usually prove consistency of the system being,
something which requires having an even higher logical strength.
Since some of this thesis – especially \arefpart{gradual} requires such models that do not
scale to propositions, we refrain from including it in our reference \kl{CIC}.

The second characteristic of $\Prop$ is \intro{proof irrelevance}.
This means that \kl{PCUIC} has a criterion, called singleton elimination, which maintains
a form of segregation between terms inhabiting types in $\uni$ and
those inhabiting types in $\Prop$, ensuring that the second cannot depend in a relevant
way on the first. For instance, if $P : \Prop$, it ensures that it is impossible
to build a function $f : P \to \Bool$ and two terms $p_1 : P$ and $p_2 : P$ such that
$f\ p_1 \conv \false$ and $f\ p_2 \conv \true$. This segregation aims at allowing separation
between the part of the system that should be seen as programs and that which should be
seen as proofs, so that it is then possible to write programs decorated with
complex correctness proofs, while later on erasing all the
logical content to keep only the computational content of the program.

\subsection{Local definitions}

It is often useful to locally introduce a shorthand to be used repeatedly, and
this is what \kl{PCUIC} allows with \intro{local definitions} $\letin{x}{A}{t}{u}$.
In such a local definition, $x$ can be used in the term $u$ as a shorthand for $t$.

The main impact of this addition is its effect on contexts: as \ruleref{rule:pcuic-letin}
illustrates, when typing $u$, not only the type of the definition is recorded, but
also its value $t$. This is again due to dependency, because the value of the definition,
and not only its type, might be needed $u$ to be well-typed.
As an example, suppose we are given a function
$\operatorname{head} : \P (A : \uni) (x : \Nat).\ \Vect(A,\S x) \to A$, and consider
\[\letin{x}{\Nat}{1}{\l v : \Vect(\Bool,x).\ \operatorname{head}\ \Bool\ 0\ v} \]
This term is well-typed only if the fact that $x$ has value $1$ is available in the
right-hand side.

\begin{figure}
  \ContinuedFloat*
  \begin{mathpar}
    \inferdef{LetIn}{\Gamma \vdash A : \uni \\ \Gamma \vdash t : A \\
    \Gamma, x := t : A \vdash u : B}
    {\Gamma \vdash \letin{x}{A}{t}{u} : \letin{x}{A}{t}{B}}
    \label{rule:pcuic-letin}
  \end{mathpar}
  \caption{Typing for local definitions}
  \label{fig:local-def}
\end{figure}

\begin{marginfigure}
  \ContinuedFloat
  \begin{mathpar}
    \inferdef{δRed}{(x := t : A) \in \Gamma}{\Gamma \vdash x \tred t}
    \label{rule:delta-red} \and
    \inferdef{ζRed}{ }{\Gamma \vdash \letin{x}{A}{t}{u} \\ \tred \subs{u}{x}{t}}
    \label{rule:zeta-red}
  \end{mathpar}
  \caption{Top-level reduction for local definitions}
\end{marginfigure}

This also means that contexts now should be recorded in
conversion/cumulativity, because those need to be able to access the value of a
variable bound by a definition, if we want to enable the behaviour we just described.
In the end, there are two \kl{top-level reductions} for definitions: they can be either
simplified right away into a substitution%
\sidenote{The notations are a bit misleading here: the local definition is part
of the syntax of terms, while substitution is a meta-level operation. While the former
encodes the latter in the syntax, they are very different!}
(\nameref{rule:zeta-red}), or recorded into the context and simplified only later on
using \ruleref{rule:delta-red}.

\subsection{Global environments}

\kl{PCUIC} offers a second way to record definitions, inside a so-called \intro{environment}.
The difference between this the addition of local definitions in a context we
just saw is not motivated by foundational issues, but rather by more concrete ones.
The (local) context corresponds to definitions and abstractions
encountered when type-checking a single definition, and should thus be relatively shallow
– the order of magnitude is a dozen variables – but it might change very often, with variable
being both added and abstracted over.
The (global) environment, on the contrary, can become huge –
corresponding to a whole library with thousands of components — but changes less often, and
usually in a monotone way – new definitions are added, but not removed.
Therefore typing in \kl{PCUIC} actually has an extra parameter, it is of the form
$\Sigma ;;; \Gamma \vdash t : T$, with $\Sigma$ corresponding to the \kl{environment}.

This environment is not only used for definitions and assumptions,
but also to keep track of inductive types.
It thus effectively implements our somewhat vague assumption that
\kl{CIC} is extended with "any number of valid inductive types". Of course, there is a notion
of environment well-formedness, which accounts for the fact that as for contexts it should
only contain objects that are well-typed, together with other constraints, for instance
that inductive types respect the strict positivity criterion.

There is a further use for this environment: it also records the level variables available for
universes, and their constraints. There are actually two kinds of those variables.
The first are global levels, that are part of the environment, and can only be extended.
This is the older approach, that was introduced in \kl{Coq} together with \kl{typical
ambiguity}, following \sidetextcite{Pollack1992}.
A second kind of variables, are instead attached locally to
an entry in the environment, corresponding to a form of universe polymorphism, somewhat
similar to the Hindley-Milner style of type polymorphism
\sidecite{Hindley1969,Milner1978} widely used in the ML family of languages.
Such polymorphic variables are a more recent addition \sidecite{Sozeau2014}, and
were designed to add even more flexibility compare to the first kind. For instance,
they allow to have a single polymorphic definition of categories,
and still be able to define the category – at level $j$ – of all categories
– at a level $i < j$ –, by instantiating the same definition at different levels.

\subsection{Enhanced inductive types}

Of course inductive types in \kl{PCUIC} are also affected by those extensions.
Not only can they be polymorphic, as definitions, they also feature a form of 
cumulativity, that makes this polymorphism more seamless – see \sidetextcite{Timany2018}
for a precise description.
This for instance prevents issues with $\lnil[A]$ not being of type $\List(A)$ because of a
mismatch between type variables – they do not appear in our presentation of \kl{CIC},
but are present in \kl{PCUIC} due to the general setting regarding polymorphic 
inductive types. Moreover, the strict positivity criterion adopted in \kl{PCUIC} is
very general, as it allows mutually defined and nested inductive types.
The former are multiple inductive types defined at the same time,
where a constructor of one type can take a recursive argument of another. For instance,
an inductive oddness/evenness predicate with constructors
$\operatorname{oddS} : \P x : \Nat.\ \operatorname{even} x \to \operatorname{odd} \S x$ and $\operatorname{evenS} : \P x : \Nat.\ \operatorname{odd} x \to \operatorname{even} \S x$.
The latter are types where a constructor can take a recursive argument mentioning the type
being defined as a parameter – for instance, a type of tree where a node takes a list of
trees as arguments.

Another important difference is that the induction principles, such as the ones we gave for
\kl{CIC}, are replaced with two new constructions: pattern-matching and fixed-points. The
first corresponds to the non-recursive component of the induction principles, while the
second allows to define a function that calls itself recursively.
To avoid paradoxical definitions, not every recursive definition is accepted, however.
Instead, there is a restriction called the \intro{guard condition} to how a recursive function
can be defined, which amounts to checking that recursive calls are made on
structurally smaller sub-terms – by means of pattern-matching. This guard condition
theoretically ensures that fixed-points can always be reduced to inductive principles
\sidetextcite{Gimenez1995}. However, in practise pattern-matching and fixed-points give more
flexible and natural definitions than those.

\subsection{Records and co-inductive types}

The last ingredient in \kl{PCUIC} goes beyond inductive types, by adding more primitive types
to the theory.

The first kind are record types, a generalization of Σ-types which allows for any number of
fields. The main addition of record types is the ability to access those fields via
\intro{projections} rather than by using pattern-matching. For the Σ-type as presented in
\cref{sec:tech-cic}, this would mean accessing the two fields of the pair $p$ with two
term formers $p._{\pi_{1}}$ and $p._{\pi_{2}}$. These record types are very useful to package
objects together, be it in programming or in mathematics – where such bundles are ubiquitous,
for instance when formalizing hierarchies of mathematical structures.

The second kind are co-inductive types. They are somewhat similar to inductive types, but
while the latter correspond to well-founded objects, the former represent
potentially infinite objects, such as streams of values. Because of this flavour of infinity,
co-inductive types pose an inherent threat to good properties of the system, such as
decidability of type-checking. At the time of their introduction in Coq
\sidecite{Gimenez1995}, they were presented in a so-called "positive" fashion, which
kept decidability of type-checking at the cost of another important property of the
system, subject reduction. Another presentation, inspired by more recent work on
co-induction and especially co-patterns \sidecite{Abel2013}, is the "negative" one,
which regains the good properties of the system. While the older positive presentation
is still present in \kl{Coq} for compatibility reasons,
only the negative one is formalized in \kl{MetaCoq}.
