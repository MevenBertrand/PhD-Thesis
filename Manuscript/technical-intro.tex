\chapter{The Calculus of Inductive Constructions}
\label{chap:tech-intro}

Most of this thesis revolves around \kl[dependent type]{dependent type systems}.
Since these are
quite complex, there is a high number of points where one can introduce slight variations
when giving a precise definition of a system to study it.
Some of these variations are unimportant, but some introduce large differences in the
resulting systems. Thus, in this chapter we go over in details over
the definition of what I will refer to as the
\kl{Calculus of Inductive Constructions} (or simply \kl{CIC}), in the rest of this
thesis, and which will serve as the basis for variations, theoretical study and extensions.
While doing so, I will try to give an idea of the trade-offs involved, and of the reasons
behind the choices made. Quite a few of those choices vary during the thesis,
and this is by design: there is no single better choice, 
something that I will try to make as clear as possible.

For the impatient specialists, let me say now that with \kl{CIC}, I
mean an intentional type theory, with Curry-style abstractions,
a predicative hierarchy of Russel universes\sidenote{And only those: by default I do \emph{not} include an impredicative sort of propositions, a feature that is sometimes associated to the name \kl{CIC}.},
and any amount of required inductive types, presented by recursors – 
the ones appearing most often being the empty and
unit types, booleans, natural numbers, dependent sums, lists, vectors and the equality.
Conversion is by default the reflexive, symmetric transitive closure of reduction, and
so in particular it is untyped. For the others, let me now detail what I mean by this.

\begin{itemize}
  \item universes (mention integers vs levels vs polymorphism)
  \item prop!
  \item inductive types (the whole variety of them?)
  \item let bindings
  \item fixpoints + match vs recursors
  \item co-inductives (briefly I guess)
  \item reduction, conversion (take 1)
  \item conversion as a judgement
\end{itemize}

\section{Terms, typing and derivations}

Throughout this chapter, \kl{CIC} is defined by means of a typing relation $\Gamma \vdash t : T$, which reads "in the context $\Gamma$, term $t$ has type $T$".
From the logical point of view, this judgement means that $\Gamma$
contains the hypothesis available to deduce the
result $T$ by means of the proof $t$. On the programming side, it means that
$t$ is a program, which uses the variables listed in $\Gamma$ together with their types,
and has type $T$.
Thus, $\Gamma$ is a list of declarations of the form $x : A$. We write $\emptycon$ for the
empty context, and $\Gamma, x : A$ for the context $\Gamma$ extended with the new variable $x : A$, and $(x : A) \in \Gamma$ to denote that the declaration $x : A$ appears at some
point in the context $\Gamma$.

\begin{marginfigure}
  \begin{mathpar}
  \inferdef{Var}{(x : A) \in \Gamma \\ \vdash \Gamma}{\Gamma \vdash x : A}
  \label{rule:cic-var}
  \end{mathpar}
\end{marginfigure}

This typing relation itself is defined by means of inference rules,
such as \ruleref{rule:cic-var} opposite. The way to read this rule is that the judgement
underneath the line follows from the one above,
\ie from $(x : A) \in \Gamma$
and $\vdash \Gamma$ – a judgement that will be defined soon asserting that the context
$\Gamma$ is well-formed – we can deduce $\Gamma \vdash x : A$.
Once a set of such inference rules is fixed,
typing is defined as the least relation closed by those
rules. Equivalently, a judgement such as $\Gamma \vdash t : T$
holds as soon as we can build a tree whose nodes are instances of the inference rules,
and whose root is the judgement in question. A general setting
to make this kind of definitions precise can be found in \sidetextcite{Bauer2020},
in our case we will restrict to the level of presentation
until \arefpart{metacoq}.

As we have already introduced variables, a word on those as well. Variables are difficult
to account for precisely, because of issues like shadowing – a conflict between two variables
with the same name – or $\alpha$-equivalence – the identification between two terms
only differing on variable names. There are multiple techniques to solve these issues
– see the many solutions to the POPLMark Challenge~\sidecite{Aydemir2005} –, 
but again, we treat these issues in an informal way until \arefpart{metacoq}, assuming
there is no shadowing whatsoever and identifying $\alpha$-equivalent terms when needed.

A final important building block that we use in all our type theories is substitution,
that we write $\subs{t}{x}{u}$. This replaces every occurrence of $x$ in $t$ by the term
$u$. Once again, we will treat this operation informally, assuming it never creates
shadowing – what is sometimes called "capture-avoiding" substitution.

\section{Functional core: \kl{CCω}}
\label{sec:tech-ccw}

\subsection{Functions and applications}

Let us now turn to the core of CIC, namely the
\intro{Calculus of Constructions} (\kl{CCω}). Through the \kl{Curry-Howard correspondence},
it is both a typed form of $\l$-calculus – \ie a kind of purely functional
programming language – and a minimal form
of logic – only containing universal quantification and implication.

\begin{marginfigure}
  \ContinuedFloat*
  \begin{mathpar}
    \inferrule{\Gamma, x : A \vdash t : T}{\Gamma \vdash \l x : A .\ t : A \to T}
    \and
    \inferrule{\Gamma \vdash f : A \to T \\ \Gamma \vdash u : A }{ \Gamma \vdash t\ u : T}
  \end{mathpar}
  \caption{Typing for non-dependent functions}
  \label{fig:cic-nondep-fun}
\end{marginfigure}

Functions, also called $\l$-abstraction are written $\l x : A .\ t$. This corresponds
to the mathematical notation $x \mapsto t$: the body $t$ of the function,
is a term that might contain the variable $x$,
and the constructor $\l$ abstracts over that variable to build a function.
Conversely, function application is written simply using juxtaposition, as in $t\ u$.
The type of functions is written $\to$, as in ordinary mathematics.
You can see those at work in \cref{fig:cic-nondep-fun}: an abstraction build a term of arrow
type, and application needs its function to be of arrow type, whose domain must correspond to
that of the argument for it to be well-typed.
Logically, those rules correspond to that of implication: if from a hypothesis $A$ one can
deduce $T$, then $A \to T$ holds – reading $\to$ as implication; conversely if $A \to T$
and $A$ both hold, then $T$ does as well.

\begin{marginfigure}
  \ContinuedFloat
  \begin{mathpar}
    \inferdef{Abs}{\Gamma, x : A \vdash t : T}{\Gamma \vdash \l x : A .\ t : \P x : A.\ T}
    \label{rule:cic-abs}
    \and
    \inferdef{App}{\Gamma \vdash f : \P x : A.\ T \\ \Gamma \vdash u : A }{ \Gamma \vdash f\ u : \subs{T}{x}{u}}
    \label{rule:cic-app}
  \end{mathpar}
  \caption{Typing for dependent functions}
  \label{fig:cic-dep-fun}
\end{marginfigure}
These arrow types, however, are not as expressive as one would wish for.
Remember that we are in the realms of dependent types, so not only $t$ might mention $x$,
but also $T$. For instance, $T$ might be something like "$x$ is even". In such a case,
we need to record that dependency, which is the point of $\P$-types – or product types –,
shown in \cref{fig:cic-dep-fun}.
Seen as function types, they record the fact that the codomain
might vary depending on the argument. This is reflected in the typing rule for application:
since the codomain $T$ might depend on $x$, the type of the application $f\ u$ is $T$
\emph{specialized at the argument $u$}, using substitution.
Seen on the logical side, product types correspond to universal quantification
$\operatorname{\forall} x : A.\ T(x)$.
Indeed, if one can show that $T(x)$ holds for an unspecified $x$,
then it must hold for all $x : A$ – this is \ruleref{rule:cic-abs}.
Conversely, if $T$ holds for all $x : A$, then one can deduce $T(u)$ for any specific
$u : A$ – this is \ruleref{rule:cic-app}.
Now the rules for arrow types in \cref{fig:cic-nondep-fun} are just a special case
of those for product types, in the case where the codomain $T$ does not depend
on the variable $x$, and we will use this convention throughout the thesis:
$A \to T$ is a shortcut for $\P x : A.\ T$ when $T$ does not mention $x$.

One last thing to note about our functions is that they record the type of their
domain – what is called \intro{Church-style}
abstraction~\sidecite[][Section~3]{Barendregt1992}. There is an alternative – 
the \intro{Curry-style} abstractions –, that
does not do so, simply using $\l x.\ t$ for functions. At this point in the
presentation, this does not make much difference,
but it is crucial as soon as one looks at the bidirectional structure. 
Indeed, that annotation is required if one wants to infer types for functions,
rather than barely checking them.
The \kl{Curry-style}
option is definitely sensible, see for instance \sidetextcite[][p.~19]{Norell2007}
– which describes the implementation of the very successful proof assistant
\kl{Agda}, which uses the \kl{Curry-style} approach –,
\sidetextcite[][Section~4.1]{Gratzer2019} or \sidetextcite{McBride2022}.
In the end, this is really a design choice between being able to infer a type for any term,
or requiring annotations that in a lot of cases are useless, and in this
thesis we stick with the approach used in \kl{Coq}, and annotate our abstractions.

\subsection{Universes}

To be able to express things like induction principles or polymorphic functions, it is
extremely useful to be able to use functions and products quantifying over types.
This is what the universe $\uni$ are for. It is the type… of a type.
This means that the frontier between types and terms is not syntactic any more.
Instead, types are simply terms of type $\uni$.
Despite this, we still use upper case letters for terms which we want to think of as types.
Such a universe is called \textit{à la} Russell~\sidecite{Palmgren1998}, by contrast with
universes \textit{à la} Tarski, which regain the distinction between types and terms at
the cost of a somewhat heavier treatment of types.
The presentation \textit{à la} Tarski
is mostly useful when building models, something we will not do here, so we keep the simpler
of the two presentations.

\begin{marginfigure}
  \ContinuedFloat
  \begin{mathpar}
    \inferdef{Univ}
    {\vdash \Gamma}
    {\Gamma \vdash \uni[i] : \uni[\unext{i}]}
    \label{rule:cic-univ}
  \end{mathpar}
  \caption{Typing for universes}
  \label{fig:cic-univ}
\end{marginfigure}

There is however, an important caveat.
Since the disappointments of Frege and the paradox exhibited
by Russell in his system, logicians know that considering a set of all sets is a great
source of inconsistencies. Type theory is not devoid of this issue:
Girard~\sidecite[][Annex~A]{Girard1972}
shows how having a type with itself as type is inconsistent.
This inconsistency directly applies to the first dependent type system proposed by
Martin-Löf~\sidecite{MartinLoef1972}, which had a single universe $\uni$ and a rule $\uni : \uni$.
A common solution to this
is to stratify universes into an infinite hierarchy, which gives us \ruleref{rule:cic-univ}:
note the \intro{universe levels} $i$ and $\unext{i}$.

\begin{marginfigure}
  \ContinuedFloat
  \begin{mathpar}
    \inferdef{Prod}
    {\Gamma \vdash A : \uni[i] \\ \Gamma, x : A \vdash B : \uni[j]}
    {\Gamma \vdash \P x : A.\ B : \uni[\umax{i}{j}]}
    \label{rule:cic-prod}
  \end{mathpar}
  \caption{Typing for product types}
  \label{fig:cic-prod}
\end{marginfigure}

\begin{marginfigure}
  \ContinuedFloat
  \begin{mathpar}
    \inferdef{EmptyCon}
    { }{\vdash \cdot}
    \label{rule:cic-empty-con} \and
    \inferdef{ConsCon}
    {\vdash \Gamma \\ \Gamma \vdash A : \uni}{\vdash \Gamma, x : A}
    \label{rule:cic-cons-con}
  \end{mathpar}
  \caption{Context well-formedness}
  \label{fig:cic-con}
\end{marginfigure}

Using those universes, \ruleref{rule:cic-prod} gives the typing rule for the product constructor. We can also use these universes to give a definition of the $\vdash \Gamma$
judgement, asserting that a context is well-formed, in \cref{fig:cic-con}.
It simply means that all its types
are indeed types. Note that in \ruleref{rule:cic-cons-con}, we did not give an index for the
universe, we will do so to mean the existence of some unconstrained level $i$.

One last important point regarding universes is the kind of levels used. The simplest solution
is to rely on natural number (of the meta-theory), with the $\unextsymb$
and $\umaxsymb$ operations interpreted by the usual ones.
This is however not strictly necessary: we need levels
to form an order so as to ensure we avoid inconsistency, and operations $\unextsymb$ and
$\umaxsymb$, but levels could very well be something different from natural numbers.
In particular, the natural number approach fixes at which level a particular construction
is done, which is usually much more rigid than what one would wish for.
A more flexible approach, introduced under the name \intro{typical ambiguity} by
\sidetextcite{Harper1991},
uses level expressions based on level variables, rather than integers.
This way, one can collect the constraints between levels required for a
term to type-check in a consistent system, without artificially enforcing a
rigid interpretation of levels by fixing them to a precise integer once and for all.
To simplify the presentation, our "standard" \kl{CCω}/\kl{CIC} will nonetheless use integers,
but we will switch to level expressions when needed – especially in \arefpart{metacoq}.

\section{50 Shades of Conversion}
\label{sec:tech-conversion}

\begin{marginfigure}
  \ContinuedFloat
  \begin{mathpar}
  \inferdef{Conv}
    {\Gamma \vdash t : T \\ \Gamma \vdash T \conv T' : \uni}
    {\Gamma \vdash t : T'}
  \label{rule:cic-conv}
  \end{mathpar}
  \caption{Conversion rule}
\end{marginfigure}

There is one big missing part in the picture so far. Remember we are working with
dependent types, and that those can contain terms, which in turn can be arbitrary programs.
If you recall for instance the vector type we used in the introduction (and that we are
about to introduce formally), what happens if a function expects an argument of type
$\Vect A\,3$, but it is given an argument of type $\Vect A\,(2+1)$, for instance the output
of a concatenation function? Surely we must have a way to relate both, since after all
the small program $2+1$ ought to compute to $3$! This is exactly what \intro{conversion} is
for: in dependent type theory, there is a way to change a type to one that
is related to it by this relation – see \ruleref{rule:cic-conv}, which wraps up our typing
rules for \kl{CCω}, collected in \cref{fig:ccw-typing}.
As usual, there are two ways to look at this relation. From the point of view of programs,
it allows to incorporate the computational aspect of those, directly inside the type system.
From the point of view of logics, this corresponds to things being the same "by definition"
rather than due to some reasoning
– which is why conversion is also called definitional, or judgmental equality. In our vector
example, for instance, the two types will be the same by virtue of the definition of addition.

\begin{figure*}[h]
  \LastFloat

  \begin{mathpar}
    %
    \jform{\vdash \Gamma}
    \inferdef{EmptyCon}
      { }{\vdash \cdot}
    \and
    \inferdef{ConsCon}
      {\vdash \Gamma \\ \Gamma \vdash A : \uni}{\vdash \Gamma, x : A}
    \\\\
    \jform{\Gamma \vdash t : T}
    \inferdef{Var}{(x : A) \in \Gamma \\ \vdash \Gamma}{\Gamma \vdash x : A}
    \and
    \inferdef{Univ}
      {\vdash \Gamma}
      {\Gamma \vdash \uni[i] : \uni[\unext{i}]}
    \and
    \inferdef{Prod}
      {\Gamma \vdash A : \uni[i] \\ \Gamma, x : A \vdash B : \uni[j]}
      {\Gamma \vdash \P x : A.\ B : \uni[\umax{i}{j}]}
    \and
    \inferdef{Abs}{\Gamma, x : A \vdash t : T}{\Gamma \vdash \l x : A .\ t : \P x : A.\ T}
    \and
    \inferdef{App}
      {\Gamma \vdash f : \P x : A.\ T \\ \Gamma \vdash u : A }
      {\Gamma \vdash f\ u : \subs{T}{x}{u}}
    \and
  \inferdef{Conv}
    {\Gamma \vdash t : T \\ \Gamma \vdash T \conv T' : \uni}
    {\Gamma \vdash t : T'}
  \end{mathpar}

  \caption{Collected typing rules for \kl{CCω}}
  \label{fig:ccw-typing}
\end{figure*}

Conversion is a very complex beast, arguably the most subtle part of dependent types.
Consequently, there are very different ways to present it, which in turn serve different
needs. There are at least two axes on which conversion can vary.

The first axis is \intro[typed conversion]{typed} \vs
\intro[untyped conversion]{untyped} conversion.
On one side, conversion is intrinsically typed, terms are only convertible
\emph{at a given type}. On the other, conversion is a relation between raw terms,
that does not presuppose any form of typing. \Cref{fig:typed-untyped-conv} gives an
example of the "same" rule – the computation rule for functions – in both systems.
As we can see, the content of the two rules is the same, it equates $(\l x : A.\ t)\ u$
and $\subs{t}{x}{u}$, only the side-conditions differ wildly.
Typed conversion goes back to
\sidetextcite{MartinLoef1972}, and is a recurring feature in its many descendants.
Untyped conversion relates strongly to (untyped) $\l$-calculus – Barendregt
for instance uses the name "conversion" for the equational theory of untyped $\l$-calculus
in his reference work on the subject~\sidecite{Barendregt1985} –, via
the Pure Type Systems (PTS)~\sidecite{Barendregt1991} literature.
In this thesis, we will mainly consider untyped conversion, as \kl{Coq}’s meta-theory
has been mostly studied in that tradition.
But the relation between both in the context of
bidirectional typing is the main subject of \cref{chap:bidir-conv}.

\begin{figure}[h]
  \begin{mathpar}
    \inferrule
      {\Gamma, x : A \vdash t : B \\ \Gamma \vdash u : A}
      {\Gamma \vdash (\l x : A.\ t)\ u \conv \subs{t}{x}{u} : \subs{B}{x}{u}}
    \and
    \inferrule{ }{(\l x : A.\ t)\ u \conv \subs{t}{x}{u}}
  \end{mathpar}
  \caption{Example: typed and untyped $\beta$ rule for conversion}
  \label{fig:typed-untyped-conv}
\end{figure}

The second axis corresponds to how close the conversion relation is to an implementation.
For instance, conversion should be an equivalence relation,
but there are two approaches to that. The first – and standard – one
is to simply \emph{define} conversion as an equivalence relation, by adding rules 
for \eg transitivity, as the one of \cref{fig:trans-conv}.
\begin{marginfigure}
  \begin{mathpar}
    \inferrule
      {t \conv t' \\ t' \conv t''}
      {t \conv t''}
  \end{mathpar}
  \caption{Example: transitivity rule for conversion}
  \label{fig:trans-conv}
\end{marginfigure}
This ensures that it has the right properties, but
makes it hard to prove it decidable, because one could have gone by
transitivity through terms whose trace is not present in the end relation.
The $\l$-calculus theorists have known this issue for a long time, and they
have a solution: characterizing conversion by means of a \kl{reduction} relation $\red$, which
corresponds to the idea of program evaluation – see \sidetextcite{Barendregt1985} for
instance. If this reduction has good
properties\sidenote{The main one being confluence.}, then
two terms will be convertible when they reduce to the same third term. This much more
operational characterization is closer to what can be implemented.
Turning things around, one can define conversion through reduction,
and only show \emph{afterwards}
that it has the good properties of the first approach – typically, that it is transitive.
Conversion of the first kind we will call \intro{declarative conversion}, while for the second
we will talk about \intro{algorithmic conversion}.

In the rest of this section we give both presentations for \kl{untyped conversion}: the
\kl{declarative} one as we define \kl{CCω} with it, since it is the most standard;
the \kl{algorithmic} one as it will appear in \arefpart{metacoq}
to show decidability of type-checking, and in
\arefpart{gradual} where we want to replace conversion by a non-transitive relation,
declarative conversion thus being a bad candidate as it enforces transitivity.

\subsection{Untyped declarative conversion}

\begin{marginfigure}
  \ContinuedFloat*
  \begin{mathpar}
    \inferdef{UConvConv}{\Gamma \vdash T' : \uni \\ T \conv T'}{\Gamma \vdash T \conv T' : \uni}
    \label{rule:cic-conv-unty}
  \end{mathpar}
  \caption{Typing constraint on untyped conversion}
\end{marginfigure}

Let us first go back to \ruleref{rule:cic-conv}.
This rule was set up in such a way that typed or untyped conversion could be plugged in
without modifying the rule.
Actually, even if we wish to describe conversion as
an untyped relation, we still need to enforce a typing constraint at this point,
namely that the target type is indeed a well-formed type,
in order to ensure that whenever $\Gamma \vdash t : T$ is derivable
$\Gamma \vdash T : \uni$ is as well.
This is exactly the content of \ruleref{rule:cic-conv-unty}.

Regarding conversion itself, the first set of rules asserts
that conversion forms an equivalence relation: it
is reflexive (\ruleref{rule:cic-uconv-refl}), symmetric (\ruleref{rule:cic-uconv-sym}),
and transitive (\ruleref{rule:cic-uconv-trans}).

\begin{figure}[h]
  \ContinuedFloat
  \begin{mathpar}
    \inferdef{Refl}{ }{\Gamma \vdash t \conv t}
    \label{rule:cic-uconv-refl} \and
    \inferdef{Sym}{\Gamma \vdash t \conv t'}{\Gamma \vdash t' \conv t}
    \label{rule:cic-uconv-sym} \and
    \inferdef{Trans}
      {\Gamma \vdash t \conv t' \\ \Gamma \vdash t' \conv t''}
      {\Gamma \vdash t \conv t''}
    \label{rule:cic-uconv-trans}
  \end{mathpar}
  \caption{Equivalence rules}
  \label{fig:cic-uconv-equiv}
\end{figure}

A second set of rules, collected in \cref{fig:cic-uconv-cong},
asserts that conversion is a congruence, meaning that it is compatible
with all term formers. As for the previous three, these correspond to properties we expect
from the conversion relation, that we simply declare to be true. Note that we include only
congruence rules for term formers with subterm – we \eg omit $\uni$. This is because
those are special cases of \ruleref{rule:cic-uconv-refl}. Conversely, we could omit 
reflexivity alltogether and only have congruence rules, which can be seen as a generalized
form of reflexivity.

\begin{figure}[ht]
  \ContinuedFloat
  \begin{mathpar}
    \inferdef{ProdConv}
      {A \conv A' \\ B \conv B'}
      {\P x : A.\ B \conv \P x : A'.\ B'}
    \label{rule:cic-uconv-prod} \and
    \inferdef{AbsConv}
      {A \conv A' \\ t \conv t'}
      {\l x : A .\ t \conv \l x : A'.\ t'}
    \label{rule:cic-uconv-abs} \and
    \inferdef{App}
      {f \conv f' \\ u \conv u' }
      {f\ u \conv f'\ u'}
    \label{rule:cic-uconv-app}
  \end{mathpar}
  \caption{Congruence rules}
  \label{fig:cic-uconv-cong}
\end{figure}

\begin{marginfigure}
  \ContinuedFloat
  \begin{mathpar}
    \inferdef{βConv}{ }{(\l x : A.\ t)\ u \conv \subs{t}{x}{u}}
    \label{rule:cic-uconv-beta}
  \end{mathpar}
  \caption{Computation rule for functions}
\end{marginfigure}
The last rule, \ruleref{rule:cic-uconv-beta} is the crucial one:
it corresponds to the computational behaviour
of functions, replacing the variable of an applied $\l$-abstraction by its argument by
means of substitution.

