\chapter{The Calculus of Inductive Constructions}
\label{chap:tech-intro}

Most of this thesis revolves around \kl[dependent type]{dependent type systems}.
Since these are
quite complex, there is a high number of points where one can introduce slight variations
when giving a precise definition of a system to study it.
Some of these variations are unimportant, but some introduce large differences in the
resulting systems. Thus, in this chapter we go over in details over
the definition of what we refer to as the
\kl{Calculus of Inductive Constructions} (or simply \kl{CIC}), in the rest of this
thesis, and which serves as the basis for variations, theoretical study and extensions.
While doing so, I try to give an idea of the trade-offs involved, and of the reasons
behind the choices made. Quite a few of those choices vary during the thesis,
and this is by design: there is no single better choice, 
something that I try to make as clear as possible.

For the impatient specialists, let me say now that with \kl{CIC}, I
mean an intentional type theory, with Curry-style abstractions,
a predicative hierarchy of universes \textit{à la Russel}\sidenote{
  And only those: by default I do \emph{not} include an impredicative sort of propositions, a feature that is sometimes associated to the name \kl{CIC}.},
and any amount of required inductive types, presented by recursors – 
the ones appearing most often in what comes next being the empty and
unit types, booleans, natural numbers, dependent sums, lists, vectors and the equality.
Conversion is by default the reflexive, symmetric, transitive and congruent
closure of β-reduction, and
so in particular it is untyped. For the others, let me now detail what I mean by this.

\section{Terms, typing and derivations}

Throughout this chapter, type systems are defined by means of a relation
$\Gamma \vdash t : T$, which reads "in the context $\Gamma$, term $t$ has type $T$".
From the logical point of view, this judgement means that $\Gamma$
contains the hypothesis available to deduce the
result $T$ by means of the proof $t$. On the programming side, it means that
$t$ is a well-formed program,
which uses the variables listed in $\Gamma$ together with their types,
and has type $T$.
Thus, $\Gamma$ is a list of declarations of the form $x : A$. We write $\emptycon$ for the
empty context, and $\Gamma, x : A$ for the context $\Gamma$ extended with the new variable $x : A$, and $(x : A) \in \Gamma$ to denote that the declaration $x : A$ appears at some
point in the context $\Gamma$.

\begin{marginfigure}
  \begin{mathpar}
  \inferdef{Var}{(x : A) \in \Gamma \\ \vdash \Gamma}{\Gamma \vdash x : A}
  \label{rule:cic-var}
  \end{mathpar}
\end{marginfigure}

This typing relation itself is defined by means of inference rules,
such as \ruleref{rule:cic-var} opposite. The way to read this rule is that the judgement
underneath the line follows from the one above,
\ie from $(x : A) \in \Gamma$
and $\vdash \Gamma$ – a judgement that will soon be defined asserting that the context
$\Gamma$ is well-formed – we can deduce $\Gamma \vdash x : A$.
When objects appear in the hypothesis but not the conclusion, they are implicitely
universally quantified.
Once a set of such inference rules is fixed,
typing is defined as the least relation closed by those
rules. Equivalently, a judgement such as $\Gamma \vdash t : T$
holds as soon as we can build a tree whose nodes are instances of the inference rules,
and whose root is the judgement in question. A general setting
to make this kind of definitions precise can be found in \sidetextcite{Bauer2020},
in our case we restrict to this level of detail until \arefpart{metacoq}.

As we have already introduced variables, a word on those as well. Variables are difficult
to account for precisely, because of issues like shadowing – a conflict between two variables
with the same name – or $\alpha$-equivalence – the identification between two terms
only differing on variable names. There are multiple techniques to solve these issues
– see the many solutions to the POPLMark Challenge~\sidecite{Aydemir2005} –, 
but again, we treat these issues in an informal way until \arefpart{metacoq}, assuming
there is no shadowing whatsoever and identifying $\alpha$-equivalent terms when needed.

A final important building block that we use in all our type theories is substitution,
that we write $\subs{t}{x}{u}$. This replaces every occurrence of $x$ in $t$ by the term
$u$. Once again, we treat this operation informally, assuming it never creates
shadowing – what is sometimes called "capture-avoiding" substitution.

\section{Functional core: \kl{CCω}}
\label{sec:tech-ccw}

\subsection{Functions and applications}

Let us now turn to the core of CIC, namely the
\intro{Calculus of Constructions} (\kl{CCω}). Through the \kl{Curry-Howard correspondence},
it is both a typed form of λ-calculus – \ie a kind of purely functional
programming language – and a minimal form
of logic – only containing universal quantification and implication.

\begin{marginfigure}
  \ContinuedFloat*
  \begin{mathpar}
    \inferrule{\Gamma, x : A \vdash t : T}{\Gamma \vdash \l x : A .\ t : A \to T}
    \and
    \inferrule{\Gamma \vdash f : A \to T \\ \Gamma \vdash u : A }{ \Gamma \vdash t\ u : T}
  \end{mathpar}
  \caption{Typing for non-dependent functions}
  \label{fig:cic-nondep-fun}
\end{marginfigure}

Functions, also called λ-abstraction are written $\l x : A .\ t$. This corresponds
to the mathematical notation $x \mapsto t$: the body $t$ of the function,
is a term that might contain the variable $x$,
and the constructor λ abstracts over that variable to build a function.
Conversely, function application is written simply using juxtaposition, as in $t\ u$.
The type of functions is written $\to$, as in ordinary mathematics.
You can see those at work in \cref{fig:cic-nondep-fun}: an abstraction build a term of arrow
type, and application needs its function to be of arrow type, whose domain must correspond to
that of the argument for it to be well-typed.
Logically, those rules correspond to that of implication: if from a hypothesis $A$ one can
deduce $T$, then $A \to T$ holds – reading $\to$ as implication; conversely if $A \to T$
and $A$ both hold, then $T$ does as well.

\begin{marginfigure}
  \ContinuedFloat
  \begin{mathpar}
    \inferdef{Abs}{\Gamma, x : A \vdash t : T}{\Gamma \vdash \l x : A .\ t : \P x : A.\ T}
    \label{rule:cic-abs}
    \and
    \inferdef{App}{\Gamma \vdash f : \P x : A.\ T \\ \Gamma \vdash u : A }{ \Gamma \vdash f\ u : \subs{T}{x}{u}}
    \label{rule:cic-app}
  \end{mathpar}
  \caption{Typing for dependent functions}
  \label{fig:cic-dep-fun}
\end{marginfigure}
These arrow types, however, are not as expressive as one would wish for.
Remember that we are in the realms of dependent types, so not only $t$ might mention $x$,
but also $T$. For instance, $T$ might be something like "$x$ is even". In such a case,
we need to record that dependency, which is the point of $\P$-types – or product types –,
shown in \cref{fig:cic-dep-fun}.
Seen as function types, they record the fact that the codomain
might vary depending on the argument. This is reflected in the typing rule for application:
since the codomain $T$ might depend on $x$, the type of the application $f\ u$ is $T$
\emph{specialized at the argument $u$}, using substitution.
Seen on the logical side, product types correspond to universal quantification
$\operatorname{\forall} x : A.\ T(x)$.
Indeed, if one can show that $T(x)$ holds for an unspecified $x$,
then it must hold for all $x : A$ – this is \ruleref{rule:cic-abs}.
Conversely, if $T$ holds for all $x : A$, then one can deduce $T(u)$ for any specific
$u : A$ – this is \ruleref{rule:cic-app}.
Now the rules for arrow types in \cref{fig:cic-nondep-fun} are just a special case
of those for product types, in the case where the codomain $T$ does not depend
on the variable $x$, and we use this convention throughout the thesis:
$A \to T$ is a shortcut for $\P x : A.\ T$ when $T$ does not mention $x$.

One last thing to note about our functions is that they record the type of their
domain – what is called \intro{Church-style}
abstraction~\sidecite[][Section~3]{Barendregt1992}. There is an alternative – 
the \intro{Curry-style} abstractions –, that
does not do so, simply using $\l x.\ t$ for functions. At this point in the
presentation, this does not make much difference,
but it is crucial as soon as one looks at the bidirectional structure. 
Indeed, that annotation is required if one wants to infer types for functions,
rather than barely checking them.
The \kl{Curry-style}
option is definitely sensible, see for instance \sidetextcite[][p.~19]{Norell2007}
– which describes the implementation of the very successful proof assistant
\kl{Agda}, which uses the \kl{Curry-style} approach –,
\sidetextcite[][Section~4.1]{Gratzer2019} or \sidetextcite{McBride2022}.
In the end, this is really a design choice between being able to infer a type for any term,
or requiring annotations that in a lot of cases are useless, and in this
thesis we stick with the approach used in \kl{Coq}, and annotate our abstractions.

\subsection{Universes}

To be able to express things like induction principles or polymorphic functions, it is
extremely useful to be able to use functions and products quantifying over types.
This is what the universe $\uni$ are for. It is the type… of a type.
This means that the frontier between types and terms is not syntactic any more.
Instead, types are simply terms of type $\uni$.
Despite this, we still use upper case letters for terms which we want to think of as types.
Such a universe is called \textit{à la} Russell~\sidecite{Palmgren1998}, by contrast with
universes \textit{à la} Tarski, which regain the distinction between types and terms at
the cost of a somewhat heavier treatment of types.
The presentation \textit{à la} Tarski
is mostly useful when building models. Since this is not the subject at hand in this thesis,
we keep the simpler of the two presentations.

\begin{marginfigure}
  \ContinuedFloat
  \begin{mathpar}
    \inferdef{Univ}
    {\vdash \Gamma}
    {\Gamma \vdash \uni[i] : \uni[\unext{i}]}
    \label{rule:cic-univ}
  \end{mathpar}
  \caption{Typing for universes}
  \label{fig:cic-univ}
\end{marginfigure}

There is however, an important caveat.
Since the disappointments of Frege and the paradox exhibited
by Russell in his system, logicians know that considering a set of all sets is a great
source of inconsistencies. Type theory is not devoid of this issue:
Girard~\sidecite[][Annex~A]{Girard1972}
shows how having a type with itself as type is inconsistent.
This inconsistency directly applies to the first dependent type system proposed by
Martin-Löf~\sidecite{MartinLoef1972}, which had a single universe $\uni$ and a rule $\uni : \uni$.
A common solution to this
is to stratify universes into an infinite hierarchy, which gives us \ruleref{rule:cic-univ}:
note the \intro{universe levels} $i$ and $\unext{i}$.

\begin{marginfigure}
  \ContinuedFloat
  \begin{mathpar}
    \inferdef{Prod}
    {\Gamma \vdash A : \uni[i] \\ \Gamma, x : A \vdash B : \uni[j]}
    {\Gamma \vdash \P x : A.\ B : \uni[\umax{i}{j}]}
    \label{rule:cic-prod}
  \end{mathpar}
  \caption{Typing for product types}
  \label{fig:cic-prod}
\end{marginfigure}

\begin{marginfigure}
  \ContinuedFloat
  \begin{mathpar}
    \inferdef{EmptyCon}
    { }{\vdash \cdot}
    \label{rule:cic-empty-con} \and
    \inferdef{ConsCon}
    {\vdash \Gamma \\ \Gamma \vdash A : \uni}{\vdash \Gamma, x : A}
    \label{rule:cic-cons-con}
  \end{mathpar}
  \caption{Context well-formedness}
  \label{fig:cic-con}
\end{marginfigure}

Using those universes, \ruleref{rule:cic-prod} gives the typing rule for the product constructor. We can also use these universes to give a definition of the $\vdash \Gamma$
judgement, asserting that a context is well-formed, in \cref{fig:cic-con}.
It simply means that all its types
are indeed types. Note that in \ruleref{rule:cic-cons-con}, we did not give an index for the
universe, we do so to mean the existence of some unconstrained level $i$.

One last important point regarding universes is the kind of levels used. The simplest solution
is to rely on natural number (of the meta-theory), with the $\unextsymb$
and $\umaxsymb$ operations interpreted by the usual ones.
This is however not strictly necessary: we need levels
to form an order so as to ensure we avoid inconsistency, and operations $\unextsymb$ and
$\umaxsymb$, but levels could very well be something different from natural numbers.
In particular, the natural number approach fixes at which level a particular construction
is done, which is usually much more rigid than what one would wish for.
A more flexible approach, introduced under the name \intro{typical ambiguity} by
\sidetextcite{Harper1991},
uses level expressions based on level variables, rather than integers.
This way, one can collect the constraints between levels required for a
term to type-check in a consistent system, without artificially enforcing a
rigid interpretation of levels by fixing them to a precise integer once and for all.
To simplify the presentation, our "standard" \kl{CCω}/\kl{CIC} nonetheless uses integers,
but we switch to level expressions when needed.

\section{50 Shades of Conversion}
\label{sec:tech-conversion}

\begin{marginfigure}
  \ContinuedFloat
  \begin{mathpar}
  \inferdef{Conv}
    {\Gamma \vdash t : T \\ \Gamma \vdash T \conv T' : \uni}
    {\Gamma \vdash t : T'}
  \label{rule:cic-conv}
  \end{mathpar}
  \caption{Conversion rule}
\end{marginfigure}

There is one big missing part in the picture so far. Remember we are working with
dependent types, and that those can contain terms, which in turn can be arbitrary programs.
If you recall for instance the vector type we used in the introduction (and that we are
about to introduce formally), what happens if a function expects an argument of type
$\Vect A\,3$, but it is given an argument of type $\Vect A\,(2+1)$, for instance the output
of a concatenation function? Surely we must have a way to relate both, since after all
the small program $2+1$ ought to compute to $3$! This is exactly what \intro{conversion} is
for: in dependent type theory, there is a way to change a type to one that
is related to it by this relation – this is \ruleref{rule:cic-conv}, which wraps up our typing
rules for \kl{CCω}, collected in \cref{fig:ccw-typing}.
As usual, there are two ways to look at this relation. From the point of view of programs,
it allows to incorporate the computational aspect of those, directly inside the type system.
From the point of view of logics, this corresponds to things being the same "by definition"
rather than due to some reasoning
– which is why conversion is also called definitional, or judgmental equality. In our vector
example, for instance, the two types are the same by virtue of the definition of addition.

\begin{figure*}[h]
  \LastFloat

  \begin{mathpar}
    %
    \jform{\vdash \Gamma}
    \inferdef{EmptyCon}
      { }{\vdash \cdot}
    \and
    \inferdef{ConsCon}
      {\vdash \Gamma \\ \Gamma \vdash A : \uni}{\vdash \Gamma, x : A}
    \\\\
    \jform{\Gamma \vdash t : T}
    \inferdef{Var}{(x : A) \in \Gamma \\ \vdash \Gamma}{\Gamma \vdash x : A}
    \and
    \inferdef{Univ}
      {\vdash \Gamma}
      {\Gamma \vdash \uni[i] : \uni[\unext{i}]}
    \and
    \inferdef{Prod}
      {\Gamma \vdash A : \uni[i] \\ \Gamma, x : A \vdash B : \uni[j]}
      {\Gamma \vdash \P x : A.\ B : \uni[\umax{i}{j}]}
    \and
    \inferdef{Abs}{\Gamma, x : A \vdash t : T}{\Gamma \vdash \l x : A .\ t : \P x : A.\ T}
    \and
    \inferdef{App}
      {\Gamma \vdash f : \P x : A.\ T \\ \Gamma \vdash u : A }
      {\Gamma \vdash f\ u : \subs{T}{x}{u}}
    \and
  \inferdef{Conv}
    {\Gamma \vdash t : T \\ \Gamma \vdash T \conv T' : \uni}
    {\Gamma \vdash t : T'}
  \end{mathpar}

  \caption{Collected typing rules for \kl{CCω}}
  \label{fig:ccw-typing}
\end{figure*}

Conversion is a very complex beast, arguably the most subtle part of dependent types.
Consequently, there are very different ways to present it, which in turn serve different
needs.
For this reason, we took care to setup the typing rules of
\cref{fig:ccw-typing} so that \emph{nothing} has to
be changed in those when one definition of conversion or another is taken. The only
difference is in how the relation $\Gamma \vdash T \conv T' : \uni$ is defined.
This way, we can treat it as a black box when talking about typing,
making the design modular.

A first divide is between \intro[typed conversion]{typed} and
\intro[untyped conversion]{untyped} conversion.
On one side, conversion is intrinsically typed, terms are only convertible
\emph{at a given type}. On the other, conversion is a relation between raw terms,
that does not presuppose any form of typing. \Cref{fig:typed-untyped-conv} gives an
example of the "same" rule – the computation rule for functions – in both systems.
As we can see, the content of the two rules is the same, it equates $(\l x : A.\ t)\ u$
and $\subs{t}{x}{u}$, only the side-conditions differ wildly.
Typed conversion goes back to
\sidetextcite{MartinLoef1972}, and is a recurring feature in its many descendants.
Untyped conversion relates strongly to (untyped) λ-calculus – Barendregt
for instance uses the name "conversion" for the equational theory of untyped λ-calculus
in his reference work on the subject~\sidecite{Barendregt1985} –, via
the Pure Type Systems (PTS)~\sidecite{Barendregt1991} literature.
In this thesis, we mainly consider untyped conversion, as \kl{Coq}’s meta-theory
has been mostly studied in that tradition.
But the relation between both in the context of
bidirectional typing is the main subject of \cref{chap:bidir-conv}.

\begin{figure}[h]
  \begin{mathpar}
    \inferrule
      {\Gamma, x : A \vdash t : B \\ \Gamma \vdash u : A}
      {\Gamma \vdash (\l x : A.\ t)\ u \tdconv \subs{t}{x}{u} : \subs{B}{x}{u}}
    \and
    \inferrule{ }{(\l x : A.\ t)\ u \udconv \subs{t}{x}{u}}
  \end{mathpar}
  \caption{Example: typed and untyped β rule for conversion}
  \label{fig:typed-untyped-conv}
\end{figure}

A second axis is about how close the conversion relation is to an implementation.
For instance, conversion should be an equivalence relation,
but there are two approaches to that. The first – and standard – one
is to simply \emph{define} conversion as an equivalence relation, by adding rules 
for \eg transitivity, as the one of \cref{fig:trans-conv}.
\begin{marginfigure}
  \begin{mathpar}
    \inferrule
      {t \udconv t' \\ t' \udconv t''}
      {t \udconv t''}
  \end{mathpar}
  \caption{Example: transitivity rule for conversion}
  \label{fig:trans-conv}
\end{marginfigure}
This ensures that conversion has the right properties, but means it does not directly correspond
to an algorithm: the transitivity rule cannot be directly implemented, since its middle
term is not recorded in any place.
The λ-calculus theorists have known this issue for a long time, and they
have a solution: characterizing conversion by means of a \kl{reduction} relation $\red$, which
corresponds to the idea of program evaluation – see \sidetextcite{Barendregt1985} for
instance. If this reduction has good
properties\sidenote{The main one being confluence.}, then
two terms are be convertible exactly when they reduce to the same third term.
This much more operational characterization is closer to what can be implemented.
Turning things around, one can define conversion through reduction,
and only show \emph{afterwards}
that it has the good properties of the first approach – typically, that it is transitive.
Conversion of the first kind we call \intro{declarative conversion}, while for the second
we talk about \intro{algorithmic conversion}.

In the rest of this section we give two presentations of \kl{untyped conversion}.
A \kl{declarative} one, which we use to define \kl{CCω}, as is the standard.
And an \kl{algorithmic} one, anticipating \arefpart{metacoq} where it is needed
to show decidability of type-checking, and
\arefpart{gradual} where we extend it into a relation that is by design not transitive, so
that basing it on declarative conversion would be nonsensical.

\subsection{Untyped declarative conversion}

\begin{marginfigure}
  \ContinuedFloat*
  \begin{mathpar}
    \inferdef{UConv}{\Gamma \vdash T' : \uni \\ T \udconv T'}{\Gamma \vdash T \conv T' : \uni}
    \label{rule:cic-conv-unty}
  \end{mathpar}
  \caption{Typing constraint on untyped conversion}
\end{marginfigure}

To start our presentation of \kl{untyped conversion},
let us first go back to \ruleref{rule:cic-conv}.
Even if we wish to describe conversion as
an untyped relation, we still enforce a typing constraint in \ruleref{rule:cic-conv},
in order to ensure that, whenever $\Gamma \vdash t : T$ is derivable,
$\Gamma \vdash T : \uni$ is as well.
This is exactly the content of \ruleref{rule:cic-conv-unty}, which combines conversion
with a check that the target type is indeed a well-formed type.

\begin{figure}[h]
  \ContinuedFloat
  \begin{mathpar}
    \inferdef{ConvRefl}{ }{t \udconv t}
    \label{rule:cic-uconv-refl} \and
    \inferdef{ConvSym}{t \udconv t'}{t' \udconv t}
    \label{rule:cic-uconv-sym} \and
    \inferdef{ConvTrans}
      {t \udconv t' \\ t' \udconv t''}
      {t \udconv t''}
    \label{rule:cic-uconv-trans}
  \end{mathpar}
  \caption{Equivalence rules}
  \label{fig:cic-uconv-equiv}
\end{figure}

Regarding conversion itself, the first set of rules asserts
that conversion forms an equivalence relation: it
is reflexive (\ruleref{rule:cic-uconv-refl}), symmetric (\ruleref{rule:cic-uconv-sym}),
and transitive (\ruleref{rule:cic-uconv-trans}).

A second set of rules, collected in \cref{fig:cic-uconv-cong},
asserts that conversion is a congruence, meaning that it is compatible
with all term formers. As for the previous three, these correspond to properties we expect
from the conversion relation, that we simply declare to be true. Note that we include only
congruence rules for term formers with sub-terms – we \eg omit $\uni$. This is because
those are special cases of \ruleref{rule:cic-uconv-refl}. Conversely, we could omit 
reflexivity altogether and only have congruence rules, which can be seen as a generalized
form of reflexivity.

\begin{figure}[h]
  \ContinuedFloat
  \begin{mathpar}
    \inferrule
    % \inferdef{ProdConv}
      {A \udconv A' \\ B \udconv B'}
      {\P x : A.\ B \udconv \P x : A'.\ B'}
    % \label{rule:cic-uconv-prod}
    \and
    \inferrule
    % \inferdef {AbsConv}
      {A \udconv A' \\ t \udconv t'}
      {\l x : A .\ t \udconv \l x : A'.\ t'}
    % \label{rule:cic-uconv-abs}
    \and
    % \inferdef{AppConv}
    \inferrule
      {f \udconv f' \\ u \udconv u' }
      {f\ u \udconv f'\ u'}
    % \label{rule:cic-uconv-app}
  \end{mathpar}
  \caption{Congruence rules}
  \label{fig:cic-uconv-cong}
\end{figure}

\begin{marginfigure}
  \ContinuedFloat
  \begin{mathpar}
    \inferdef{βConv}{ }{(\l x : A.\ t)\ u \udconv \subs{t}{x}{u}}
    \label{rule:cic-uconv-beta}
  \end{mathpar}
  \caption{Computation rule for functions}
\end{marginfigure}
Finally \ruleref{rule:cic-uconv-beta}, is the crucial one:
it corresponds to the computational behaviour
of functions, replacing the variable of an applied λ-abstraction by its argument by
means of substitution.

\subsection{Reduction}

Before we can describe \kl{algorithmic conversion}, we first need
to give a look at \intro{reduction}. Reduction is in some way an operational variant of
conversion. The main difference is that it is oriented, in the direction which would
correspond to program evaluation. It itself decomposes into three components.

\begin{marginfigure}
  \ContinuedFloat*
  \begin{mathpar}
    \inferdef{βRed}{ }{(\l x : A.\ t)\ u \tred \subs{t}{x}{u}}
    \label{rule:beta-red}
  \end{mathpar}
  \caption{Top-level reduction}
\end{marginfigure}

The first is \intro{top-level
reduction} $\tred$, which corresponds purely to computation, without any congruent closure.
In \kl{CCω} there is only the single \ruleref{rule:beta-red}.

The second component is the congruent closure of top-level reduction,
\intro{one-step reduction} $\ored$, which allows triggering top-level reduction exactly once,
but at any position in a term. Its definition is given in \cref{fig:ccw-ored}.
Note that while we talk about congruent closure both for
\kl{conversion} and \kl{one-step reduction}, we mean it differently: in the
case of conversion, we demand the relation to hold in all sub-terms,
while for one-step reduction it is allowed in exactly one position.

\begin{figure}[ht]
  \ContinuedFloat
  \begin{mathpar}
    \inferrule
    % \inferdef{TopRed}
      {t \tred t'}
      {t \ored t'}
    % \label{rule:top-red}
    \and
    \inferrule
    % \inferdef{ProdRedDom}
      {A \ored A'}
      {\P x : A.\ B \ored \P x : A'.\ B}
    % \label{rule:red-prod-dom}
    \and
    \inferrule
    % \inferdef{ProdRedCod}
      {B \ored B'}
      {\P x : A.\ B \ored \P x : A.\ B'}
    % \label{rule:red-prod-cod}
    \and
    \inferrule
    % \inferdef{AbsRedDom}
      {A \ored A'}
      {\l x : A .\ t \ored \l x : A'.\ t}
    % \label{rule:red-abs-dom}
    \and
    \inferrule
    % \inferdef{AbsRedBod}
      {t \ored t'}
      {\l x : A .\ t \ored \l x : A.\ t'}
    % \label{rule:red-abs-bod}
    \and
    \inferrule
    % \inferdef{AppRedFun}
      {f \ored f'}
      {f\ u \ored f'\ u}
    % \label{rule:red-app-fun}
    \and
    \inferrule
    % \inferdef{AppRedArg}
      {u \ored u'}
      {f\ u \ored f\ u'}
    % \label{rule:red-app-arg}
  \end{mathpar}
  \caption{One-step reduction}
  \label{fig:ccw-ored}
\end{figure}

Finally, we obtain \kl{reduction} as the reflexive
transitive closure of one-step reduction, see \cref{fig:red}.

\begin{figure}[h]
  \begin{mathpar}
    \inferrule
    % \inferdef{TopRed}
      {t \tred t'}
      {t \ored t'} \and
    % \label{rule:top-red}
    \inferrule{ }{t \fred t}
    \and
    \inferrule
      {t \ored t' \\ t' \fred t''}
      {t \fred t''}
  \end{mathpar}
  \caption{Reduction}
  \label{fig:red}
\end{figure}

Now, the reason we separate these three layers is that reduction as just defined is
somewhat too unrestricted, in particular it is non-deterministic.
In a lot of places, what we care about is exposing the head
constructor of a term, and there is a deterministic strategy we can impose to do so, called
\intro{weak-head reduction} $\hred$. It amounts to restricting the places in the term where
\kl{top-level reduction} can be used, by removing some congruence rules. More precisely,
λ-abstractions, product types and universes are not reduced, as they already are canonical
forms of their types. Variables are not reduced either, since they simply cannot be. Thus,
the only reduction that is allowed is in the function position of an application, with the
hope to get an abstraction there that can be further reduced at top-level. In the end,
we get \cref{fig:wh-red}. When we want to contrast this weak-head reduction with the
previously defined one $\red$, we call the latter \kl{full reduction}.
\begin{figure}[h]
  \begin{mathpar}
    \inferrule
    % \inferdef{TopRed}
      {t \tred t'}
      {t \hored t'}
    % \label{rule:top-red}
    \and
    \inferrule
      {f \hored f'}
      {f\ u \hored f'\ u}
    \and
    \inferrule{ }{t \hred t}
    \label{rule:red-refl} \and
    \inferrule
      {t \hored t' \\ t' \hred t''}
      {t \hred t''}
    \label{rule:red-trans}
  \end{mathpar}
  \caption{Weak-head redution}
  \label{fig:wh-red}
\end{figure}



\subsection{Algorithmic conversion}

\section{Adding Inductive Types: \kl{CIC}}

\subsection{Datatypes}

\subsection{Indexed inductive types}

\section{Beyond \kl{CIC}: \kl{PCUIC}}

\subsection{Cumulativity}

\subsection{The sort of propositions}

\subsection{Local and global definitions}

\subsection{Enhancing inductive types}

\subsection{Co-inductive types and records}