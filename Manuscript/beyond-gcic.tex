\chapter{Beyond \kl(tit){CastCIC}: Models, Indices and Pure Reasoning}
\label{chap:beyond-gcic}

\cref{chap:bidir-gradual-elab} establishes quite a few properties of \kl{GCIC} and
\kl{CCIC}, culminating with \kl{elaboration graduality}. This is, however, still far
from satisfactory. First, it is missing proofs of \kl{normalization} for the two variants
which are supposed to satisfy it – \kl{CCICT} and \kl{CCICs} –, and of \kl{graduality} – for
\kl{CCICP} and \kl{CCICs}.
Next, it only treats \kl{CIC-}, \ie it does not handle indexed inductive types.
But these are crucial in order to really exploit dependency:
for instance, most of our introductory examples were based on the vector type, an indexed
inductive type outside the scope of \cref{chap:bidir-gradual-elab}.

In this chapter we go over these issues and possible solutions:
\cref{sec:realizing-cast-calculus} describes model constructions to establish
both \kl{normalization} and \kl{graduality};
\cref{sec:indices-issue} describes the issue with indexed inductive types, and gives
possible solutions in case the indices are “nice enough”, which covers vectors; finally,
\cref{sec:ReTT} gives a much more ambitious solution to handle indices, giving a proper
treatment of the equality type – the stereotypical pathologic inductive type –, and much more.

As these have no direct relation to bidirectional typing \textit{per se},
we do not dwell on the technical details in this chapter.
The interested reader can consult either \sidetextcite{LennonBertrand2022} –
\cref{sec:realizing-cast-calculus} corresponds roughly to
Section 6 there, and \cref{sec:indices-issue} to Sections 6 and 8.3 –, or
\sidetextcite{Maillard2022} – corresponding to \cref{sec:ReTT}.

\section{Realizing \kl(tit){CastCIC}}
\label{sec:realizing-cast-calculus}

\subsection{The discrete model}

To inform the design and justify the reduction rules provided for
\kl{CCIC}, we build a syntactic model%
\sidenote{
  \intro{Syntactic models} \cite{Boulier2017,Boulier2018},
  are a kind of models of type theory defined directly by induction
  on the raw syntax, in a way akin to program translation or compilation. This allows for
  simple models, that moreover can be used to capture fine-grained properties that only
  make sense on that raw syntax,
  typically those that need to separate between convertible terms.
}%
\margincite{Boulier2017}%
\margincite{Boulier2018}
of \kl{CCIC} by translation to \kl{CIC}
augmented with induction-recursion
\sidecite{MartinLoef1996,Dybjer2003,Ghani2015}.

From a type theoretical point of view, what makes \kl{CCIC} peculiar are the
possibility of having \emph{exceptions} – both “pessimistic” ($\err$) and
“optimistic” ($\?$) –, and the necessity to do \emph{intensional type
analysis} in order to resolve casts. For the former, we build upon the work of
\sidetextcite{Pedrot2018} on the exceptional type theory \kl{ExTT}.
For the latter, we reuse the technique of \textcite{Boulier2017} to equip the universe with
an elimination principle $\intro*\typerec$%
\sidenote{Corresponding to a form of ad-hoc polymorphism.},
which requires induction-recursion to be implemented.
%

\AP We call this syntactic model of \kl{CCIC} the \intro{discrete model}.
% , in contrast with a semantic model motivated by \kl{graduality}. This \kl{discrete model}
It captures the intuition that the unknown type is
inhabited by “tagged values”, \eg a term together with its type.
In other words, the \kl{unknown type} $\?[\uni]$ behaves as a dependent sum $\Sb A : \uni.\ A$.
Projecting out of it is realized through type analysis using $\typerec$,
and may fail – raising an error in the \kl{ExTT} sense.

Note that we provide a particular interpretation of the unknown
term in the universe, which is legitimized by an observation
made by \textcite{Pedrot2018}: \kl{ExTT} does not constrain in
any way the definition of exceptions in the universe.
This is crucial to combine \kl{ExTT} with a universe equipped with $\typerec$.
%

The key point to prove \kl{normalization} is that reduction is preserved,
in the sense that a reduction step in the source theory \kl{CCIC} is mapped to at least one step
in the target. Thus, the target being \kl{normalizing}, so is \kl{CCIC}.

\begin{theorem}[Normalization for \kl{CCIC}]
  Both \kl{CCICT} and \kl{CCICs} have the \kl{normalization}
  property (\cref{prop:normalization}).  
\end{theorem}

An important corollary of this property, in combination with \kl{safety}
(\cref{thm:ccic-psafe}), is a weak form of \kl{logical consistency}, characterizing the
possible inhabitants of the empty type:

\begin{theorem}[Weak logical consistency]
  Suppose $\tcol{t}$ is a closed inhabitant of the empty type
  $\tcol{\Empty}$ in \kl{CCICT} or \kl{CCICs}, \eg $\vdash \tcol{t} \cty \tcol{\Empty}$.
  Then $\tcol{t}$ must reduce to either $\tcol{\err[\Empty]}$ or $\tcol{\?[\Empty]}$. 
\end{theorem}

\subsection{The monotone model}

The simplicity of the discrete model comes at the price of an inherent inability to
characterize which casts are guaranteed to succeed, \ie a \kl{graduality} theorem.
%
\AP To overcome this limitation, and prove graduality of \kl{CCICs},
we can build a more elaborate \intro{monotone model}, inducing a
precision relation that is well-behaved with respect to conversion.

In this model, each type $A$ comes equipped with an order
structure $\sqsubseteq^A$ – a reflexive, transitive, antisymmetric and
proof-irrelevant relation – modelling precision between terms.
%
In particular, the exceptions $\err[A]$ and $\?[A]$ correspond respectively to the smallest
and greatest element of $A$ for this order.
%
Saying that this interpretation of types as posets is a model is equivalent to
saying that each term and type former is enforced to be monotone,
providing a strong form of \kl{graduality}.
%
This implies in particular that such a model cannot be defined for \kl{CCICT},
as this type theory lacks graduality.

\AP The precision order of the monotone model can be reflected back to
\kl{CCIC}, giving rise to the \intro{propositional precision} judgment
$\tcol{\Gamma} \vdash \tcol{t} \intro*\precision{\tcol{T}}{\tcol{U}} \tcol{u}$, where
$\tcol{T}$ and $\tcol{U}$ are the respective types of $\tcol{t}$ and $\tcol{u}$.
Type dependency naturally demands such a notion of inhomogeneous precision,
rather than a simpler notion relating only terms of the same type.

This precision relation bears a similar relationship to \kl{definitional precision}
$\cdpre$ as propositional equality to \kl{conversion}/definitional equality in \kl{CIC}.
%
\kl{Propositional precision} can be used to prove precision statements
inside the target type theory, for instance we can show by
case analysis on $\tcol{b} : \tcol{\Bool}$ that 
\[ \tcol{b} : \tcol{\Bool} \vdash \tcol{\ind{\Bool}{b}{x.\uni}{A,A}}
\precision{\tcol{\square}}{\tcol{\square}} \tcol{A}\]
a judgment that does not hold for \kl{definitional precision}.
%
In particular, \kl{propositional precision} is invariant by \kl{conversion}: if
$\tcol{t} \aconv \tcol{t'}$, $\tcol{u} \aconv \tcol{u'}$ and
$\tcol{\Gamma} \vdash \tcol{t} \precision{\tcol{T}}{\tcol{U}} \tcol{u}$
then
$\tcol{\Gamma} \vdash \tcol{t'} \precision{\tcol{T}}{\tcol{U}}
\tcol{u'}$.
%
But this means that \kl{propositional precision} is too coarse to capture properties such
as the \kl{simulation} property (\cref{thm:simulation}) and its corollaries
(\cref{cor:red-types,cor:mon-cons}), because these distinguish \kl{convertible} terms.

Still, we can relate the two notions, as follows:
\begin{theorem}[Compatibility of \kl{structural} and \kl{propositional} precision]
  \label{thm:prop-prop-prec}
   
  If $\cinferty{\Gamma}{t}{T}$, $\cinferty{\Gamma}{u}{U}$ and
  $\tcol{\Gamma \mid \Gamma} \vdash \tcol{t} \capre \tcol{u}$, then
  $\tcol{\Gamma} \vdash \tcol{t} \precision{\tcol{T}}{\tcol{U}} \tcol{u}$.
  
  Conversely, if
    $\emptycon \vdash \tcol{v_1} \precision{\tcol{\Bool}}{\tcol{\Bool}}
    \tcol{v_2}$ for normal forms $\tcol{v_1}, \tcol{v_2}$, then
    $\emptycon \vdash \tcol{v_1} \capre \tcol{v_2}$.

\end{theorem}
Again, this is similar to the relation between \kl{conversion} and propositional equality:
the former always implies the latter, and one can come back from the second to the first in
a constrained enough setting – here, on closed booleans.

Finally, the main property satisfied by \kl{propositional precision} is \kl{graduality}:

\begin{theorem}[\kl{Graduality}]
  \label{thm:graduality}

  \kl{Propositional precision} satisfies the \kl{Dynamic Gradual Guarantee}: if
  $\cinferty{\Gamma}{t}{T}$, $\cinferty{\Gamma}{t'}{T}$ and
  $\tcol{\Gamma} \vdash \tcol{t} \precision{\tcol{T}}{\tcol{T}} \tcol{t'}$ hold, then
  $ \tcol{t} \obsRef \tcol{t'}$.
  
  Casts form \kl{embedding-projection pairs}. That is, if
  $\cinferty{\Gamma}{t}{T}$ and $\cinferty{\Gamma}{u}{U}$,
  and moreover
    $\tcol{\Gamma} \vdash \tcol{T} \precision{\tcol{\square}}{\tcol{\square}} \tcol{U}$,
  then the following three properties are equivalent:
  \begin{align*}
    \tcol{\Gamma} \vdash \tcol{\cast{T}{U}{t}} \precision{\tcol{U}}{\tcol{U}} \tcol{u}
    \Leftrightarrow  \tcol{\Gamma} \vdash \tcol{t} \precision{\tcol{T}}{\tcol{U}} \tcol{u}
    \Leftrightarrow  \tcol{\Gamma} \vdash \tcol{t}
    \precision{\tcol{T}}{\tcol{T}} \tcol{\cast{U}{T}{u}}
  \end{align*}
  And furthermore,
    $\tcol{\Gamma} \vdash \tcol{\cast{U}{T}{\cast{T}{U}{t}}} \precision{\tcol{T}}{\tcol{T}} \tcol{t}$ – this is the retraction property.
\end{theorem}



\section{The issue with indices: gradual vectors and equalities}
  [Gradual vectors and equalities]
\label{sec:indices-issue}

\subsection{The issue with propositional equality}

% In \kl{CIC}, propositional equality \coqe{eq A x y}, corresponds to the Martin-L{\"o}f
% identity type~\cite{Martin-Lof-1973}, with a single constructor \coqe{refl} for reflexivity, and the
% elimination principle known as \coqe{J}:
% %
% \begin{coq}
%   Inductive eq (A : Type) (x : A) : A -> Type := refl : eq A x x.
% \end{coq}
% \begin{coq}
%   J : forall (A : Type) (P : A -> Type) (x : A) (t : P x) (y : A) (e : eq A x y), P y
% \end{coq}
% together with the conversion rule:

% \begin{center}
%  \coqe{J A P x t x (refl A x) equiv t}
% \end{center}

For the sake of exposing the problem, suppose that we can define the equality type
$\tcol{\eqty[A]{a}{a'}}$ in \kl{CCIC}, 
while still satisfying \kl{canonicity}, \kl{conservativity} and \kl{graduality}.
%
This means that for an equality $\tcol{\eqty{t}{u}}$ involving closed terms
$\tcol{t}$ and $\tcol{u}$ of \kl{CIC}, there should only be three possible canonical forms:
$\tcol{\refl[A][t]}$ whenever $\tcol{t}$ and $\tcol{u}$ are convertible terms,
as well as $\tcol{\err}$ and $\tcol{\?}$.

Just under these assumptions, 
we can show that there exist two functions that are pointwise equal in \kl{CIC},
but are no longer equivalent in \kl{CCIC}. 
Consider the two functions $\tcol{\id_{\Nat}}$ and $\tcol{\addz}$ defined respectively as
$\tcol{\id_{\Nat}} \coloneqq \tcol{\l x : \Nat.\ n}$ and
$\tcol{\addz} \coloneqq \tcol{\l x : \Nat.\ x + \z}$.
In \kl{CIC}, these functions are not convertible, but they are pointwise equal,
and observationally equivalent.
However, they would not be observationally equivalent in \kl{GCIC} under our assumptions.
%
To see why, consider the following term:
\[\tcol{\test} \coloneqq \tcol{\l f : \Nat \to \Nat. \ind{\eqtyop}{y.z.\Bool}{\cast{\eqty{\id_{\Nat}}{\id_{\Nat}}}{\eqty{\id_{\Nat}}{f} \Leftarrow \?[\uni]}{\refl}}{\true}} \]

We have $\tcol{\test \id_{\Nat} \red \tcol{\true}}$ because, by \kl{graduality}, the
upcast-downcast in the scrutinee must succeed, \ie
\[\tcol{\cast{\eqty{\id_{\Nat}}{\id_{\Nat}}}{\eqty{\id_{\Nat}}{\id_{\Nat}} \Leftarrow \?[\uni]}{\refl}} \red \tcol{\refl[\id_{\Nat}]}\]
However, since $\tcol{\addz}$ is \emph{not} \kl{convertible} to $\tcol{\id_{\Nat}}$,
\[\tcol{\cast{\eqty{\id_{\Nat}}{\id_{\Nat}}}{\eqty{\id_{\Nat}}{\addz} \Leftarrow \?[\uni]}{\refl}}\]
cannot possibly reduce to $\tcol{\refl}$, and
thus would need to reduce either to $\tcol{\err}$ or $\tcol{\?}$;
and so does $\tcol{\test \addz}$.
%
This means that the theory would be very intensional, by being able to distinguish between
any two functions that are not \kl{convertible}, even if they are pointwise equal.

More generally, the issue is the following: given a constructor $\tcol{c}$
of an inductive type $\tcol{I}$, we need to decide what to do when confronted with some
$\tcol{\cast{I(b_1, \dots, b_n)}{I(b'_1, \dots, b'_n)}{c(a_1, \dots, a_m)}}$.
If $\tcol{I}$ does not have indices, as in the case of lists, we know that $\tcol{c}$
can always be used to inhabit $\tcol{I(b_1', \dots, b_n')}$,
if given arguments of the right types.
If $\tcol{I}$ has indices, however, this might not be possible due to typing constraints,
as in the example of $\tcol{\refl}$.
But we still need to provide an inhabitant of that type as redex for the cast! If we resort to
the wildcards $\tcol{\?}$ or $\tcol{\err}$,
then we expose a very intensional behaviour such as the one above.
However, in the setting of a generic inductive type
– such as that of the equality –, deciding whether it is inhabited by a “valid”,
non-wildcard, term in a given non-empty context is undecidable,
so we cannot hope to always decide whether the cast should fail or
return such a valid term.

\subsection{Solutions for vectors}

Thankfully, not all indexed inductive types are as thorny as equality. Indeed,
in examples such as $\Vect$,%
\sidenote{And quite a lot of those used in the context of dependently typed programming.}
solutions are possible that avoid the dead-end identified for equality, by carefully
using the structure of indices. These rely on two well-known alternatives to indexed
inductive types for capturing properties intrinsically: type-level eliminators,
and “forded” inductive types.

\paragraph{Type-level eliminators.}

Instead of an inductive type, $\Vect$ can be defined as a recursive function on its index,
at the type level, effectively representing lists as nested pairs:
\[\FVect \coloneqq \l (A : \uni) (n : \Nat). \ind{\Nat}{n}{x.\uni}{\Unit,y.p_y.A \times p_y}\]
corresponding to the \kl{Coq} definition

\begin{coqcode}
  Fixpoint FVect (A : Type) (n : ℕ) : Type :=
    match n with 0 => unit | S n => A * FVec A n end.
\end{coqcode}

\AP Type-level eliminators can be used as soon as the indices are \intro{concretely
forceable} \sidecite{Brady2004}. Intuitively, \kl{concretely forceable} indices
are those that can be matched upon (like $n$ in the example of $\Vect$).
\sidetextcite{Gilbert2019} give a general translation of this kind
to build mock-up inductive types inside a sort of definitionally irrelevant propositions.

This presentation coincides with the indexed inductive one  on standard, non-exceptional terms, but quickly becomes very imprecise in presence of unknown indices.

\paragraph{Forded inductive type.}

Instead of using an indexed inductive type, one can use a parametrized inductive type, with \emph{explicit} equalities as arguments to constructors.% 
\sidenote{This technique has been coined “fording” by \textcite[Section 3.5]{McBride1999},
as an allusion to Henry Ford' quote “\textit{Any customer can have a car painted any color that he wants, so long as it is black.}”}%
\margincite{McBride1999}
For instance, vectors can be defined in this style as follows:
\begin{coqcode}
  Inductive Vectf (A : Type) (n : ℕ) : Type :=
  | nilf : eq_nat 0 n -> Vectf A n
  | consf : A -> forall m : ℕ, eq_nat (S m) n
      -> Vectf A m -> Vectf A n.
\end{coqcode}
Here, \coqe{eq_nat} is the type of \emph{decidable} equality proofs between natural numbers,
expressing the constraints on $n$ – \eg $\eqty{n}{\z}$ – but avoiding the use of the unavailable propositional equalities –, which can be defined like this:
\begin{coqcode}
  Fixpoint eq_nat (m n : ℕ) : Type :=
  match m, n with
    | 0, 0 => True
    | S m, S n => eq_nat m n
    | _, _ => False
  end.
\end{coqcode}

This presentation is more accurate than the previous one when dealing with unknown indices,
but is too permissive with invalid index assertions. It fails very late when such invalid
assertions are made, meaning that error-reporting is bad.

\paragraph{Direct support.}

In contrast to the two previously-exposed encodings that both have serious shortcomings, 
extending \kl{CCIC} with direct support for indexed inductive types can provide a
much more satisfactory solution. The idea is to reason about indices directly in
the reduction of casts. 

To do so, we first add two new canonical forms,
corresponding to the casts of $\tcol{\vnil}$ and $\tcol{\vconsop}$ to
$\tcol{\GVect(A,\?[\Nat])}$: namely, $\tcol{\gvnil[A]}$ and
$\tcol{\gvcons[A][n]{a}{v}}$.

\begin{figure*}[ht]
\ContinuedFloat*
\begin{mathpar}
  % \inferrule{  }{\can{\GnilU{A}}}
  % \and
  % \inferrule{ }{\can{(\GconsU{A}{a}{n}{v})}}
  % \\
  \redrule{\tcol{
    \castrev{\left(\vcons[A][k]{a}{v}\right)}{\GVect(A,\S n)}{\GVect(B,\?[\Nat])} 
  }}
  {\tcol{ \gvcons[B][n]{\left(\castrev{a}{A}{B}\right)}{\left(\castrev{v}{\GVect(A,k)}{\GVect(B,n)}\right)}
  }}[V-cons-?] \label{red:v-S?} \and
  \redrule{\tcol{
    \castrev{\left(\gvcons[A][k]{a}{v}\right)}{\GVect(A,\?[\Nat])}{\GVect(B,\S n)}
  }}
  {\tcol{ \vcons[B][n]{\left(\castrev{a}{A}{B}\right)}{\left(\castrev{v}{\GVect(A,k)}{\GVect(B,n)}\right)}
  }}[V-cons?-s] \label{red:v-?S} \and
  \redrule{\tcol{
    \castrev{\left(\vcons[A][k]{a}{v}\right)}{\GVect(A,\S n)}{\GVect(B, \S m)} 
  }}
  {\tcol{\vcons[B][m]{\left(\castrev{a}{A}{B}\right)}
    {\left(\castrev{v}{\GVect(A,k)}{\GVect(B,m)}\right)}
  }}[V-cons-s]\label{red:v-S}
  \and
  \redrule{\tcol{
    \castrev{\left(\vcons[A][k]{a}{v}\right)}{\GVect(A,\S n)}{\GVect(B,\z)} 
  }}{\tcol{
    \err[\GVect(B,\z)]
  }}[V-cons-0]\label{red:v-S0} \and
  \redrule{\tcol{
    \castrev{\left(\gvcons[A][k]{a}{v}\right)}{\GVect(A,\?[\Nat])}{\GVect(B,\z)} 
  }}{\tcol{
    \err[\GVect(B,\z)]
  }}[V-cons?-0]\label{red:v-S?0}
\end{mathpar}
\caption{Casts between gradual vector types (excerpt)}
\label{fig:grad-vect-casts}
\end{figure*}

We then extend \kl{reduction} to account for casts on vectors
in canonical forms. \cref{fig:grad-vect-casts} presents these rules
when the argument of the cast is a non-empty vector.
%
\ruleref{red:v-S?} propagates the cast on the arguments,
but using the newly introduced $\tcol{\gvconsop}$. This effectively loses precision in the type
information, but keeps it all recorded in the term, so that it can be used
in case of a downcast. This is exactly what \ruleref{red:v-?S} does.
%
\ruleref{red:v-S} applies when both source and target indices are successors,
and propagates the cast of the arguments, just like in the case of lists.
%
Finally, as expected, \ruleref{red:v-S0} raises an error when the indices do not match.
Similarly, \ruleref{red:v-S?0} also raises an error when trying to create a vector of length
$\z$ from one with an unknown index, but whose underlying vector is non-empty.

\begin{figure}[ht]
\ContinuedFloat
\begin{mathpar}

\redrule{\tcol{
  \ind{\Vect}{\gvnil[A]}{y.z.P}{b_{\vnil},y_1.y_2.y_3.p_{y_3}.b_{\vconsop}}
}}
{\tcol{\castrev{b_{\vnil}}{\mathop{P}\z}{\mathop{P} \?[\Nat]}
}}[V-rect-nil?]\label{red:v-rect-unk-0}

\end{mathpar}
\caption{Eliminator for the gradual vector type (excerpt)}
\label{fig:vectors-excerpt}
\end{figure}

For the eliminator, there are two new computation rules, one for each
new constructor. We give the one for the case of $\tcol{\gvnil}$,
this is \ruleref{red:v-rect-unk-0}. These rules apply the eliminator to the 
underlying non-exceptional constructor,
and then cast the result to $\tcol{\mathop{P} \?[\Nat]}$.
Intuitively, they transfer the cast on vectors to a cast on the reduct.

Note that all of these rules crucially use the fact that it is possible to discriminate between
$\tcol{\z}$, $\tcol{\S n}$ and $\tcol{\?[\Nat]}$,
which is a specificity of the vector type and explains why
this solution is not possible for \eg the equality.

This “definitive” presentation is justified by a modification of the models described in
\cref{sec:realizing-cast-calculus}, and gives the satisfactory behaviour described
in the examples of \cref{sec:indices,sec:specif}: it preserves as much computational content
as possible, while failing early when invalid assumptions are used.


\section{A Reasonably Gradual Type Theory}
\label{sec:ReTT}

In the context of a gradual proof assistant based on \kl{CIC},
the \kl{normalizing} and \kl{conservative} variant \kl{GCICT} is the most appealing,
as it ensures decidability of typing, (weak) \kl{canonicity}, and supports all existing developments and libraries by virtue of being a \kl{conservative extension} of \kl{CIC}.
Unfortunately, the universe shift introduced in \kl{CCICT} during reduction
means that some terms break graduality. 
For instance, while the term
\[\tcol{\nArrow} \coloneqq
  \tcol{\l n : \Nat.\ \ind{\Nat}{n}{x.\uni[0]}{\Nat,y.p_{y}.\Nat \to p_{y}}} \]
or, in \kl{Coq},
\begin{coqcode}
  Fixpoint nArrow (n : ℕ) : Type :=
  match n with 
    | 0 => ℕ
    | S m => ℕ -> nArrow n
  end.
\end{coqcode}
is well-typed in \kl{GCICT},
the type $\tcol{\P n : \Nat. \nArrow n}$ does not satisfy the \kl{embedding-projection} property
with respect to any unknown type $\tcol{\?[\uni[i]]}$,
because the appropriate universe level is not known \textit{a priori}.
However, apart from the fact that \kl{GCICT} does not satisfy \kl{graduality} globally,
little is known about its gradual properties as its metatheory
in this regard has not been developed. In particular, there is no clear characterization
of a class of terms for which \kl{graduality} holds.

\paragraph{A refined stratification of precision} 
%
\AP However, by refining the stratification of
\kl{precision} we can develop a full account of graduality for an extension
of \kl{CCICT}, called \intro{CCICPrec}. The key idea is that $\tcol{\?[\uni[i]]}$ should be
the least precise type among all types at level $i$ and below,
\emph{except} for dependent function types at level $i$
– which are however still less precise than $\tcol{\?[\uni[\unext{i}]]}$.

\AP We can precisely characterize problematic terms as those that are not \intro{self-precise} –
\ie more precise than themselves. For function types,
self-precision means monotonicity with respect to precision.
A recursive large elimination as in $\tcol{\nArrow}$ is not monotone because
there is no fixed level $i$ for which 
$\tcol{\nArrow n} \precision{}{} \tcol{\?[\uni[i]]}$, given 
$\tcol{n} \precision{}{} \tcol{\?[\Nat]}$.

On the contrary, we can prove that the \kl{dynamic gradual guarantee} holds in \kl{CCICPrec}
for \emph{any} \kl{self-precise} context, and that casts between types related by
\kl{precision} induce \kl{embedding-projection pairs} between \kl{self-precise} terms.
%
Therefore, this shift in perspective in the interpretation of the unknown type and
the associated notion of precision yields a gradual theory
that conservatively extends \kl{CIC}, is normalizing,
and satisfies graduality for a large and well-defined class of terms.

\paragraph{Internalizing precision, reasonably}
While we could study graduality for \kl{CCICPrec} externally,
we observe that we can exploit the expressiveness of the type-theoretic setting to
internalize precision and its associated reasoning.
In particular, this makes it possible to state and prove, within the theory itself,
results about (self-)precision and graduality for specific terms.
In a way, this is the natural next step following the definition of
\kl{propositional precision} in \cref{sec:realizing-cast-calculus}.

Introducing \intro{internal precision} in a gradual type theory however
requires us to address two main obstacles.
First, when adding exceptions to CIC \sidecite{Pedrot2018}, 
the theory becomes inconsistent as a logic,
because it is possible to inhabit any type $\tcol{A}$ by raising an
exception $\tcol{\err[A]}$.
In the gradual setting, there is also the alternative of using the \kl{unknown term}
$\tcol{\?[A]}$ to inhabit any type $\tcol{A}$.
If we want to support valid internal reasoning about precision and graduality,
we need to avoid these degenerate proofs and provide a logically consistent theory.
Second, the gradual type theory needs to satisfy extensionality principles in order to support 
the notion of precision as error approximation \sidecite{New2018}. 
% In particular, precision on function types 
% should be defined pointwise, \ie~functions should be extensionally monotone with respect to precision (if $t \precision{}{} u$ then 
% $f\;t \precision{}{} f\;u$ for any function $f$).
Embracing extensionality principles in an intensional type theory such as \kl{CIC}
is a challenge in itself.

\AP We can address both issues by combining recent advances in type theory:
the reasonably exceptional type theory \intro{RETT} \sidecite{Pedrot2019}
and the observational type theory \intro{TTOBS} \sidecite{Pujet2022}.
First, \kl{RETT} supports consistent reasoning about exceptional terms.
It features a layer of possibly exceptional terms,
and a separate layer of pure terms in which raising an exception is prohibited.
This way, the consistency of the logical layer is guaranteed,
while allowing non-trivial interaction with the exceptional layer.
Technically, the two layers are defined using two distinct universe hierarchies.
%
Second, based on the seminal work on Observational Type Theory \sidecite{Altenkirch2007}, 
\kl{TTOBS} provides a setoidal equality in a specific universe $\intro*\SProp$
of definitionally proof-irrelevant propositions. 
This universe of strict propositions, introduced by \sidecite{Gilbert2019}
and supported in recent versions of \kl{Coq} and \kl{Agda}, 
makes it possible to define an extensional notion of equality,
while trivializing the so-called higher coherence hell by imposing that any two proofs
of a given equality are \emph{convertible}.
% The resulting theory is arguably much simpler and closer to the current practice of proof assistants than 
% cubical type theory~\cite{cubicaltt,cubical-agda}, which is another approach to provide extensional principles with computational content.

A major insight of \kl{CCICPrec} is to realize that we can actually merge
the logical universe of \kl{RETT} used to reason about exceptional terms
with the universe $\SProp$ of proof-irrelevant propositions in order to 
define an internal notion of precision that is extensional and whose proofs
cannot be trivialized with exceptional terms.

\paragraph{Applications of internal precision}

In addition to supporting reasoning about the graduality of terms in a theory
that is not globally gradual, \kl{internal precision} makes it possible
to support gradual subset types, in which a type can be refined
by a proposition expressed using precision.
Moreover, in the literature, exception handling is never considered
when proving \kl{graduality} because this mechanism inherently allows terms that
do not behave monotonically with respect to \kl{precision}.
\kl{Internal precision} enables us to support exception handling in the impure layer
of the type theory,
and to consistently reason about the graduality (or not) of exception-handling terms.