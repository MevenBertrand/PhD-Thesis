% \pagelayout{fullwidth}
% \includegraphics{./figures/test.pdf}
% \pagelayout{margin}

\chapter{Formalized Meta-Theory of \kl(tit){PCUIC}}
\label{chap:metacoq-general}

\margintoc

Before we can attempt to build a certified kernel, we need a thorough meta-theoretical study
of the type system. This is necessary in order to show that the invariants used by the kernel – typically, well-formation of the objects it manipulates – 
are preserved during the type-checking algorithm.
The use of these invariants goes beyond correctness:
the cumulativity test used as a sub-routine by the \kl{kernel} needs to reduce terms, and,
since all functions in \kl{Coq} must be terminating,
this reduction is defined by well-founded induction on the \kl{normalization}
of well-typed terms.
Since evaluation is not normalizing on ill-typed terms, the mere \emph{definition} of the
cumulativity check relies on \kl{subject reduction} to be able to iterate reduction steps.

The properties under scrutiny in this chapter are not new,
and neither are the basic strategy of most proofs.
Indeed, the development roughly follows the architecture we already exposed in
\cref{sec:tech-properties}. The main difficulty is the scale: due to the
complexity of \kl{PCUIC}, even well-understood techniques are challenging to apply.
Moreover, subtleties that do not appear in a simpler setting become apparent –
typically pertaining to universe levels or general inductive types –,
demanding original ideas. Thus, rather than getting lost in
the gory details of the formalization which are best understood by looking at it
– and maybe replaying it –, we try and focus on describing these interesting subtleties.

In more details, we start with the main definitions : the syntax, cumulativity and typing
judgments (\cref{sec:meta-defs}). We follow with the basic
stability properties (\cref{sec:meta-stabilities}):
renaming, substitution, environment extension, etc.
Next comes the first important proof, that of confluence, and its multiple consequences
(\cref{sec:meta-confluence}). This leads to the properties pertaining to typing, culminating
with subject reduction (\cref{sec:meta-typing-prop}).
Finally, we discuss the place of normalization (\cref{sec:meta-normalization}).

\section{Setting up the Definitions: Terms, Cumulativity and Types}[Setting up the Definitions]
\label{sec:meta-defs}

\subsection{Terms}

First thing first: the syntax of terms, defined in \pcuicfile{Ast} and reproduced in \cref{fig:metacoq-ast}.

\begin{figure*}
  \coqfile{./code/PCUICAst.v}
  \caption{The Abstract Syntax Tree of terms in \kl{MetaCoq} (\pcuicline{Ast}{term}{194})}
  \label{fig:metacoq-ast}
\end{figure*}

It of course contains  the term formers introduced in \cref{chap:tech-intro}:
\coqe{tRel} for variables, \coqe{tAbs} for abstractions,
\coqe{tApp} for application, and \coqe{tProd} for dependent function types.
The syntax uses De Bruijn indices for binders – the integer argument of the \coqe{tRel}
term former –, but names are still recorded, mainly for printing purposes, directly in the
binders – the \coqe{aname} argument of \coqe{tProp} and \coqe{tAbs}.
There are also local definitions, in the form of the \coqe{tLetIn} constructor,
binding the term \coqe{b} of type \coqe{B} in \coqe{t}.

The \coqe{tSort} constructor is for sorts – what we have called universes earlier.
Its \coqe{Universe.t} argument represents its \kl{universe level}, which can be either $\Prop$
or an algebraic expression based on universe variables, in order to handle \kl{typical ambiguity}.

Next come \coqe{tVar} and \coqe{tEvar}, which
correspond respectively to named variables and existential variables.
These are ill-typed in the current notion of typing,
and thus ignored in most of the development. Still, they are kept to be as
faithful as possible to the representation of the \kl{Coq} kernel.
Indeed, the inductive \coqe{term} corresponds directly to the
\mintinline{OCaml}{constr} datatype used there.%
\sidenote{The only differences are that the latter uses
an n-ary application rather than a binary one, and casts that inform the \kl{kernel} as to
which cumulativity algorithm to use, but which is left out since we implement only one such
algorithm.}

Follow the three term formers \coqe{tConst}, \coqe{tInd} and
\coqe{tConstruct} all referring to previous definitions, stored in a \kl{global environment}.
The first corresponds to constants, that either have a body – definitions –
or do not – axioms. The next two are respectively for
inductive types and inductive constructors. Co-inductive and record types and
constructors are also represented by these term formers, the information contained in the
\coqe{inductive} argument is used to separate between them.
All of these can be \kl{universe polymorphic}, in which case they must be instantiated
with a list of universes – their \coqe{Instance.t} argument.

The two subsequent \coqe{tCase} and \coqe{tProj} are destructors for (co-)inductive types. The
latter is a \kl{projection}, used to destruct \kl{record types}. The former represents the
pattern-matching construction. Its main components are the \kl{predicate} \coqe{p}, the
\kl{scrutinee} \coqe{c} and the \kl{branches} \coqe{brs}. While it will appear more clearly
when giving the typing rule, let us note already that \coqe{p} and \coqe{brs} both contain not
only the body of the \kl{predicate}/\kl{scrutinee}, but also the context extension over which
they live, roughly corresponding to the variable bounds in the \kl{recursors} of \cref{sec:tech-cic}.
Thus, they represent a form of binding in the “primitive” way of a context extension, rather
than using Π-types or λ-abstraction. This is the new case representation alluded to
at the end of \cref{sec:bidir-pcuic-inductives}.

Finally, the two very similar \coqe{tFix} and \coqe{tCoFix} are for (co-)fixed-points. 
These can be mutual: the \coqe{mfix} argument is a list of definitions,
that can refer to each other.

\subsection{Cumulativity}

The next important definition is that of \kl{cumulativity}, given in \pcuicfile{CumulativitySpec}.%
\sidenote{The “Spec” part comes from the fact that this is the \emph{specification} of cumulativity,
by contrast to the algorithmic version encountered later on.}
It is stated in the \kl[declarative conversion]{declarative} \kl(conv){untyped} fashion,
akin to how we defined \kl{declarative conversion} in
\cref{fig:cic-uconv-beta,fig:cic-uconv-equiv,fig:cic-uconv-cong}.
This time, however, it is done relatively to
both a \kl{global environment} \coqe{Σ} and a context \coqe{Γ},
as these contain definitions that \kl{cumulativity} can unfold.

\kl{Cumulativity} is defined mutually with \kl{conversion}, because for instance when two Π-types
are compared for \kl{cumulativity}, their codomains are recursively compared for \kl{cumulativity},
but their domains are compared for \kl{conversion} instead.%
\sidenote{As explained in \cref{sec:tech-pcuic},
this is a consequence of the models justifying cumulativity.}
Since the two relations are extremely similar, they are actually fused in a single
inductive relation, \coqe{Σ ;;; Γ ⊢ t ≤s[pb] u}.
\AP This relation is indexed by a \intro{conversion problem} \coqe{pb : conv_pb}, which can take
the two values \coqe{Conv} and \coqe{Cumul}, so that \kl{cumulativity}
is actually \coqe{≤s[Cumul]} – and \kl{conversion} is
\coqe{≤s[Conv]}. This has the advantage that a lot of definitions and proofs
can be factored using \coqe{conv_pb}.
Moreover, using a simple boolean allows for case reasoning
when needed, which would be more complex if the index was \eg a relation.%
\sidenote{This used to be the case prior to the uniform introduction of \coqe{conv_pb}: the
relation was the one to be used at leaves to compare universes, which differed between 
\kl{conversion} and \kl{cumulativity}.}

\begin{figure}[h]
  \ContinuedFloat*
  \coqfile[firstline=3,lastline=12]{./code/PCUICCumulativitySpec.v}
  \caption{Pre-order rules (\pcuicline{CumulativitySpec}{cumulSpec0}{29})}
  \label{fig:meta-cumul-struct}
\end{figure}

\AP The first set of rules are the pre-order rules of \cref{fig:meta-cumul-struct}:
transitivity, symmetry and reflexivity. Note that symmetry restricts the \kl{conversion problem},
since only \kl{conversion} should be symmetric. Using this rule twice shows that 
\kl{conversion} is included inside \kl{cumulativity}.
Another important thing to note is that transitivity is somewhat restricted:
the middle term is required to be \intro{well-scoped},%
\sidenote{Expressed by the \pcuicline[Syntax]{OnFreeVars}{is\_open\_term}{253} predicate.}
\ie all its variables refer correctly
to either a binder or to the context \coqe{Γ}.
This is key when proving the equivalence between this notion of \kl{cumulativity} and the
algorithmic version that appears later on. Indeed, this equivalence relies on \kl{confluence},
which is only true on \kl{well-scoped} terms in \kl{PCUIC}. Thus, we need to know that
\kl(conv){declarative} \kl{cumulativity} only ever goes through \kl{well-scoped} terms,
which is exactly what this condition enforces.

\begin{figure}[ht]
  \ContinuedFloat
  \coqfile[firstline=44,lastline=48]{./code/PCUICCumulativitySpec.v}
  \caption{Example of congruence rule (\pcuicline{CumulativitySpec}{cumulSpec0}{29})}
  \label{fig:meta-cumul-cong}
\end{figure}

\begin{figure}[h]
  \ContinuedFloat
  \coqfile[firstline=15,lastline=29]{./code/PCUICCumulativitySpec.v}
  \caption{Cumulativity rules (\pcuicline{CumulativitySpec}{cumulSpec0}{29})}
  \label{fig:meta-cumul-cumul}
\end{figure}

Next come the rules of congruence. There are actually two kinds of them. The first are the
“standard” ones, similar to those of \cref{fig:cic-uconv-cong}. An example
is given in \cref{fig:meta-cumul-cong}.
More interesting are the rules of
\cref{fig:meta-cumul-cumul}, which implement “real” cumulativity. Rule \coqe{cumul_Sort} directly
implements subtyping between universes, while rules \coqe{cumul_Ind} and \coqe{cumul_Construct}
implement cumulativity of inductive types \sidecite{Timany2017}. The latter two apply respectively
to \emph{fully applied} inductive types and inductive constructors, that can be considered equal
if their arguments are one-to-one convertible, and their universe levels are correctly related.
This means that \eg the \coqe{nil} constructor of polymorphic lists \emph{always} satisfies that
\coqe{nil@{u} A} is convertible to \coqe{nil@{u'} A}, irrespective of the universe levels \coqe{u}
and \coqe{u'}.

\begin{figure*}
  \ContinuedFloat
  \coqfile[firstline=87,lastline=96]{./code/PCUICCumulativitySpec.v}
  \caption{Computation rules for destructors (\pcuicline{CumulativitySpec}{cumulSpec0}{29})}
  \label{fig:meta-cumul-dest}
\end{figure*}

Last are the rules for computation.
The three rules of \cref{fig:meta-cumul-dest} are for destructors, \ie applied functions,
pattern-matching on a constructor, and projections. The β rule for functions directly uses
substitution: \coqe|b{0 := a}| denotes the substitution of \coqe{a} for the variable of De Bruijn
index \coqe{0} in \coqe{b}. Similarly, the
\coqe{iota_red} function computes the substitution of the branch \coqe{br} by the arguments
of the constructor \coqe{args}.
Finally, the rule for projections simply selects the right field of the record.

\begin{figure*}
  \ContinuedFloat
  \coqfile[firstline=99,lastline=106]{./code/PCUICCumulativitySpec.v}
  \caption{Computation rules for definitions (\pcuicline{CumulativitySpec}{cumulSpec0}{29})}
  \label{fig:meta-cumul-defs}
\end{figure*}

The next three rules deal with definitions (\cref{fig:meta-cumul-defs}).
Rule \coqe{cumul_zeta} directly reduces a
let-binder into a substitution, while definitions are unfolded using \coqe{cumul_rel} and \coqe{cumul_delta},
respectively those of the local context or the \kl{global environment}. In the latter
case, the definition must be instantiated with a universe instance,%
\sidenote{A list of universe levels, corresponding to its polymorphic universe variables.}
which is denoted \coqe{@[u]}.

\begin{figure}[h]
  \ContinuedFloat
  \coqfile[firstline=108,lastline=119]{./code/PCUICCumulativitySpec.v}
  \caption{Computation rules for fixed-points (\pcuicline{CumulativitySpec}{cumulSpec0}{29})}
  \label{fig:meta-cumul-fix}
\end{figure}

The last rules (\cref{fig:meta-cumul-fix}) pertain to the reduction of (co-)fixed-points.
In all cases, they are unfolded in a guarded fashion, in order to avoid a non-terminating behaviour.
On fixed-points, this guard is that they have to be applied to a constructor,
and dually co-fixed-points are unfolded when
they are forced, either by a pattern-matching or a projection. In both cases, this ensures that
the unfolded (co-)fixed-point can reduce further, either by consuming the constructor of
its recursive argument, or by producing a constructor to be consumed by the destructor
that forced the unfolding.

\FloatBarrier
\subsection{Typing}

With the \kl{cumulativity} relation defined, we can turn to typing, defined in \pcuicfile{Typing}.
Similarly to \kl{cumulativity}, \pcuicline{Typing}{typing}{188}
is an inductively defined relation \coqe{Σ ;;; Γ |- t : T},
relative to a global environment \coqe{Σ} and a local context \coqe{Γ}.
The rules correspond roughly to ones we already went over in \cref{chap:tech-intro}.

\begin{figure}
  \ContinuedFloat*
  \coqfile[firstline=2,lastline=22]{./code/PCUICTyping.v}
  \caption{Functional fragment (\pcuicline{Typing}{typing}{188})}
  \label{fig:meta-typing-ccw}
\end{figure}

The first set of typing rules, given in \cref{fig:meta-typing-ccw},
pertain to the purely functional fragment. There are not many differences
there with respect to \cref{fig:ccw-typing}. Rule \coqe{type_Rel} looks up for the type of a
variable in the context, and ensures that said context is well-formed:
the \pcuicline{Typing}{wf\_local}{284} predicate corresponds to $\vdash \Gamma$,
but here as everything else it is relative to a global environment.
Rule \coqe{type_Sort} uses the \coqe{super} function to compute
the successor of an algebraic universe, and similarly Rule \coqe{type_Prod} uses
\coqe{sort_of product} to compute the universe level of a Π-type.%
\sidenote{This not only includes computing the maximum of two algebraic universes expressions,
but also handling the \kl{impredicativity} of the sort $\Prop$.}
Context extension with an assumption, \ie a variable without a body,
is written \coqe{Γ ,, vass na A}, and used as expected in Rules \coqe{type_Prod}
and \coqe{type_Lambda}. Finally, \coqe{type_App} is for application. It contains an
assumption that the product is well-formed, which is not strictly speaking needed once we
prove \kl{validity}, but is useful in some cases to provide a needed induction hypothesis.

\begin{figure}
  \ContinuedFloat
  \coqfile[firstline=23,lastline=27]{./code/PCUICTyping.v}
  \caption{Local definitions (\pcuicline{Typing}{typing}{188})}
  \label{fig:meta-typing-letin}
\end{figure}

The rule for local definitions, given in \cref{fig:meta-typing-letin}, is similar to the one
for λ-abstractions, with the only difference that the body too is typed, and that the context
is extended with a definition, \ie a variable with a body, which is written \coqe{Γ,, vdef na b B}.

\begin{figure*}
  \ContinuedFloat
  \coqfile[firstline=28,lastline=42]{./code/PCUICTyping.v}
  \caption{Globally defined terms (\pcuicline{Typing}{typing}{188})}
  \label{fig:meta-typing-genv}
\end{figure*}

Next (\cref{fig:meta-typing-genv}) are the three rules performing look-ups in
the \kl{global environment}, respectively constants – \coqe{type_Const} –, inductive types –
\coqe{type_Ind} – and inductive constructors – \coqe{type_Construct}. In all cases the
term should be declared in the \kl{global environment}, the context well-formed
– since these are leaves of a term –,
and the universe instance given should respect the constraints coming from the
entry in the \kl{environment} – this is the \coqe{consistent\_instance\_ext} predicate.

\begin{figure*}
  \ContinuedFloat
  \coqfile[firstline=43,lastline=58]{./code/PCUICTyping.v}
  \caption{(Co-)inductive destructors (\pcuicline{Typing}{typing}{188})}
  \label{fig:meta-typing-idest}
\end{figure*}

The rules of \cref{fig:meta-typing-idest} are the ones for the destructors of (co-)inductive
types. Rule \coqe{type_Case} is somewhat similar to \ruleref{rule:bool-ind}. First,
the \kl{scrutinee} should be of an inductive type, declared in the global environment.
Next, the \kl{predicate} and \kl{branches} should all be well-typed
in the appropriate context – obtained by combining the information stored in
\coqe{p} or \coqe{brs}, with that retrieved from the entry in the environment corresponding to
the \kl{scrutinee}'s type.
Finally, \pcuicline{Typing}{case\_side\_conditions}{159} handles universe instances, checks
that the elimination is allowed%
\sidenote{This is where \kl{PCUIC} enforces computational irrelevance of proofs,
by imposing the so-called
“singleton elimination” criterion, which ensures that only inductive
types of a certain specific shape – sub-singletons – can be matched on to build proof relevant
content, so that that content cannot actually depend on the value of a proof.}…
The second rule, \coqe{type_Proj}, is somewhat similar, albeit a bit simpler: the \kl{scrutinee}
should still be some applied co-inductive/record type, and the type is constructed by
substitution from the projection information.

\begin{figure}
  \ContinuedFloat
  \coqfile[firstline=59,lastline=76]{./code/PCUICTyping.v}
  \caption{(Co-)fixed-points (\pcuicline{Typing}{typing}{188})}
  \label{fig:meta-typing-fix}
\end{figure}

The last typing rules for terms are those for (co-)fixed-points,
given in \cref{fig:meta-typing-fix}. They are almost identical,
the main part amounts to checking that the types and bodies of all
the mutual definitions are well-typed.
The \pcuicline{Typing}{wf\_fixpoint}{118} and \pcuicline{Typing}{wf\_cofixpoint}{145}
predicates both check that the definitions are on
the same block of mutually-defined (co-)inductive types, and that these are of the right kind –
inductive for \coqe{tFix} and co-inductive for \coqe{tCoFix}.
Finally, the \coqe{fix_guard} and \coqe{cofix_guard} predicates correspond to the guard condition,
ensuring that the definitions do not endanger \kl{normalization}. We come back to those in
\cref{sec:meta-normalization}.

\begin{figure}
  \ContinuedFloat
  \coqfile[firstline=77,lastline=81]{./code/PCUICTyping.v}
  \caption{Cumulativity (\pcuicline{Typing}{typing}{188})}
  \label{fig:meta-typing-cumul}
\end{figure}

The final rule (\cref{fig:meta-typing-cumul}) is that which uses \kl{cumulativity} as just defined
to change the type of the term, \eg the equivalent of \ruleref{rule:cic-cum}.

From the definition of \pcuicline{Typing}{typing}{188}, two more pervasively used definitions
follow. We have already encountered \pcuicline{PCUICTyping}{wf\_local}{284},
asserting that a local context is well-formed.
Its sibling predicate \pcuicline{PCUICTyping}{wf}{439}%
\sidenote{Often replaced by \pcuicline{PCUICTyping}{wf\_ext}{444},
an extension that in addition takes into account the universes of the current definition.}
asserts that the \kl{global environment} is well-formed. It ensures not only that all definitions
are properly typed, but also of the validity of various information
related to inductive types – in particular the positivity criterion which ensures that inductive
definitions are well-founded –, and universe polymorphism.

\section[Stabilities]{The Easy Properties: Stabilities}
\label{sec:meta-stabilities}

With the main definitions set up, we can turn to the properties that we collectively called
stabilities in \cref{sec:tech-properties}. These assert that \kl{cumulativity} and typing
as just defined are stable by various ubiquitous operations: extension of the local context%
\sidenote{\pcuicfile[Conversion]{WeakeningConv} and \pcuicfile[Typing]{WeakeningTyp}.}
and global environment,%
\sidenote{\pcuicfile[Conversion]{WeakeningEnvConv} and \pcuicfile[Typing]{WeakeningEnvTyp}.}
and substitution, not only for terms,%
\sidenote{\pcuicfile[Conversion]{InstConv} and \pcuicfile{Substitution}}
but also for universe variables.%
\sidenote{\pcuicfile[Conversion]{UnivSubstitutionConv} and \pcuicfile[Typing]{UnivSubstitutionTyp}}

\AP One last property falling in the section of low-hanging fruits as well
is the fact that well-typed terms are \kl{well-scoped}.%
\sidenote{\pcuicfile{ClosedTyp}.}
%, that is that if \coqe{Σ ;;; Γ |- t : T}
%then all free variables of \coqe{t} appear in \coqe{Γ}.
This \kl{well-scoping} conditions appears
in the transitivity rule for \kl{cumulativity}
(\cref{fig:meta-cumul-struct}) and is a hypothesis for many lemmas.

All of these are proven by induction on the typing derivation.
While the proof by themselves are not very surprising, the formalization of the definitions
deserves a few comments.

The first point to note is that while the weakening and substitution operations are defined
directly by induction on the syntax of terms, the proofs are not done directly
on those definitions. Rather, \kl{MetaCoq} uses notions of renaming and instantiation inspired
by the σ-calculus \sidecite{Abadi1991,Schaefer2015}, as functions from natural numbers
to natural numbers for renamings, and to terms for instantiations.
\kl{Weakening} and \kl{substitution} then correspond respectively to a specific form of
renaming and of instantiation, and the stability for the former those follows from
more general versions for the latter.
For instance, \kl{weakening} is but a consequence of general stability by
(unconditional) renaming, as presented in \cref{prop:stab-renaming}.
This approach makes it easier to handle the complex binding structures
present in the syntax of \kl{PCUIC}, which require parallel substitution or
lifting by a whole context at once,
operations that are easier to handle in the general framework of the σ-calculus.

More interestingly and novel, the same approach is taken also for \kl{well-scoping}:
\kl{MetaCoq} generalizes that predicate into a way of lifting
any boolean function on natural numbers – seen as a property of variables – to a boolean function
on terms.\sidenote{This is \pcuicline[Syntax]{OnFreeVars}{on\_free\_vars}{71}.}
This makes it easy to relate the σ-calculus
to \kl{well-scoping} assumptions, \eg to show that if two substitutions \coqe{σ} and \coqe{σ'}
agree on the free variables of a term \coqe{t}, then the application of \coqe{σ} and \coqe{σ'}
to \coqe{t} are equal.

\begin{figure}[h]
  \coqfile{./code/PCUICSubstitution.v}
  \caption{Well-formed substitution (\pcuicline{Substitution}{subslet}{44})}
  \label{fig:meta-subslet}
\end{figure}

A last point of interest is the definition of a well-formed substitution, a predicate called
\pcuicline{Substitution}{subslet}{44}.
Indeed, the usual typing judgment for substitutions
is of the form $\Delta \vdash \sigma \ty \Gamma$, meaning that
$\sigma$ maps each assumption $(x : A) \in \Gamma$ to a term $t$ such that
$\Delta \vdash t \ty \multisubs{T}{\sigma}$. But in our setting we must account for variables
that can be \emph{defined} in $\Delta$. This leads to the definition of a well-formed substitution as in \cref{fig:meta-subslet}.
A similar definition, called \pcuicline[Syntax]{InstDef}{well\_subst}{65}, is also available for
instantiations.

\section[Confluence]{Things Get Serious: Confluence}
\label{sec:meta-confluence}

\begin{figure*}[ht]
  \coqfile{./code/PCUICReduction.v}
  \caption{\kl{One-step reduction} and \kl{reduction}}
  \label{fig:meta-red}
\end{figure*}


As in \cref{sec:tech-properties}, the next step is to define \kl{reduction} and
establish its properties.
An excerpt of the definitions is given in \cref{fig:meta-red}:
\kl{reduction} is \pcuicline{Reduction}{red}{303},
defined as as the reflexive, transitive closure
of \kl{one-step reduction} \pcuicline{Reduction}{red1}{18}.
The former is written \coqe{Σ ;;; Γ |- t ~>* u}, and the latter
\coqe{Σ ;;; Γ |- t ~> u}.

\subsection{Parallel reduction}
\label{sec:metacoq-parred}

\AP The proof of \kl{confluence} follows the standard Tait-Martin-Löf approach as exposed by
\sidetextcite{Takahashi1995}. It relies on a notion of \intro{parallel reduction}
$\mathord{\intro*\parred}$, which can reduce multiple redexes present in a term $t$ in parallel.
As an example, the rules for application are given in \cref{fig:parred-app}.
\begin{marginfigure}
  \begin{mathpar}
  \inferrule
    {t \parred t' \\ u \parred u'}
    {t\ u \parred t'\ u'} \and
  \inferrule
    {t \parred t' \\ u \parred u'}
    {(\l x : A.\ t)\ u \parred \subs{t'}{x}{u'}}
  \end{mathpar}
  \caption{Parallel reduction for application}
  \label{fig:parred-app}
\end{marginfigure}
The generic congruence
rule allows reduction to happen in parallel in both the function and argument. Moreover,
if the function is an abstraction, a β step can also be fired simultaneously 
with those. Note that this does \emph{not} allow reducing further redexes
that would be produced by the substitution. For instance, we do not have
\[(\l f : \Nat \to \Nat . f\ 0)\ (\l x : \Nat. x) \parred 0\]
because the redex $(\l x : \Nat. x)\ 0$ only appears after a first step of substitution.

\kl{Parallel reduction} has two interesting properties. First, it
is related to \kl[reduction]{standard reduction}.

\begin{lemma}[Parallel reduction and reduction]
  \label{lem:parred-red}
  We have $\ored \subset \mathord{\parred} \subset \mathord{\fred}$.

  This implies that if \kl{parallel reduction} is \kl{confluent}, then so is \kl{reduction}.
\end{lemma}

\AP But the interesting characteristic of \kl{parallel reduction}, which \kl{reduction}
does not satisfy, is the
\intro{diamond property}, a strong version of local confluence,%
\sidenote{
  Which says that if $t \ored t_1$ and $t \ored t_2$ then there exists $t'$ such that $t_1 \red t'$
  and $t_2 \red t'$.}
which contrarily to it implies \kl{confluence} even in the absence of \kl{normalization}.
Thanks to \cref{lem:parred-red} above, in order to establish \kl{confluence} of 
\kl{reduction}, it suffices to show this \kl{diamond property}.

The proof idea goes as follows. First, show that \kl{parallel reduction} is substitutive, in
the following sense.
\begin{lemma}[Parallel reduction is substitutive]
  If $t \parred t'$ and $u \parred u'$ then
  $\subs{t}{x}{u} \parred \subs{t'}{x}{u'}$.
\end{lemma}
\AP This allows to define a \intro{best parallel reduct} $\intro*\bestredop$,
which reduces \emph{all} possible redexes in parallel,%
\sidenote{ For instance $\bestred{(\l x : A.\ t)\ u}$ is $\subs{\bestred{t}}{x}{\bestred{u}}$.
}
and to show that it is really a best reduct.
\begin{lemma}[Triangle property]
  Given a term $t$, if $t \parred t'$ then $t' \parred \bestred{t}$.
\end{lemma}

\begin{marginfigure}
  \centering 
  \begin{tikzcd}
      t \arrow[triple,dr] \arrow[triple,d] & \\
      t' \arrow[triple,r] & \bestred{t}
  \end{tikzcd}

  \begin{tikzcd}
    & t \arrow[triple,dl] \arrow[triple,dr] & \\
    t_1 \arrow[triple, dr] && t_2 \arrow[triple,dl] \\
    & \bestred{t} &
  \end{tikzcd}
  \caption{The triangle and diamond properties, as diagrams}
\end{marginfigure}

This is enough to get the diamond property, because any two parallel reducts of a term $t$
both reduce to $\bestred{t}$ \emph{in one step}. This basically amounts to firing in both reducts
all the redexes that could have been triggered but have not been.

\begin{minipage}{\textwidth}
\begin{lemma}[Diamond property]
  Given a term $t$ and $t_1$, $t_2$ such that $t \parred t_1$ and $t \parred t_2$, there exists
  $t'$ such that $t_1 \parred t'$ and $t_2 \parred t'$.
\end{lemma}
\end{minipage}

From this, it follows by a bit of diagram chasing that \kl{parallel reduction} is confluent,
and thus that \kl{reduction} is.

\begin{minipage}{\textwidth}
\begin{theorem}[\kl{Confluence} of \kl{reduction}]
  \kl{Reduction} is \kl{confluent}, that is if $t \red t_1$ and $t \red t_2$ then there exists
  $t'$ such that $t_1 \red t'$ and $t_2 \red t'$.
\end{theorem}
\end{minipage}

Note that it can directly be shown without resorting to \kl{parallel reduction} that \kl{reduction}
is \emph{locally} confluent. However, in the absence of \kl{normalization}
to apply Newman’s lemma, this
is not enough to ensure \kl{confluence}. But, while we can hope that \kl{normalization} holds for
well-typed term, it is clearly false on untyped terms. Yet, it is crucial to get confluence on
those, as otherwise we would not be able to establish \kl{injectivity of type constructors} and thus
\kl{subject reduction} with our untyped notion of \kl{cumulativity}.
Thus, this detour through \kl{parallel reduction} is really unavoidable.

\subsection{Formalizing Takahashi’s proof}

The previous section sets down a quite precise plan, that we can almost directly follow in \kl{MetaCoq}. There is one important subtlety though: because of local definitions, \kl{reduction}
depends on contexts, so these must be taken into account in \kl{parallel reduction}. But
bodies of definitions should also be reduced by \kl{parallel reduction},
and so the actual relation is between pairs of a context and a term,
something like $\Gamma, t \parred \Gamma', t'$.

Apart from this difficulty%
\sidenote{And the technicality of defining $\bestredop$ in a terminating fashion, 
which is done using the \kl{Equations} plugin \cite{Sozeau2019a}.}%
\margincite{Sozeau2019a}
the plan can be followed quite closely.
\kl{Parallel reduction} is defined as \pcuicline{ParallelReduction}{pred1}{241} in
\pcuicfile{ParallelReduction}. Then the diamond property is proven in \pcuicfile{ParallelReductionConfluence} – $\bestredop$ is \pcuicline{ParallelReductionConfluence}{rho}{697},
and the diamond property itself is \pcuicline{ParallelReductionConfluence}{pred1\_diamond}{4421}.
Finally, \pcuicfile{Confluence} goes back from this to properties of \kl{reduction}, concluding
with its \kl{confluence} (\pcuicline{Confluence}{red\_confluence}{3959}).

\section[The Road to Subject Reduction]{Reaping the Fruits: the Road to Subject Reduction}
\label{sec:meta-typing-prop}

With \kl{confluence} proven, it is time to reap the fruits. First,
\kl(conv){declarative} \kl{cumulativity},
as used to define typing, can be related to \kl(conv){algorithmic} \kl{cumulativity}. This entails
many useful consequences, including \kl{injectivity of type constructors}.
A series of important properties then follow, culminating with \kl{subject reduction}.

\subsection{Algorithmic cumulativity}

The first use of confluence is to relate \kl(conv){declarative} \kl{cumulativity}
as used to define typing to \kl(conv){algorithmic} \kl{cumulativity} –
\pcuicline{Cumulativity}{cumulAlgo}{31} –
defined as reduction to terms related by the \kl{α-pre-order} $\alphleq$.%
\sidenote{The generalization of \kl{α-equality} to handle \kl{cumulativity}.}
But in the setting of \kl{PCUIC} this relation, called \pcuicline{Equality}{leq\_term}{382} in
the formalization, is far from being trivial to define!
It needs to handle algebraic universe levels, but also polymorphic inductive types. Moreover,
it is parameterized over the relation used to compare universes, so that it can also be used
to express “pure” \kl{α-equality}%
\sidenote{Meaning that the terms can differ only on binder names, but that their universe
must be syntactically the same rather than related using the constraints present in the
environment.}
when instantiated with equality rather than universe comparison.

To show that \kl{algorithmic conversion} is equivalent to \kl{declarative conversion},
\kl{confluence} is the major ingredient, but it is not enough. Instead, we also need to show
that \kl{reduction} interacts well with this \kl{α-pre-order}.

\begin{lemma}[The \kl{α-pre-order} is a simulation
    (\pcuicline{Confluence}{red1\_eq\_term\_upto\_univ\_l}{838})]
  \begin{marginfigure}
    \centering
    \begin{tikzcd}
        t \arrow[d, "1" very near end] \arrow[r, phantom, "\alphleq" description] &
        t' \arrow[d, dashed, "1" very near end] \\
        u \arrow[r,phantom,"\alphleq" description] & u'
    \end{tikzcd}
    \caption{Simulation, as a diagram}
    \label{fig:alphleq-sim}
  \end{marginfigure}
  If $t \alphleq t'$ and $\Gamma \vdash t \ored u$ then there exists some $u'$ such that
  $\Gamma \vdash t' \ored u'$ and $u \alphleq u'$.
\end{lemma}

While the proof
is still relatively straightforward since $t$ and $t'$ have the same structure, it becomes
much more challenging if we wish to integrate extensionality rules such as η-equality or 
strict propositions%
\sidenote{Two features present in \kl{Coq} but not yet in \kl{MetaCoq}, exactly due to this
kind of difficulties.}
into $\alphleq$.

Combining this simulation property with \kl{confluence},
transitivity of \kl(conv){algorithmic} \kl{cumulativity} follows,
and finally its equivalence with \kl(conv){declarative} \kl{cumulativity}.

\begin{theorem}[Equivalence of the presentations of \kl{cumulativity}]
  \kl(conv){Algorithmic} and \kl(conv){declarative} \kl{cumulativity} are equivalent.
\end{theorem}

This equivalence is the main theorem of \pcuicfile{Conversion} – 
one direction is \pcuicline{Conversion}{cumulSpec\_cumulAlgo}{3680},
and the other is \pcuicline{Conversion}{cumulAlgo\_cumulSpec}{190}.
That central file also proves multiple variants of
\kl{injectivity of type constructors}.
For instance, \kl{injectivity of function types} is \pcuicline{Conversion}{ws\_cumul\_pb\_Prod\_Prod\_inv}{718},
and the stronger form used in \kl(bidir){completeness} of bidirectional typing
(\cref{lem:conv-red-tycons}) is \pcuicline{Conversion}{ws\_cumul\_pb\_Prod\_r\_inv}{686}.

\subsection{Reaping the fruits}

With injectivity settled, we can get to the main properties of typing.
The easiest is \pcuicline{Validity}{validity}{453},
asserting that if $\Gamma \vdash t \ty T$ then $T$ is a well-formed type in $\Gamma$.
The second is that typing is insensible to names (\pcuicline{Alpha}{typing\_alpha}{1009}):
if two terms differ only in variable names and one is typable, then so is the other.
And, finally, comes \pcuicline{SR}{subject\_reduction}{3072}.

While the main proofs of these theorems are far from simple,%
\sidenote{That for \kl{subject reduction} needs more than 1500 lines for the main induction!}
an important part of the proof effort is actually required ahead of them
in \pcuicfile{Inductives} and \pcuicfile{InductiveInversion},
in order to show that the various contexts, substitutions, applications, etc.
that appear in conversion and typing for inductive types behave as expected.

Regarding \kl{subject reduction}, a caveat applies. Because the “positive”
presentation of co-fixed-points does not preserve types, as explained in
\cref{sec:pcuic-records}, only the “negative” presentation based on projections is allowed.
In practice, this means
that the typing rule \pcuicline{Typing}{type\_case}{241} of \cref{fig:meta-typing-idest}
forbids the \kl{scrutinee} from being of a co-inductive type.%
\sidenote{This is part of the \pcuicline{Typing}{case\_side\_conditions}{159}.}
Hence, while \kl{reduction} and \kl{conversion} take this presentation into account,%
\sidenote{See the \pcuicline{CumulativitySpec}{cumul\_cofix\_case}{158}
constructor in \cref{fig:meta-cumul-fix}.}
thus showing at least that it is confluent, it can never appear in a well-typed term.

An alternative solution, which would allow an easier transition away from today’s positive
co-inductive types, is to see co-inductive scrutinees as effectful terms
and restricts predicates allowed
for dependent elimination to be linear, following \sidetextcite[0em]{Pedrot2017}.
This approach is not formalized in \kl{MetaCoq} – yet –, but the project
provides a natural setting to explore this kind of questions in all their gritty details
before working towards an actual implementation in \kl{Coq}.

\section[Normalization]{Gödel’s Thorn in the Side: Normalization}
\label{sec:meta-normalization}

One last important property remains: \kl{normalization}. In \kl{PCUIC}, the key constraint to
ensure it is the \kl{guard condition}, to which (co-)fixed-points are subject in order to
ensure that they are well-founded – see \cref{sec:pcuic-ind}.
However, due to Gödel’s second incompleteness theorem, if \kl{PCUIC} is \kl(log){consistent}
then it cannot prove its own \kl(log){consistency}.
But if \kl{normalization} were provable, then so would be \kl(log){consistency}.
There is therefore no hope to give a guard condition and prove that it entails \kl{normalization}.

\subsection{An abstract guard condition}

\begin{figure}[h]
  \coqfile{./code/PCUICGuard.v}
  \caption{The \pcuicline{Typing}{guard}{54} axiom}
  \label{fig:meta-guard}
\end{figure}

Instead, \kl{MetaCoq} takes a different approach. The existence of a \kl{guard condition} is
\emph{assumed} in an abstract, axiomatic fashion – see \cref{fig:meta-guard} –
and used in typing – see \cref{fig:meta-typing-fix}.
Similarly, \pcuicfile{GuardCondition} assumes properties of this axiomatic guard: it
should be stable by universe and term substitution, extension of the \kl{global environment},
cumulativity of the local context insensible to names, and, most importantly, stable by
reduction.

These abstract properties are enough to handle the whole development outlined above. In other
words, given any notion of \kl{guard condition} that satisfies the criteria of
\pcuicfile{GuardCondition}, typing satisfies \kl{injectivity of type constructors},
\kl{validity}, \kl{subject reduction}… Thus, our abstract approach provides a precise
characterization of the properties the \kl{guard condition} needs to satisfy in order
for typing to be well-behaved.

In particular, since the trivial guard condition
that is always true fulfils the requirements, none of the aforementioned
properties rely on \kl{normalization} – or \kl(log){consistency} of the theory.
Thus, \kl{PCUIC} is a \kl[safety]{safe} programming language, \emph{unconditionally}.

\subsection{The normalization axiom}

\begin{figure}[h]
  \coqfile{./code/PCUICSN.v}
  \caption{The \pcuicline{SN}{normalization}{36} axiom}
  \label{fig:meta-sn}
\end{figure}

But of course we need more. In particular, if we wish to define a convertibility check
inside \kl{Coq}, which only allows to define terminating functions,
we must know that reduction is terminating. Once again, we axiomatize the necessary
axiom of (strong) \kl{normalization}, as given in \cref{fig:meta-sn}.
Note that this axiom takes exactly the form we gave to \kl{normalization}
in \cref{prop:normalization}, as the accessibility of any well-typed term for
co-reduction. This is done under the assumption that the global context is well-formed,
otherwise it could contain \eg non-positive inductive types which
could be used to define non-terminating terms.

Using this axiom, it is possible to prove \kl(log){consistency} of \kl{PCUIC}, as shown in
\safefile{Consistency}. The rough idea of the proof is that given for \cref{prop:log-cons},
\ie to deduce \kl(log){consistency} from \kl{canonicity}.
However, instead of proving \kl{progress} directly we rely on a proven-complete function 
computing \kl(red){weak-head} normal forms implemented as part of the type-checker.%
\sidenote{Which can be seen as a constructive witness of \kl{canonicity}!}
Assuming an inhabitant \coqe{t} of $\P b : \Prop.\ b$%
\sidenote{Corresponding to the \kl{MetaCoq} term
\coqe{tProd b (tSort Prop_univ) (tRel 0)}.}
in any axiom-free global environment \coqe{Σ}, the proof extends
\coqe{Σ} with the empty inductive type $\Empty$, use \coqe{t} to construct a
\kl(red){weak-head} normal inhabitant of that type, and from this finally derive a contradiction.