\chapter{Introduction (Français)}
\label{ch:intro-fr}

\margintoc

\selectlanguage{english}
\begin{kaobox}[backgroundcolor=Black!10!White,frametitlebackgroundcolor=Black!10!White]
  This section is an introduction intended for French-speaking readers.
  If your English is better than your French,
  you should instead read \cref{ch:intro-en}, its translation in English.
\end{kaobox}
\selectlanguage{french}

\todo{Références manquantes}

Cette thèse se situe dans le domaine de la \kl{théorie des
types (dépendants)}, lui-même au croisement entre informatique et logique mathématique.
L’objectif principal est de participer aux fondements théoriques et pratiques des outils
que l’on appelle \kl{assistants à la preuve}, des logiciels qui, comme leur nom
l’indique, ont pour but d’assister des êtres humains dans la construction
et la vérification de preuves – au sens mathématique du terme. Il sera dans cette thèse
en particulier beaucoup question de l’assistant à la preuve \kl{Coq}, qui est celui
sur lequel mon travail s’est principalement concentré.

Pour replacer ce travail dans son contexte large, je propose dans cette introduction une histoire très parcellaire et orientée de la logique mathématique
(\cref{sec:logique-histoire}), puis une courte présentation des assistants à la preuve,
notamment ceux qui, comme \kl{Coq}, se basent sur la théorie des types (\cref{sec:assistants-preuve}). Enfin je finis par une présentation de mes contributions
personnelles pendant la durée de cette thèse (\cref{sec:cette-these}).
\todo{Ajouter disclaimer ?}

\section{Une (très courte) histoire de la logique}
\label{sec:logique-histoire}

\subsection{Les syllogismes}
Dans la tradition occidentale, on peut faire remonter l’étude de
la logique à Aristote, dans son \cite{Organon}.
L’un des apports de ce travail est d’introduire les syllogismes.\sidenote{
  Le syllogisme le plus connu est probablement Barbara, dont un exemple est :
  \emph{tous les humains sont mortels ; Socrate est humain ; donc Socrate est mortel.}
}
Il s’agit de raisonnements dont la validité tient seulement au fait qu’ils
suivent une structure générale, et non à son contenu particulier.
Si un raisonnement est construit en assemblant ces syllogismes,
le raisonnement dans son entier doit nécessairement l’être également, puisque
chaque pas de raisonnement est valide.
L’idée importante ici est celle de décomposition en composantes élémentaires. À
partir d’un système de règles de raisonnement qu’on a identifiées comme valides 
\textit{a priori},\sidenote{
  Il peut s’agir de syllogismes, mais de bien d’autres systèmes… On en rencontra
  un certain nombre dans cette thèse !
}
on a un moyen de s’assurer de la validité de raisonnements potentiellement
très complexes.
Il suffit de vérifier qu’ils peuvent être décomposés à partir
des règles de base – et, bien entendu, que celles-ci soient correctes.

\subsection{Les débuts de la logique mathématique}
À la suite d’Aristote, les mathématicien·ne·s se sont emparé·e·s de la question
de la logique, en cherchant comment il était possible de fonder les mathématiques
rigoureusement. Bien qu’il s’agisse d’une question ancienne, de véritables
progrès concrets sur sa résolution ont commencé à voir le jour dans la deuxième
moitié du 19\textsuperscript{e} siècle, sur deux fronts principaux.

Le premier a consisté à se dégager du langage dit
naturel\sidenote{
  Par opposition aux langages formels qui apparaissent
  en mathématiques, informatique, etc.
}, inadapté pour la description formellement précise de la déduction, et à
concevoir à la place une nouvelle forme de langage spécifique qui puisse servir de
base à un système de raisonnement. Une étape importante
de cette ligne de recherche est
probablement \sidecite{Begriffsschrift}, qui introduit un certain nombre de
caractéristiques des langages dont il sera question dans la suite de cette thèse,
en particulier la notion de quantificateur.

Le second a pour but de montrer que les mathématiques dans leur entier peuvent
effectivement être reconstruites à partir de briques élémentaires. Une étape
importante ici a été la réduction de l’analyse à un petit nombre de propriétés
des nombres réels, puis les constructions de ces nombres réels à partir
de l’arithmétique, données simultanément par plusieurs auteurs\sidenote{
  Cantor, Méray, Dedekind, Bertrand, Weierstraß \cite{??}
} autour de 1870. De son côté Peano \cite{PeanoAxioms} propose
l’axiomatisation des nombres entiers qui est nommée en son honneur.
Enfin Cantor \cite{??}
propose la théorie des ensembles comme un formalisme permettant
de décrire la totalité des objets mathématiques sous la forme d’ensemble
d’éléments.

\subsection{La crise des fondements}
Hélas, le système proposé dans \cite{Begriffsschrift}, fortement inspiré par
les travaux de Cantor, est incohérent\sidenote{
  C'est-à-dire qu’il permet de prouver le faux, et donc qu’il ne peut pas servir
  de base valide pour la logique.
} !
Ce constat, dû à Russell \cite{Begriffsschrift}\todo{Citer le bon appendice},
ouvre une période de crise, où la problématique de décrire un système qui permette
de fonder l’entièreté des mathématiques,
tout en évitant les inconsistances desquelles
le système de Frege et probablement ceux de Cantor étaient victimes.

Une première proposition de solution est avancée par Whitehead et Russell
avec \cite{Principia}, un énorme travail qui non seulement propose un système
logique qui évite les paradoxes conduisant à l’incohérence de
\cite{Begriffsschrift}, mais de plus réalise dans ce système une quantité importante
de mathématiques, en particulier une construction des entiers, de l’arithmétique et
finalement des nombres réels.

En parallèle, dans la continuité des travaux de Cantor, Zermelo \cite{Zermelo1905} et
d’autres travaillent à fournir une description formelle de la théorie des ensemble
qui soit elle aussi cohérente. Ceci aboutit à ce qui est appelé de nos jours la
théorie des ensembles de Zermelo-Fraenkel (ZF, ou ZFC quand on y ajoute l’axiome
du choix), qui semble également à même de fournir une base solide pour les
mathématiques.

\paragraph{L’incomplétude.}
Un deuxième coup vient cependant frapper la recherche d’un système adéquat pour
servir de fondation aux mathématiques : le théorème d’incomplétude de Gödel
\cite{incomplétude}. Celui-ci affirme que tout système formel dans lequel on peut
construire des nombres entiers vérifiant les axiomes de Peano – donc
\textit{a fortiori} tout système suffisamment riche pour faire des mathématiques –
ne peut pas démontrer sa propre cohérence. Ainsi, il n’existe pas de
système sur lequel on puisse fonder les mathématiques en ayant la certitude
formelle que ce système est adéquat : puisqu’on ne peut prouver la cohérence du
système dans lui-même, il pourrait finalement s’avérer incohérent, ruinant les
efforts fournis.

Une conséquence de ce théorème est qu’un système suffisamment riche
pour faire des mathématiques est nécessairement incomplet.\sidenote{
  Cela signifie qu’il
  existe des énoncés indépendants, à savoir des assertions qu’on ne peut
  ni démontrer, ni réfuter – c’est-à-dire montrer la négation. La cohérence du
  système considéré en est un exemple.
}
Ainsi, dans la suite je ne parlerai jamais de vérité absolue –
ce qui n’aurait de sens que dans un système complet
où tout énoncé est vrai ou faux –, mais
uniquement de prouvabilité \emph{dans un système donné}.

\subsection{Une première conclusion}
Malgré les difficultés mises au jour au début du 20\textsuperscript{e}
siècle, la communauté mathématique est globalement
satisfaite de la situation : ZFC fournit un système raisonnable sur
lequel baser en principe toutes les mathématiques, et même si peu de
personnes se risquent à tenter, dans la veine de \cite{Principia},
d’effectivement écrire leurs mathématiques à ce niveau de précision,
elle est globalement convaincue que ce serait en théorie
possible, et cela suffit amplement à la plupart.

De plus, le développement et la vérification humaine de mathématiques véritablement
formalisées semble essentiellement impossible et inutile.
D’un côté, cela demanderait un effort considérable,
tant de la part de l’autrice que de celle de la lectrice, tout en étant
extrêmement laborieux et désagréable.
Dans le même temps, cela ne permettrait pas de réduire de manière importante
les risques d’erreurs, puisqu’il est
humainement très difficile de détecter une petite erreur au milieu de centaines de pages de raisonnement.
Enfin, décrire les mathématiques à ce niveau obscurcirait
considérablement les intuitions mathématiques importantes,
rendant la communication stérile.

\section{Les ordinateurs entrent en scène}
\label{sec:assistants-preuve}

Un nouvel élément vient cependant changer fondamentalement cette situation :
l’avènement des ordinateurs.

\subsection{Pourquoi les ordinateurs ?}

Les ordinateurs excellent là où les humains pêchent : leur spécialité est de traiter
d’immenses volumes d’information très précise, exactement le type
de besoins que soulève la manipulation de preuves formelles. C’est pourquoi, dès
la fin des années 60\sidenote{
  Avec des systèmes comme Automath \cite{??}, Mizar \cite{??}…}
ont commencé à apparaître ce que l’on appelle \intro{assistants à la
preuve}, des outils informatiques servant à écrire, vérifier et communiquer des
preuves.
Via la formalisation de ces preuves et la vérification par l’ordinateur qu’elles
suivent bien les règles du système logique sous-jacent, les assistants à la preuve
donnent accès à une fiabilité bien plus important que celle des preuves
traditionnelles. Des mathématiciens de premier plan comme
Hales~\sidecite[0em][Preface, p. xi]{Hales2012},
Voevodsky~\cite{??} ou
Scholze~\cite{LiquidTensorExperiment}
se sont déjà emparés des assistants à la preuve dans le but de lever les incertitudes
sur la solidité de leur propre travail.

Mais le terme d’\emph{assistant} à la preuve n’a pas été choisi par hasard : au-delà
de la simple vérification, les assistants à la preuve ouvrent la porte à un large
éventail d’outils mis à la disposition de la programmeuse, souvent de façon
interactive. La preuve est ainsi construite comme le produit d’un échange entre 
la programmeuse et l’outil, plutôt que fournie d’un seul bloc.
Il peut s’agir de simples
facilités, comme la possibilité de pouvoir visualiser la structure des
preuves, de suivre l’utilisation des hypothèses…
Mais l’informatique rend surtout possible l’automatisation de pans entiers
de l’écriture de preuves,
par exemple via l’utilisation d’un langage de tactique \cite{??} qui permettent
de programmer la manière dont les preuves sont
générées, voire en utilisant directement la recherche en preuve automatique
\cite{Sledgehammer,SMTCoq}.
\textit{In fine}, cette automatisation permet généralement d’écrire
les preuves à haut niveau, en laissant à l’assistant à la preuve le soin
de construire une preuve formelle.
Un autre axe important, bien qu’encore relativement peu développé, concerne
les interactions entres les outils informatiques dédiés au calcul mathématique
(systèmes de calcul formel, analyse numérique) et les assistants à la
preuve sont une piste très prometteuse.

Enfin, si l’utilisation de programmes de toutes sortes permet de grandement augmenter
les possibilités offertes par les assistants à la preuve,
la présence au même endroit – l’ordinateur –
de preuves et de programmes est également parfaite pour… la preuve de programmes.
Ils offrent en effet un cadre naturel dans lequel décrire au même endroit
le code source d’un programme, sa spécification et la preuve formelle que le
programme s’exécute correctement,
remplissant sa spécification sans rencontrer de bug.
\todo{Citation ?}

\subsection{Logique, programmation et théorie des types}

Pour fonctionner, les assistants à la preuve ont besoin d’une
description formelle des "règles du jeu" mathématiques qu’ils sont censés imposer.
En clair, ils demandent une étude renouvelée de la logique, mais dans le but
pratique de construire des outils à la fois fonctionnels,
puissants et faciles à utiliser.
Il existe plusieurs familles d’assistants à la preuve, basées sur des fondements
logique relativement différents. La famille qui m’intéresse dans cette thèse, est
celle à laquelle appartient Coq, celle basée sur la
\kl{correspondance de Curry-Howard} et la \kl{théorie des types dépendants}.

Si on compare un programme informatique à un texte dans une langue naturelle,
les types sont une sorte d’équivalent des catégories grammaticales.
Contrairement aux langues naturelles, cependant, ces règles de typage sont conçues
en même temps que le langage de programmation, en général
de telle sorte à refléter des propriétés
des objets manipulés par le programme. Cela permet en premier lieu de
détecter des erreurs manifestes.
Par exemple, si une procédure attendant un objet de type "image" est
appliquée à un objet de type "chaîne de caractères", une erreur pourra être rapportée
à la programmeuse.\sidenote{
  Un slogan dû à Milner~\sidecite[5em]{Milner1978} est que
  « Les programmes bien typés ne peuvent pas  mal s’exécuter. »
}
Mais les utilisations des types sont très versatiles, et leurs capacités à encoder
des propriétés des programmes sous-jacents peuvent servir à la compilation, la
documentation, etc.

\begin{marginfigure}[2em]

  % \begin{mathpar}
  %   \inferrule{ \Gamma, A \vdash B}{\Gamma \vdash A \Rightarrow B} \and
  %   \inferrule{\Gamma \vdash A \Rightarrow B \\ \Gamma \vdash A}{\Gamma \vdash B} \and
  %   \inferrule{\Gamma, x : A \vdash t : B}{\Gamma \vdash \lambda x : A . t : A \to B} \and
  %   \inferrule{\Gamma \vdash f : A \to B \\ \Gamma \vdash u : A}{\Gamma \vdash f~u : B}

  % \end{mathpar}

  % \caption{Règles d’inférence pour l’implication et de typage des fonctions}

  \begin{mathpar}
    \inferrule{A \\ B}{A \wedge B} \and
    \inferrule{A \wedge B}{A} \and
    \inferrule{A \wedge B}{B} \\
    \inferrule{a : A \\ b : B}{(a,b) : A \times B} \\
    \inferrule{p : A \times B}{p.1 : A} \and
    \inferrule{p : A \times B}{p.2 : B}
  \end{mathpar}
  
  \caption{Règles d’inférence pour la conjonction et de typage pour les paires}
  \label{fig:curry-howard-exemple}
\end{marginfigure}

Plutôt qu’un théorème précis, la \intro{correspondance de Curry-Howard} est une
idée très générale,
selon lequel il existe une ressemblance forte entre d’un côté le monde de la
logique et des preuves, et de l’autre celui des programmes et de leurs types.
Un exemple valant mieux qu’un discours abstrait, on peut voir la correspondance à l’œuvre dans la \cref{fig:curry-howard-exemple}, sous la forme de règles d’inférence
ou de typage : chaque bloc présente une règle, avec au-dessus de la barre les
hypothèses, et en dessous la conclusion.
Les trois premières règles gouvernent la conjonction logique $\wedge$.
La première signifie que pour déduire la proposition $A \wedge B$,
il suffit de déduire $A$ et $B$ individuellement.
À l’inverse si a comme hypothèse $A \wedge B$, alors on peut déduire $A$ et $B$
individuellement.
Les trois dernières règles gouvernent le typage du type des paires $A \times B$.
Une paire $(a,b)$ a le type $A \times B$ si $a$ est de type $A$ et $b$
est de type $B$.
À l’inverse si $p$ est de type $A \times B$, alors sa première projection $p.1$
est de type $A$ et sa seconde projection $p.2$ est de type $B$.
Si on efface les termes\sidenote{
  Dans ce contexte, on parle souvent de \emph{termes} plutôt que de programmes,
  mais les deux sont synonymes.
} des règles du bas, on obtient \emph{exactement} les règles du haut !

Ceci s’étend bien au-delà du cas de la conjonction,
en une correspondance générale entre d’une part les énoncés de la logique et les types des langages de programmation – \emph{propositions as types} –, et d’autre part les preuves d’un énoncé et les programmes ayant le type correspondant – \emph{proofs as programs}.
Au-delà de la simple analogie entre formalismes d’origines différentes, cette correspondance est un outil puissant pour faire dialoguer deux mondes.
En particulier, elle permet de relier deux problèmes \textit{a priori} éloignés :
la vérification de la correction d’une preuve et l’analyse du type d’un terme.

La correspondance de Curry-Howard est donc idéale pour servir de fondements aux
assistants à la preuve, puisqu’elle permet de voir un système
comme une logique, tout en autorisant l’utilisation d’idées venant de
la large littérature sur les langages de programmation, notamment
la théorie et l’implémentation des systèmes de types.
Dans ce cadre, les \intro{systèmes de types dépendants} sont une famille dont
la caractéristique principale, comme leur nom l’indique, est d’autoriser les
types à dépendre de termes. L’exemple archétypique du point de vue de la 
programmation est le type
$Vect~A~n$\todo{setup macros} des listes contenant exactement $n$ éléments
de type $A$, où $n$ est un entier.
Du point de vue de la logique, cette
dépendance correspond aux quantificateurs, nécessaires pour exprimer des
propriétés universelles – pour tout entier $n$, la propriété $P(n)$ est
vérifiée — et existentielles – il existe un entier $n$ tel que $P(n)$ tient,
sans lesquelles il est tout bonnement impossible d’exprimer la plupart des
mathématiques. En revanche les systèmes à types dépendants, eux, sont suffisamment
riches et puissant pour espérer y développer la plupart des mathématiques.

\section{Grandeur et décadence d’un assistant à la preuve}
\todo{Trouver un meilleur titre}
\label{sec-coq}

  Intéressons-nous un peu plus en détail à l’assistant à la
  preuve dont il sera le plus question dans cette thèse : Coq.

  \subsection{La clé de voûte du système}
  
  \begin{figure}[h]

    \centering
    \includegraphics{./figures/coq-kernel-fr.pdf}
  
    \caption{Le fonctionnement de Coq}
    \label{fig-coq}
  \end{figure}

  Coq est basé sur la \kl{correspondance de Curry-Howard} : les preuves sont vues comme des programmes dans un langage appelé \intro{Gallina},
  et leur vérification est effectuée par un algorithme proche
  de ceux utilisés pour les types des langages conventionnels.
  Cependant, si dans les premières versions des années 80 Coq ressemblait à un langage de programmation un peu étrange, ce n’est actuellement plus du tout le cas.
  La raison, comme on l’a expliqué plus haut, est que Coq est un \emph{assistant} à la preuve.
  C’est pourquoi la majeure partie de Coq dans ses versions actuelles a pour but d’aider l’utilisatrice à générer une preuve correcte sans avoir à l’écrire directement.
  Ce fonctionnement est illustré en \cref{fig-coq} : l’utilisatrice échange interactivement avec Coq, qui utilise cette interaction pour générer un terme de preuve. Celui-ci est ensuite envoyé à une partie bien spécifique du système, appelée \intro{noyau}.
  C’est lui qui implémente l’algorithme de vérification de type, et s’assure ainsi de la correction des termes de preuve construits interactivement.
  Le noyau est donc l’élément crucial du système, car c’est lui – et lui seul – qui est responsable en dernier recours de la validation des preuves.
  
  
  Cette architecture, qui isole clairement la partie critique du système
  est appelée \intro{critère de de Bruijn} \sidecite{Barendregt2001}, en 
  hommage à l’un des pionniers des assistants à la preuve.
  Elle a permis de mener à bien des projets de grande ampleur, parmi lesquels CompCert \sidecite{Kaestner2017} – un compilateur optimisant pour le langage C entièrement prouvé correct –, ou les preuves du théorème des quatre couleurs \sidecite{Gonthier2007} et du théorème de l’ordre impair \sidecite{Gonthier2013}, deux théorèmes importants et difficiles respectivement de la théorie des graphes et des groupes.
  Cependant, si le reste de l’écosystème s’est beaucoup plus développé que le noyau depuis les débuts, celui-ci a également évolué, en se complexifiant graduellement.
  Et comme tout développement logiciel, le noyau n’est pas à l’abri de bugs\sidenote{De l’ordre d'un bug détecté par an, une liste est maintenue à l’adresse suivante : \url{https://github.com/coq/coq/blob/master/dev/doc/critical-bugs}.}.
  Ceux-ci sont en général difficilement exploitables par une utilisatrice de Coq, encore plus sans s’en rendre compte.
  Néanmoins, ils existent, et la tendance à la complexification du noyau ne risque pas de s’arrêter de si tôt.

\subsection{MetaCoq, une formalisation par Coq, pour Coq}
\label{sec-metacoq}

Si on veut garantir un niveau de fiabilité le plus élevé possible, il faut donc de nouvelles idées.
Le projet MetaCoq, a pour but de répondre à cette problématique.
L’idée est simple : il s’agit d’utiliser Coq lui-même pour certifier la correction de son noyau.

Plus précisément, la première étape est de décrire à haut niveau le système de type sur lequel est basé le noyau, puis de démontrer ses propriétés théoriques.
% , comme la confluence de la réduction, la préservation du typage par cette même réduction, etc.
Une fois ces propriétés établies, la deuxième étape consiste à programmer un algorithme de vérification de type ressemblant au maximum à celui du noyau, directement en Gallina,\sidenote{
  En effet, grâce à la correspondance de Curry-Howard, Gallina est certes un langage pour décrire des preuves, mais aussi un véritable langage de programmation fonctionnel !}
tout en démontrant qu’il est bien correct\sidenote{
  Si l’algorithme prétend qu’un terme est bien typé, alors c’est bien le cas.}
et complet\sidenote{
  L’algorithme répond bien affirmativement sur tous les termes bien typés.}.
Enfin, une troisième étape extrait de ce programme Gallina certifié
un programme efficace qui puisse être utilisé en lieu et place du noyau actuel.
Cette extraction est elle-même complexe, car pour obtenir ce programme efficace il
faut effacer le contenu lié à la preuve de correction
pour ne garder que le contenu algorithmiquement intéressant.
C’est pourquoi là encore on prouve que l’extraction est correcte\sidenote{
  C'est-à-dire qu’elle préserve la sémantique des programmes.},
à nouveau en la programmant en Gallina.

Pour prouver la complétude de l’algorithme de typage, il est très utile de
passer par une spécification intermédiaire plus structurée que la description
théorique du système de type utilisée dans la première étape.
En particulier, il est important de séparer deux problèmes proches, mais
différents :
d'une part, la vérification, où on cherche à \emph{vérifier}
qu’un terme a bien un type
donné ; de l’autre, l’inférence, où on cherche à \emph{trouver}
un type pour un terme, s'il existe.
L’algorithme de typage du noyau de Coq est bidirectionnel,
c'est-à-dire qu’il alterne en permanence entre ces deux problèmes
lorsqu’il vérifie qu’un terme est bien typé.
% Par exemple, dans le cas d’une application $f~u$, il commence par inférer un type pour $f$, vérifie qu’il s’agit d’un type produit $\P x : A . B$ (une généralisation du type fonctionnel $A \to B$), puis vérifie que $u$ a le type $A$.
Décrire formellement cette structure bidirectionnelle plus proche de l’algorithme
permet de bien diviser les difficultés entre d’un côté
son équivalence avec la présentation
originale, et de l’autre la partie purement liée aux questions d’implémentation.

\subsection{Un peu de flexibilité dans un monde désespérément statique}


\section{Et cette thèse, alors ?}
\label{sec:cette-these}

Bien que très puissante, l’utilisation des types dépendants a un coût,
car du fait de leur puissance logique, leur  tous leurs aspects
est très complexe.

\subsection{Le typage bidirectionnel}

Ce typage bidirectionnel présente par ailleurs un certain nombre d’avantages théoriques au-delà du cadre de MetaCoq, qui sont cruciaux pour le travail évoqué en \cref{sec-graduel}.

J’ai donc entrepris dans \cite{LennonBertrand2021} à la fois l’étude théorique de ce typage bidirectionnel dans le cadre complexe de Gallina, et la preuve en Coq de l’équivalence avec le système non-dirigé.
Au passage, cette preuve a permis de détecter et corriger un bug dans le noyau qui était passé inaperçu jusque là.
Il reste maintenant à prouver la complétude de l’algorithme vis-à-vis de cette nouvelle spécification, travail qui est en cours.

\subsection{MetaCoq}

\subsection{Élaboration bidirectionnelle pour le typage graduel}
